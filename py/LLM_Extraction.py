import os
import re
import json
import openai
import faiss
import numpy as np
from datetime import datetime
from Methods import *
openai.api_key = "sk-3fk05T3Cme02GzUGBc56BaBfA7Ff4dCa9d7dE5AeA689913c"

openai.base_url = "https://api.gpt.ge/v1/"
openai.default_headers = {"x-foo": "true"}

# ===== 1. åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼ˆFAISSï¼‰ =====
dimension = 1536  # OpenAI text-embedding-3-small è¾“å‡ºå‘é‡ç»´åº¦
index = faiss.IndexFlatL2(dimension)  # L2 è·ç¦»ç´¢å¼•

def chunk_text(text, max_chars=40000):
    """æŠŠé•¿æ–‡æœ¬åˆ‡æˆå®‰å…¨çš„å¤šæ®µ"""
    chunks = []
    while len(text) > max_chars:
        # å°½é‡åœ¨å¥å·ååˆ‡
        split_idx = text.rfind("ã€‚", 0, max_chars)
        if split_idx == -1:
            split_idx = max_chars
        chunks.append(text[:split_idx+1])
        text = text[split_idx+1:]
    if text.strip():
        chunks.append(text)
    return chunks

def Semantic_pre_scanning(history):
    if isinstance(history, dict):
        history = history.get("content", "")
    else:
        history = str(history)

    prompt = f"""è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
        ä»»åŠ¡ï¼šè¯·ä½ åŸºäºä»¥ä¸‹çš„è¯­ä¹‰æ‘˜è¦ï¼Œæ ¹æ®è¿™æ®µæ‘˜è¦ç”Ÿæˆå¯èƒ½å­˜åœ¨çš„ä¸€çº§å¯¹è¯ä¸»é¢˜ã€‚
        è¯­ä¹‰æ‘˜è¦ï¼š{history}

        è¾“å‡ºè¦æ±‚ï¼š
        1. ä¸¥æ ¼è¾“å‡ºä¸ºæ ‡å‡† JSON æ•°ç»„ï¼Œç¦æ­¢ä»£ç å—æ ‡è®°å’Œå¤šä½™æ–‡å­—ã€‚
        2. æ¯ä¸ªä¸»é¢˜åŒ…å«å­—æ®µï¼š
        - "topic": ä¸»é¢˜åç§°ï¼ˆåè¯æˆ–åè¯çŸ­è¯­ï¼Œä¸»é¢˜å¿…é¡»å…·æœ‰æ™®éæ€§ï¼Œå¹¶ä¸”ä¸æ˜“è¿‡äºå…·ä½“ï¼Œæ–¹ä¾¿æ‰©å……å‡ºæ›´å¤šçš„å­ä¸»é¢˜ï¼‰
        - "support_count": ä»æ‘˜è¦ä¸­å¯ä½è¯è¯¥ä¸»é¢˜çš„è¦ç‚¹æ•°é‡ï¼ˆç²—ç•¥ä¼°è®¡ï¼Œæ•´æ•°ï¼‰
        - "support_examples": 1~3 æ¡æ‘˜è‡ªæ‘˜è¦çš„çŸ­è¯æ®ç‰‡æ®µï¼ˆå¿…é¡»æ˜¯åŸæ–‡å­ä¸²ï¼‰
        3. ä¸»é¢˜åº”äº’ç›¸åŒºåˆ†ã€æ¶µç›–ä¸»è¦è¯­ä¹‰æ–¹å‘ï¼›å¦‚æ— è¶³å¤Ÿè¯æ®ï¼Œä¸è¦è‡†é€ ã€‚
        æ­£ç¡®è¾“å‡ºç¤ºä¾‹ï¼ˆç¤ºæ„ï¼‰ï¼š
        [
            {{
            "topic": "äººå·¥æ™ºèƒ½",
            "support_count": 3,
            "support_examples": ["â€¦åŸæ–‡ç‰‡æ®µAâ€¦", "â€¦åŸæ–‡ç‰‡æ®µBâ€¦"]
            }},
            {{
            "topic": "åŸå¸‚æ’æ°´ä»¿çœŸ",
            "support_count": 2,
            "support_examples": ["â€¦åŸæ–‡ç‰‡æ®µCâ€¦"]
            }}
        ]
    """
    completion = openai.chat.completions.create(
        model="gpt-4o",
        temperature=0.2,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€åæ–‡æœ¬èšç±»åˆ†æå¸ˆï¼Œæ“…é•¿ä»å¯¹è¯ä¸­æå–å‡ºå¯¹è¯ä¸»é¢˜ã€‚"},
            {"role": "user", "content": prompt }
            ],)
    try:
        result = json.loads(completion.choices[0].message.content)
    except json.JSONDecodeError:
        result = []

    return result

def Topic_cleaning(history,topic_description,min_support=2):
    # ç»Ÿä¸€è½¬ str
    if isinstance(history, dict):
        history = history.get("content", "")
    else:
        history = str(history)

    # å…¥å‚æ—¢å¯èƒ½æ˜¯ str(JSON)ï¼Œä¹Ÿå¯èƒ½æ˜¯ Python list
    if isinstance(topic_description, (list, tuple)):
        topic_json_str = json.dumps(topic_description, ensure_ascii=False)
    else:
        topic_json_str = str(topic_description)

    prompt = f"""è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
        ä»»åŠ¡ï¼šå¯¹ä¸‹é¢çš„â€œä¸»é¢˜åˆ—è¡¨â€è¿›è¡Œæ¸…æ´—ä¸åˆå¹¶ï¼Œä»…ä¿ç•™ä¸â€œè¯­ä¹‰æ‘˜è¦â€é«˜åº¦ç›¸å…³ã€ä¸”ä¸ç©ºæ³›çš„ä¸»é¢˜ã€‚
        è¯­ä¹‰æ‘˜è¦ï¼š{history}
        ä¸»é¢˜åˆ—è¡¨ï¼ˆJSONæ•°ç»„ï¼Œå…ƒç´ å¯èƒ½åŒ…å« support_count/support_examplesï¼Œä¹Ÿå¯èƒ½ä¸åŒ…å«ï¼‰ï¼š{topic_json_str}

        æ¸…æ´—è§„åˆ™ï¼š
        1. ç›¸å…³æ€§ä¸è¯æ®ï¼š
        - å¦‚å­˜åœ¨ "support_count"ï¼Œè¦æ±‚ support_count â‰¥ {min_support}ï¼›
        - å¦‚ä¸å­˜åœ¨ "support_count"ï¼Œè¯·åŸºäºè¯­ä¹‰ä¸æ‘˜è¦æ˜¯å¦åŒ¹é…æ¥åˆ¤å®šæ˜¯å¦ä¿ç•™ï¼ˆä¿å®ˆç­–ç•¥ï¼Œå®ç¼ºæ¯‹æ»¥ï¼‰ã€‚
        2. å»é‡åˆå¹¶ï¼š
        - åˆå¹¶è¯­ä¹‰é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„ä¸»é¢˜ï¼Œåˆå¹¶ååç§°æ›´æ¸…æ™°ã€æè¿°æ›´å…·ä½“ï¼›
        - å¦‚å­˜åœ¨å¤šä¸ª support_countï¼Œè¯·ç´¯åŠ æˆ–å–æœ€å¤§å€¼ï¼›
        - "support_examples" åˆå¹¶åä¿ç•™ 1~3 æ¡ä»£è¡¨æ€§åŸæ–‡å­ä¸²ã€‚
        3. ç©ºæ³›ä¸»é¢˜å‰”é™¤ï¼šå¦‚ä»…å‡ºç°â€œç ”ç©¶/é—®é¢˜/ç°çŠ¶/å‘å±•/è®¨è®ºâ€ç­‰ã€‚
        4. å­—æ®µç»“æ„ï¼š
        - è‹¥è¾“å…¥å…ƒç´ å«æœ‰ "support_count"/"support_examples"ï¼Œè¯·ä¿ç•™ï¼›
        - è‹¥è¾“å…¥å…ƒç´ æ²¡æœ‰è¿™äº›å­—æ®µï¼Œä¸è¦æ–°å¢ï¼ˆä¿æŒä¸åŸç»“æ„ä¸€è‡´ï¼‰ã€‚
        è¾“å‡ºè¦æ±‚ï¼š
        - ä¸¥æ ¼è¾“å‡ºæ ‡å‡† JSON æ•°ç»„ï¼Œä¸å¾—å‡ºç°ä»£ç å—æ ‡è®°æˆ–å¤šä½™æ–‡å­—ã€‚
    """
    completion = openai.chat.completions.create(
        model="gpt-4o",
        temperature=0.2,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€åæ–‡æœ¬åˆ†æå¸ˆï¼Œæ“…é•¿ä¸»é¢˜å»é‡ã€ç›¸å…³æ€§åˆ¤å®šå’Œè¯æ®æ£€æŸ¥ã€‚"},
            {"role": "user", "content": prompt }
            ])
    try:
        result = json.loads(completion.choices[0].message.content)
    except json.JSONDecodeError:
        result = []

    return result

def Topic_Allocation(history,topic_description):
    # ç»Ÿä¸€è½¬ str
    if isinstance(history, dict):
        history = history.get("content", "")
    else:
        history = str(history)

    # å…¥å‚æ—¢å¯èƒ½æ˜¯ str(JSON)ï¼Œä¹Ÿå¯èƒ½æ˜¯ Python list
    if isinstance(topic_description, (list, tuple)):
        topic_json_str = json.dumps(topic_description, ensure_ascii=False)
    else:
        topic_json_str = str(topic_description)

    prompt = f"""è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
        ä»»åŠ¡ï¼šä½ çš„ä»»åŠ¡æ˜¯è¯†åˆ«å¯ä»¥æ¦‚æ‹¬çš„äºŒçº§å­ä¸»é¢˜ï¼Œè¿™äº›å­ä¸»é¢˜å¯ä»¥ä½œä¸ºæ‰€æä¾›çš„ä¸€çº§ä¸»é¢˜çš„å­ä¸»é¢˜ã€‚
        å¯¹è¯å†å²ï¼š{history}
        ä¸€çº§ä¸»é¢˜æ¸…å•ï¼š{topic_json_str}

        å®ä¾‹ï¼š
        [
            {{
                "topic": "äººå·¥æ™ºèƒ½",
                "slots": [
                    {{"sentence": "äººå·¥æ™ºèƒ½ä¼¦ç†å…³æ³¨çš„ä¸ä»…æ˜¯ç®—æ³•çš„å…¬å¹³æ€§ä¸éšç§ä¿æŠ¤ï¼Œè¿˜åŒ…æ‹¬æ•°æ®ä½¿ç”¨çš„é€æ˜åº¦ã€æ¨¡å‹å†³ç­–çš„å¯è§£é‡Šæ€§ã€‚", "slot": "äººå·¥æ™ºèƒ½ä¼¦ç†"}},
                    {{"sentence": "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨æ­£åœ¨è¿…é€Ÿæ‰©å±•ï¼Œä»è¾…åŠ©è¯Šæ–­åˆ°ä¸ªæ€§åŒ–æ²»ç–—æ–¹æ¡ˆã€‚", "slot": "åŒ»ç–—é¢†åŸŸåº”ç”¨"}}
                ]
            }},
            {{
                "topic": "å¯è§†åŒ–",
                "slots": [
                    {{"sentence": "å¯è§†åŒ–æŠ€æœ¯çš„å‘å±•ä½¿å¾—æ•°æ®æ›´åŠ ä¸°å¯Œã€å¤æ‚ï¼ŒåŒæ—¶ä¹Ÿå¸¦æ¥äº†æ–°çš„å¯è§†åŒ–æ–¹å¼ã€‚", "slot": "å¯è§†åŒ–æŠ€æœ¯çš„å‘å±•"}}
                ]
            }}
        ]


        è§„åˆ™ï¼š
        1. "topic" å¿…é¡»æ˜¯ä¸Šé¢æ¸…å•ä¸­çš„æŸä¸€ä¸ªï¼Œç¦æ­¢åˆ›é€ æ–°ä¸»é¢˜åï¼Œä¸»é¢˜å¿…é¡»å…·æœ‰æ™®éæ€§ï¼Œå¹¶ä¸”ä¸æ˜“è¿‡äºå…·ä½“ï¼Œæ–¹ä¾¿æ‰©å……å‡ºæ›´å¤šçš„å­ä¸»é¢˜ã€‚
        2. "slots[*].sentence" å¿…é¡»æ˜¯ä¸Šé¢å¯¹è¯å†å²ä¸­çš„åŸæ–‡å­ä¸²ï¼›è¯·ä¼˜å…ˆç›´æ¥æ‹·è´å¯¹åº”è¡Œçš„å­ä¸²ã€‚
        3. "slots[*].slot" å¿…é¡»æ˜¯äºŒçº§å­ä¸»é¢˜ï¼Œå¿…é¡»æ˜¯**ç®€æ´ã€å…·ä½“ã€å¯è½åœ°çš„åè¯çŸ­è¯­æˆ–åŠ¨å®¾çŸ­è¯­**ï¼Œèƒ½æŒ‡å‘ä¸€ä¸ªæ¸…æ™°çš„å…³æ³¨ç‚¹ã€‚
        4. æ¯æ¡å¯¹è¯/å¥å­æœ€å¤šåˆ†é… 1 ä¸ªä¸»é¢˜ï¼›è‹¥å‡ä¸åŒ¹é…ï¼Œè·³è¿‡è¯¥æ¡ã€‚

        è¾“å‡ºè¦æ±‚ï¼š
        1. è¾“å‡ºå¿…é¡»æ˜¯æ ‡å‡† JSON æ•°ç»„ï¼Œä¸¥ç¦åŒ…å«ä»£ç å—æ ‡è®°ï¼ˆå¦‚```jsonï¼‰æˆ–å¤šä½™æ–‡å­—ã€‚
        2. æ¯ä¸ªä¸»é¢˜åŒ…å«å­—æ®µï¼š
        - "topic": ä¸»é¢˜åç§°ï¼ˆæœ€é«˜å±‚é¢†åŸŸåï¼Œå¦‚â€œäººå·¥æ™ºèƒ½â€â€œå¯è§†åŒ–â€â€œæ™ºæ…§åŸå¸‚â€ï¼‰ï¼Œä¸ºåè¯çŸ­è¯­ã€‚
        - "slots": ä¸€ä¸ªæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {{ "sentence": åŸå§‹å¥å­}}ï¼Œ{{ "slot": äºŒçº§å­ä¸»é¢˜}}
        3. è¾“å‡ºçš„æ ‡å‡† JSON æ ¼å¼ï¼š
        [
            {{
                "topic": "ä¸€çº§ä¸»é¢˜åç§°",
                "slots": [
                    {{"sentence": "åŸå§‹å¥å­", "slot": "äºŒçº§å­ä¸»é¢˜"}}
                ]
            }}
        ]
        

    """

    completion = openai.chat.completions.create(
        model="gpt-4o",
        temperature=0.2,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€åå¯¹è¯åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä»å¯¹è¯ä¸­æå–å‡ºå¯¹è¯ä¸»é¢˜ã€‚"},
            {"role": "user", "content": prompt }
            ],)
    
    try:
        result = json.loads(completion.choices[0].message.content)
    except json.JSONDecodeError:
        result = []

    return result

def topic_extraction(history):
     # --- ç»Ÿä¸€ history è§†å›¾ ---
    if isinstance(history, list):
        # å°† parse_conversation çš„è¾“å‡ºæ¡ç›®åŒ–ï¼Œæ–¹ä¾¿ LLM é€æ¡å¼•ç”¨åŸæ–‡å­ä¸²
        lines = []
        for m in history:
            if not m.get("content"): 
                continue
            lines.append(f"[{m.get('id')}] ({m.get('role')}) {m['content'].strip()}")
        history_view = "\n".join(lines)
    elif isinstance(history, dict):
        history_view = history.get("content", "")
    else:
        history_view = str(history)

    topic_description = Semantic_pre_scanning(history=history_view)
    topic_description = Topic_cleaning(history=history_view,topic_description=topic_description)

    result = Topic_Allocation(history=history_view,topic_description=topic_description)

    return result 


def llm_extract_information_incremental(history_sentences,new_sentence, existing_topics=None): 
    
    """
    å¯¹æ–°å¥å­è¿›è¡Œä¸»é¢˜æŠ½å–ï¼Œå¹¶ä¸å·²æœ‰æŠ½å–ç»“æœåˆå¹¶
    """
    existing_topics = existing_topics or []

    # ğŸ‘‰ æ”¯æŒ dict æˆ– str
    if isinstance(new_sentence, dict):
        sentence_text = new_sentence.get("content", "")
    else:
        sentence_text = str(new_sentence)
        history_sentences = str(history_sentences)

    prompt = f"""è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

        ä»»åŠ¡ï¼šé¦–å…ˆä½ éœ€è¦å°†æ–°çš„ä¸€å¥å¯¹è¯ä¸­æ— å…³ç´§è¦çš„ä¿¡æ¯è¿›è¡Œè¿‡æ»¤ï¼Œç„¶åå¯¹è¿™å¥å¯¹è¯è¿›è¡Œä¸»é¢˜æŠ½å–ã€‚
        å†å²å¯¹è¯:{history_sentences}

        æ–°çš„å¯¹è¯ï¼š{new_sentence}

        æ–°çš„å¥å­: {sentence_text}

        å·²æœ‰ä¸»é¢˜: {json.dumps(existing_topics, ensure_ascii=False)}

        æŠ½å–ä¸»é¢˜è¿‡ç¨‹è¦æŒ‰ç…§ä¸‹é¢ä¸‰æ­¥æ¥è¿›è¡Œï¼š
        Step 1ï¼šç†è§£æ•´è½®è¯­ä¹‰èƒŒæ™¯
        è¯·å…ˆé˜…è¯»å†å²å¯¹è¯å†…å®¹ï¼Œç†è§£æ•´è½®å¯¹è¯çš„ä¸»è¦è¯­ä¹‰ç„¦ç‚¹æˆ–è®¨è®ºæ–¹å‘ã€‚

        Step 2ï¼šèšç„¦å½“å‰è½®çš„å‰10å¥
        ä»å½“å‰å¯¹è¯ä¸­é€‰å–**æ–°çš„å¥å­: {sentence_text}çš„å‰10å¥**ï¼ˆè‹¥ä¸è¶³10å¥åˆ™å…¨éƒ¨ä½¿ç”¨ï¼‰ï¼Œ
        å’Œå®ƒä»¬çš„ä¸»é¢˜ã€‚

        Step 3ï¼šä¸»é¢˜æŠ½å–ä¸è¾“å‡º
        ç»“åˆ Step 1 çš„å…¨å±€è¯­ä¹‰ç†è§£ä¸ Step 2 çš„å±€éƒ¨ç„¦ç‚¹ï¼ŒæŠ½å–å‡ºæœ¬è½®å¯¹è¯çš„ä¸»é¢˜ï¼Œè¯·åªè¾“å‡ºæ–°å¥å­çš„ä¸»é¢˜ JSONï¼Œä¸ä¿®æ”¹å·²æœ‰ä¸»é¢˜ã€‚

        è¾“å‡ºè¦æ±‚ï¼š
        1. è¾“å‡ºå¿…é¡»æ˜¯æ ‡å‡† JSON å¯¹è±¡ï¼Œä¸¥ç¦åŒ…å«ä»£ç å—æ ‡è®°ï¼ˆå¦‚```jsonï¼‰æˆ–å¤šä½™æ–‡å­—ã€‚
        2. æ¯ä¸ªä¸»é¢˜åŒ…å«å­—æ®µï¼š
        - "topic": ä¸»é¢˜åç§°ï¼ˆæœ€é«˜å±‚é¢†åŸŸåï¼Œå¦‚â€œäººå·¥æ™ºèƒ½â€â€œå¯è§†åŒ–â€â€œæ™ºæ…§åŸå¸‚â€ï¼‰ï¼Œä¸ºåè¯çŸ­è¯­ã€‚
        - "slots": ä¸€ä¸ªæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {{ "sentence": åŸå§‹å¥å­, "slot": å¯¹åº”çš„å­ä¸»é¢˜}}
        3. slotå¿…é¡»æ˜¯**ç®€æ´ã€å…·ä½“ã€å¯è½åœ°çš„åè¯çŸ­è¯­æˆ–åŠ¨å®¾çŸ­è¯­**ï¼Œèƒ½æŒ‡å‘ä¸€ä¸ªæ¸…æ™°çš„å…³æ³¨ç‚¹ã€‚
        4. ä¿æŒä¸»é¢˜ä¸å­ä¸»é¢˜è¡¨è¿°ç®€æ´ã€‚
        5. è¾“å‡ºçš„æ ‡å‡† JSON æ ¼å¼ï¼š
        [
            {{
                "topic": "ä¸»é¢˜åç§°",
                "slots": [
                    {{"sentence": "åŸå§‹å¥å­", "slot": "å­ä¸»é¢˜"}}
                ]
            }}
        ]
        ä¾‹å­ï¼š
        [
            {{
                "topic": "äººå·¥æ™ºèƒ½",
                "slots": [
                    {{"sentence": "äººå·¥æ™ºèƒ½ä¼¦ç†å…³æ³¨çš„ä¸ä»…æ˜¯ç®—æ³•çš„å…¬å¹³æ€§ä¸éšç§ä¿æŠ¤ï¼Œè¿˜åŒ…æ‹¬æ•°æ®ä½¿ç”¨çš„é€æ˜åº¦ã€æ¨¡å‹å†³ç­–çš„å¯è§£é‡Šæ€§ã€‚", "slot": "äººå·¥æ™ºèƒ½ä¼¦ç†"}}
                ]
            }}
        ]


        è§„åˆ™è¡¥å……ï¼š
        1. æ‰€æœ‰é—®é¢˜é¦–å…ˆè¦è¯†åˆ«æœ€é«˜å±‚çš„å¤§ä¸»é¢˜ï¼Œä½œä¸ºå”¯ä¸€çš„topicã€‚
        2. è‹¥å¥å­æ¶‰åŠå¤šä¸ªå†…å®¹ï¼Œè¯·æç‚¼å‡ºæœ€æ ¸å¿ƒçš„ä¸»é¢˜ã€‚
        3. slot **ç¦æ­¢**ç©ºæ³›/ç¬¼ç»Ÿ/æŠ½è±¡æŒ‡ä»£ï¼Œä¾‹å¦‚åªå†™â€œç ”ç©¶â€â€œé—®é¢˜â€â€œåº”ç”¨â€â€œæ–¹æ³•è®ºâ€â€œå½±å“â€â€œå‘å±•â€â€œç°çŠ¶â€â€œè®¨è®ºâ€ç­‰æ³›è¯ã€‚
        4. topicåªè¡¨ç¤ºæ ¸å¿ƒé¢†åŸŸï¼Œslots è´Ÿè´£ç»†åˆ†é—®é¢˜ã€‚
        5. å¤§éƒ¨åˆ†æ—¶é—´botçš„å›å¤æ˜¯æ ¹æ®userçš„é—®é¢˜æ¥çš„ï¼Œæ‰€ä»¥å¤§éƒ¨åˆ†æ—¶é—´botå›å¤çš„ä¸»é¢˜å’Œuserçš„é—®é¢˜çš„ä¸»é¢˜æ˜¯ä¸€è‡´çš„ã€‚
        """
    
    completion = openai.chat.completions.create(
        model="gpt-4o",
        temperature=0.5,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€åå¯¹è¯åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä»å¯¹è¯ä¸­æå–å‡ºç”¨æˆ·çš„å¯¹è¯ä¸»é¢˜å¹¶åˆå¹¶åˆ°å·²æœ‰ä¸»é¢˜ã€‚"},
            {"role": "user", "content": prompt }
            ],)
    
    try:
        result = json.loads(completion.choices[0].message.content)
    except json.JSONDecodeError:
        result = []

    print("æŠ½å–ç»“æœï¼š")
    print(result)

    return result

# ç”Ÿæˆ embedding
def get_embedding(text):
    emb = openai.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return np.array(emb.data[0].embedding, dtype="float32")

# ===== åˆå§‹åŒ– Memoryï¼ˆå¯ä»¥æ¢æˆæ•°æ®åº“ï¼‰ =====
user_memory_db = {}  # ç¤ºä¾‹: {user_id: {key: value}}

# ===== Memoryæ“ä½œå‡½æ•° =====
# å¾—åˆ°ç”¨æˆ·Memory
def get_user_memory(user_id):
    """
    è¿”å›æ•´ç†å¥½çš„æ–‡æœ¬ï¼Œç”¨äºæ‹¼æ¥åˆ°prompt
    """
    if user_id not in user_memory_db:
        return ""
    memory = user_memory_db[user_id]
    return "\n".join([f"{k}: {v}" for k, v in memory.items()])

# æ›´æ–°ç”¨æˆ·Memory
def update_user_memory(user_id, key, value):
    global user_memory_db
    if user_id not in user_memory_db:
        user_memory_db[user_id] = {}
    
    # å¦‚æœ key å·²å­˜åœ¨ï¼Œåšâ€œè¿½åŠ â€è€Œä¸æ˜¯è¦†ç›–
    if key in user_memory_db[user_id]:
        old_value = user_memory_db[user_id][key]
        if value not in old_value:
            user_memory_db[user_id][key] = f"{old_value}; {value}"
    else:
        user_memory_db[user_id][key] = value
    
    user_memory_db[user_id]['last_updated'] = str(datetime.now())

    print(f"Memoryæ›´æ–°ï¼š{user_id} - {key} - {value}")
    print("Memoryåº“:" ,user_memory_db)

def extract_memory_from_text(user_id, new_sentence):
    """
    è°ƒç”¨ GPT è‡ªåŠ¨ä»æ–‡æœ¬ä¸­æŠ½å–å…³é”®ä¿¡æ¯ï¼ˆå¦‚åå­—ã€å…´è¶£ã€åå¥½ç­‰ï¼‰
    å¹¶æ›´æ–° Memory
    """
    prompt = f"""
    è¯·ä»ä¸‹é¢çš„æ–‡æœ¬ä¸­æå–ç”¨æˆ·å¯èƒ½æƒ³è®°ä½çš„å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬å§“åã€å…´è¶£ã€çˆ±å¥½ã€å­¦ä¹ ç›®æ ‡ç­‰ã€‚
    è¾“å‡ºå¿…é¡»æ˜¯æ ‡å‡† JSON å¯¹è±¡ï¼Œä¸¥ç¦åŒ…å«ä»£ç å—æ ‡è®°ï¼ˆå¦‚```jsonï¼‰æˆ–å¤šä½™æ–‡å­—ï¼Œé”®åéšæ„ä½†è¦èƒ½æè¿°ä¿¡æ¯ï¼Œå¦‚ï¼š
    {{
        "name": "Alice",
        "favorite_language": "Python"
    }}

    æ–‡æœ¬å†…å®¹ï¼š
    {new_sentence}
    """

    completion = openai.chat.completions.create(
        model="gpt-4o",
        temperature=0,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€åä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ]
    )
    result = (completion.choices[0].message.content)
    # æ›´æ–°è§£æç»“æœåˆ°Memory
    try:
        memory_data = json.loads(result)
        for key, value in memory_data.items():
            update_user_memory(user_id, key, value)
    except json.JSONDecodeError:
        # å‡ºç°è§£æé”™è¯¯æ—¶å¯ä»¥å¿½ç•¥æˆ–è®°å½•æ—¥å¿—
        print("Memory JSONè§£æå¤±è´¥:", result)

# ===== GPT + Memory + RAGå‡½æ•° =====
def talk_to_chatbot(user_id, content, source, history_msgs, top_k=3):
    """
    content: æœ¬è½®ç”¨æˆ·é—®é¢˜
    source: æ¶ˆæ¯æ¥æºï¼Œå¯ä¼ å…¥
    history_msgs: çŸ­æœŸå¯¹è¯åˆ—è¡¨ [{"role": "user"/"assistant", "content": "..."}]
    index: FAISS ç´¢å¼•å¯¹è±¡ï¼Œç”¨äºå­˜å‚¨é•¿æœŸè®°å¿†
    top_k: RAG æ£€ç´¢æ•°é‡
    """
    global index

    # 1. å…ˆæ£€ç´¢Memory
    memory_context = get_user_memory(user_id)

    # 2. ä¸ºç”¨æˆ·è¾“å…¥ç”Ÿæˆ embedding
    query_emb = get_embedding(content).reshape(1, -1)

    # 3. æ£€ç´¢ç›¸å…³ Memory
    related_context = ""
    if index is not None and memory_context:
        D, I = index.search(query_emb, top_k)
        # ç®€å•ç¤ºä¾‹ï¼šç”¨ç´¢å¼•å¯¹åº”çš„ Memory è¡Œï¼ˆå‡è®¾ memory_text åˆ†è¡Œå­˜å‚¨ï¼‰
        memory_lines = memory_context.split("\n")
        related_context = "\n".join([memory_lines[i] for i in I[0] if i < len(memory_lines)])

    # 4.å…ˆæŠŠå†å²å¯¹è¯æ•´ç†æˆæ–‡æœ¬
    history_text = "\n".join(
    [f"{msg.get('from', 'user').capitalize()}: {msg.get('text', '')}" for msg in history_msgs])

    # 5. ç»„ç»‡ promptï¼ŒæŠŠ Memory å’Œ RAG æ£€ç´¢å†…å®¹éƒ½æ‹¼è¿›å»
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€åå¯¹è¯åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä¸ç”¨æˆ·è¿›è¡Œæ²Ÿé€š, è¯·æ ¹æ®ç”¨æˆ·çš„è¾“å…¥ï¼Œåˆç†å›ç­”ï¼Œå¹¶ä¿æŒæ²Ÿé€šè¿è´¯ã€‚"},
        {"role": "user", "content": f"""
        ä¸‹é¢æ˜¯ä¸æœ¬é—®é¢˜ç›¸å…³çš„å†å²å¯¹è¯ï¼š{history_text}
        ç”¨æˆ·ä¿¡æ¯ï¼š
        {memory_context}
        ç›¸å…³ Memory æ£€ç´¢å†…å®¹ï¼š
        {related_context}
        ç°åœ¨ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š
        {content}"""}
        ]
    
    # 6. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›å¤ 
    completion = openai.chat.completions.create(
        model="gpt-4o",
        temperature=0.5,
        messages=messages,
        )
    result = (completion.choices[0].message.content)

    # 7. æ›´æ–° Memoryï¼ˆé•¿æœŸè®°å¿†ï¼‰åˆ° FAISS
    if index is not None and memory_context:
        memory_text = "\n".join([f"{k}: {v}" for k, v in user_memory_db[user_id].items()])
        emb = get_embedding(memory_text).reshape(1, -1)
        index.add(emb)

    # 8. è‡ªåŠ¨æŠ½å–æœ€æ–° Memory ä¿¡æ¯
    extract_memory_from_text(user_id, content)

    return result

def create_theme_variables(result_dict):
    for theme, questions in result_dict.items():
        # åˆ›å»ºåˆæ³•å˜é‡åï¼ˆç§»é™¤éæ³•å­—ç¬¦ï¼‰
        var_name = theme.replace(" ", "_").replace("ï¼š", "").replace("-", "_")
        globals()[var_name] = questions
    
    print(f"{var_name} = {questions}")  # å¯é€‰ï¼šæ‰“å°å‡ºæ¥
