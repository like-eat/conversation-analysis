import os
import re
import json
import openai
import faiss
import numpy as np
from datetime import datetime
from typing import List, Dict, Any
from Methods import *
openai.api_key = "sk-3fk05T3Cme02GzUGBc56BaBfA7Ff4dCa9d7dE5AeA689913c"

openai.base_url = "https://api.gpt.ge/v1/"
openai.default_headers = {"x-foo": "true"}

# ===== 1. åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼ˆFAISSï¼‰ =====
dimension = 1536  # OpenAI text-embedding-3-small è¾“å‡ºå‘é‡ç»´åº¦
index = faiss.IndexFlatL2(dimension)  # L2 è·ç¦»ç´¢å¼•

# ====== å·¥å…·ï¼šæŠŠå¯¹è¯åˆ‡æˆçª—å£ ======
def build_conv_chunks(history, window_size=20, stride=20):
    """
    æŠŠæ•´è½®å¯¹è¯æŒ‰ id æ’åºåï¼Œåˆ‡æˆä¸€æ®µæ®µ chunkï¼Œ
    æ¯æ®µåŒ…å« window_size æ¡å¯¹è¯ï¼ˆå¯é‡å ï¼Œæ­¥é•¿ strideï¼‰ã€‚
    æ¯ä¸ª chunk ç»“æ„ï¼š{start_id, end_id, text}
    """
    # å…ˆæŒ‰ id æ’åº
    history_sorted = sorted(
        [m for m in history if isinstance(m, dict) and m.get("content")],
        key=lambda x: x.get("id", 0)
    )

    chunks = []
    n = len(history_sorted)
    if n == 0:
        return chunks

    idx = 0
    while idx < n:
        sub = history_sorted[idx: idx + window_size]
        if not sub:
            break
        text = "\n".join(
            f"[{m['id']}][{m['role']}]: {m['content'].strip()}"
            for m in sub if m.get("content")
        )
        chunks.append({
            "start_id": sub[0]["id"],
            "end_id": sub[-1]["id"],
            "text": text,
        })
        if idx + window_size >= n:
            break
        idx += stride

    return chunks

def embed_texts(
        text_list, 
        model="text-embedding-3-large",
        max_chars_per_item=2000,\
        max_batch_chars=6000,
        max_batch_items=8 ):
    """
    è¾“å…¥: [str, str, ...]
    è¾“å‡º: [np.array(d), ...]
    """
    processed = []
    for text in text_list:
        t = str(text)
        if len(t) > max_chars_per_item:
            t = t[:max_chars_per_item]
        processed.append(t)

    embs = []
    batch = []
    batch_chars = 0

    def _flush_batch(batch_texts):
        if not batch_texts:
            return []
        resp = openai.embeddings.create(
            model=model,
            input=batch_texts,
        )
        return [np.array(d.embedding, dtype="float32") for d in resp.data]

    for t in processed:
        # å¦‚æœå†åŠ è¿™ä¸€æ¡ä¼šè¶…å‡ºé™åˆ¶ï¼Œå°±å…ˆæŠŠå½“å‰ batch å‘å‡ºå»
        if (batch and (batch_chars + len(t) > max_batch_chars
                       or len(batch) >= max_batch_items)):
            embs.extend(_flush_batch(batch))
            batch = []
            batch_chars = 0

        batch.append(t)
        batch_chars += len(t)

    # åˆ«å¿˜äº† flush æœ€åä¸€ä¸ª batch
    if batch:
        embs.extend(_flush_batch(batch))

    return embs

# å‘é‡æ•°æ®åº“ç±»
class ConvVectorStore:
    def __init__(self, chunks, index, emb_dim):
        self.chunks = chunks          # list[dict], æ¯ä¸ªæœ‰ start_id/end_id/text
        self.index = index            # faiss index
        self.emb_dim = emb_dim

    @classmethod
    def from_history(cls, history, window_size=20, stride=20):
        """
        ä»ä¸€æ•´è½®å¯¹è¯æ„å»ºå‘é‡åº“ï¼š
        - åˆ‡ chunk
        - å¯¹æ¯ä¸ª text åš embedding
        - ç”¨ FAISS å»º IndexFlatIP
        """
        chunks = build_conv_chunks(history, window_size=window_size, stride=stride)
        if not chunks:
            # ç©º history
            index = None
            return cls([], index, 0)

        texts = [c["text"] for c in chunks]
        embs = embed_texts(texts)  # list[np.array]
        emb_dim = embs[0].shape[0]

        emb_matrix = np.stack(embs, axis=0)  # (N, d)
        index = faiss.IndexFlatIP(emb_dim)   # å†…ç§¯ï¼Œç›¸å½“äºä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆéœ€å…ˆå½’ä¸€åŒ–çš„è¯å¯ä»¥å†å°è£…ï¼‰
        index.add(emb_matrix)

        # æŠŠ embedding ä¸€èµ·å­˜ä½æ–¹ä¾¿ debugï¼ˆä¸ä¸€å®šå¿…é¡»ï¼‰
        for c, e in zip(chunks, embs):
            c["embedding"] = e

        return cls(chunks, index, emb_dim)

    def search_by_text(self, query_text, top_k=8):
        """
        ç»™ä¸€æ®µ query æ–‡æœ¬ï¼Œè¿”å›æœ€ç›¸å…³çš„ top_k ä¸ª chunkï¼ˆå·²ç»æŒ‰ç›¸ä¼¼åº¦æ’å¥½ï¼‰
        """
        if not self.chunks or self.index is None:
            return []

        q_emb = embed_texts([query_text])[0].reshape(1, -1).astype("float32")  # (1, d)
        D, I = self.index.search(q_emb, top_k)  # I: (1, top_k)

        idxs = I[0]
        selected = [self.chunks[i] for i in idxs if 0 <= i < len(self.chunks)]
        # æŒ‰æ—¶é—´é¡ºåºæ’ä¸€ä¸‹ï¼Œæ–¹ä¾¿ä½ åé¢ç”¨
        selected.sort(key=lambda c: c["start_id"])
        return selected

    def build_context(self, query_text, top_k=5):
        """
        æŠŠæ£€ç´¢åˆ°çš„ top_k chunks æ‹¼æˆä¸€ä¸ªä¸Šä¸‹æ–‡æ–‡æœ¬ï¼Œç”¨æ¥ä¸¢ç»™ LLM çœ‹ã€‚
        """
        selected = self.search_by_text(query_text, top_k=top_k)
        if not selected:
            return ""

        ctx = "\n\n".join(
            f"[å¯¹è¯ç‰‡æ®µ {c['start_id']}~{c['end_id']}]\n{c['text']}"
            for c in selected
        )
        return ctx

def Score_turn_importance(history):
    """
    history: list[dict]ï¼Œå½¢å¦‚ï¼š
      [{"id": 1, "role": "user", "content": "..."}, ...]
    è¿”å›ï¼šåŒæ ·é•¿åº¦çš„ listï¼Œæ¯ä¸ªå…ƒç´ å¤šä¸€ä¸ª "info_score" å­—æ®µï¼ˆ0.2 ~ 1.0ï¼‰
    """

    if not isinstance(history, list) or not history:
        print("âš ï¸ Score_turn_importance: history ä¸ºç©ºæˆ–æ ¼å¼å¼‚å¸¸ï¼Œå°†è¿”å›åŸæ ·ã€‚")
        return history

    # 1) æŠŠå¯¹è¯æ•´ç†æˆ [id][role]: content å½¢å¼ï¼Œç»™ LLM çœ‹
    lines = []
    for m in history:
        mid = m.get("id")
        role = m.get("role") or m.get("from") or "user"
        text = (m.get("content") or m.get("text") or "").strip()
        if mid is None or text == "":
            continue
        lines.append(f"[{mid}][{role}]: {text}")

    if not lines:
        return history

    conv_text = "\n".join(lines)

 # 2. æ„é€  promptï¼šåªè®©æ¨¡å‹è¾“å‡º id + info_score
    prompt = f"""ä½ æ˜¯ä¸€åä¸¥è°¨çš„å¯¹è¯åˆ†æåŠ©æ‰‹ã€‚

        ç°åœ¨ç»™ä½ ä¸€æ®µå¤šè½®å¯¹è¯ï¼Œæ¯ä¸€è¡Œçš„æ ¼å¼ä¸ºï¼š
        [id][role]: content

        å…¶ä¸­ï¼š
        - id æ˜¯å¯¹è¯è½®æ¬¡çš„æ•´æ•°ç¼–å·ï¼›
        - role æ˜¯è¯´è¯äººè§’è‰²ï¼›
        - content æ˜¯è¯¥è½®çš„å‘è¨€å†…å®¹ã€‚

        è¯·ä½ æ ¹æ®æ•´æ®µå¯¹è¯çš„è¯­ä¹‰ï¼Œä¸ºå…¶ä¸­æ¯ä¸€è½®â€œå®é™…æœ‰å†…å®¹çš„å¯¹è¯â€æ‰“ä¸€ä¸ªâ€œä¿¡æ¯é‡/é‡è¦ç¨‹åº¦â€åˆ†æ•° info_scoreï¼Œç”¨æ¥è¡¡é‡è¿™æ¡å‘è¨€åœ¨æ•´æ®µå¯¹è¯ä¸­çš„é‡è¦æ€§ã€‚

        è¦æ±‚ï¼š
        1. å¯¹æ¯ä¸€æ¡å‡ºç°çš„ idï¼ˆå³æ¯ä¸€è¡Œå‘è¨€ï¼‰éƒ½ç»™å‡ºä¸€ä¸ª info_scoreï¼›
        2. info_score ä¸ºæµ®ç‚¹æ•°ï¼ŒèŒƒå›´åœ¨ 0.2 ~ 1.0 ä¹‹é—´ï¼š
        - è¶Šæ¥è¿‘ 1.0ï¼Œè¯´æ˜è¿™è½®å‘è¨€è¶Šå…³é”®ã€ä¿¡æ¯é‡è¶Šå¤§ï¼›
        - è¶Šæ¥è¿‘ 0.2ï¼Œè¯´æ˜è¿™è½®å‘è¨€è¶Šè¾¹ç¼˜ã€é‡å¤æˆ–é—²èŠæ€§è´¨ï¼›
        3. ä¸éœ€è¦è¾“å‡º role æˆ– contentï¼Œåªéœ€è¦è¾“å‡º id å’Œ info_scoreï¼›
        4. ä¸¥æ ¼è¾“å‡ºä¸€ä¸ª JSON æ•°ç»„ï¼Œç¦æ­¢ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€æ³¨é‡Šæˆ–ä»£ç å—æ ‡è®°ã€‚

        å¯¹è¯å†…å®¹å¦‚ä¸‹ï¼š
        {conv_text}

        è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆç¤ºä¾‹ï¼‰ï¼š
        [
        {{"id": 1, "info_score": 0.85}},
        {{"id": 2, "info_score": 0.35}}
        ]
        """

    completion = openai.chat.completions.create(
        model="gpt-4o",
        temperature=0.2,
        messages=[
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€åä¸¥è°¨çš„å¯¹è¯åˆ†æåŠ©æ‰‹ï¼Œåªè¾“å‡ºä¸¥æ ¼ JSONã€‚",
            },
            {"role": "user", "content": prompt},
        ],
    )

    raw = completion.choices[0].message.content.strip()

    # 3) ç®€å•é²æ£’è§£æï¼šå»æ‰ ```json åŒ…è£¹
    clean = raw
    if clean.startswith("```"):
        first_newline = clean.find("\n")
        if first_newline != -1:
            clean = clean[first_newline + 1 :]
        end_fence = clean.rfind("```")
        if end_fence != -1:
            clean = clean[:end_fence]
        clean = clean.strip()

    if "[" in clean and "]" in clean:
        start = clean.find("[")
        end = clean.rfind("]")
        if start != -1 and end != -1 and end > start:
            clean = clean[start : end + 1].strip()

    # æˆªå–ç¬¬ä¸€ä¸ª [ åˆ° æœ€åä¸€ä¸ª ] ä¹‹é—´
    if "[" in clean and "]" in clean:
        start = clean.find("[")
        end = clean.rfind("]")
        if start != -1 and end != -1 and end > start:
            clean = clean[start : end + 1].strip()

    id2score: Dict[int, float] = {}

    try:
        arr = json.loads(clean)
        if isinstance(arr, list):
            for item in arr:
                if not isinstance(item, dict):
                    continue
                try:
                    mid = int(item.get("id"))
                except Exception:
                    continue
                score = item.get("info_score")
                try:
                    score = float(score)
                except Exception:
                    score = 0.5
                # çº¦æŸåˆ° [0.2, 1.0]
                score = max(0.2, min(1.0, score))
                id2score[mid] = score
    except Exception as e:
        print(f"âš ï¸ Score_turn_importance: JSON è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°ã€‚err={e}, raw={raw}")

    # 4. æŠŠ score è´´å›åŸ history_chunkï¼Œä¿è¯æ¯æ¡éƒ½æœ‰ info_score
    new_history = []
    for m in history:
        mid = m.get("id")
        m2 = dict(m)
        # å¦‚æœè¯¥ id æ²¡åœ¨ LLM è¾“å‡ºé‡Œï¼Œå°±ç»™ä¸€ä¸ªé»˜è®¤å€¼ 0.5
        m2["info_score"] = float(id2score.get(mid, 0.5))
        new_history.append(m2)

    return new_history


def Semantic_pre_scanning(history):
    if isinstance(history, dict):
        history = history.get("content", "")
    else:
        history = str(history)

    prompt = f"""è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
        ä»»åŠ¡ï¼šè¯·ä½ åŸºäºä»¥ä¸‹çš„è¯­ä¹‰æ‘˜è¦ï¼Œæ ¹æ®è¿™æ®µæ‘˜è¦ç”Ÿæˆå¯èƒ½å­˜åœ¨çš„ä¸€çº§å¯¹è¯ä¸»é¢˜ã€‚
        è¯­ä¹‰æ‘˜è¦ï¼š{history}

        ã€é‡è¦çº¦æŸï¼ˆè¯·ä¸¥æ ¼éµå®ˆï¼‰ã€‘ï¼š
        1. æ¯ä¸€ä¸ª "topic" å¿…é¡»åªè¡¨è¾¾**ä¸€ä¸ª**æ ¸å¿ƒä¸»é¢˜ï¼Œè€Œä¸æ˜¯ä¸¤ä¸ªæˆ–å¤šä¸ªå¹¶åˆ—çš„ä¸»é¢˜ã€‚
        2. ç¦æ­¢ä½¿ç”¨å¦‚ä¸‹å¹¶åˆ—å†™æ³•ï¼š
           - "XXXä¸YYY"
           - "XXXå’ŒYYY"
           - "XXXåŠYYY"
           - "XXX / YYY"
           - åŒ…å«å¤šä¸ªâ€œã€â€æŠŠå¥½å‡ ä¸ªè¯ä¸²åœ¨ä¸€èµ·ï¼ˆå¦‚ "å­¦ä¹ ã€å·¥ä½œã€æ„Ÿæƒ…é—®é¢˜"ï¼‰ã€‚
        3. å¦‚æœä½ å‘ç°æŸä¸ªæ–¹å‘å…¶å®åŒ…å«ä¸¤ä¸ªå­ä¸»é¢˜ï¼Œä¾‹å¦‚ï¼š
           - åŸæœ¬ä½ æƒ³å†™æˆ "ç»æµå‹åŠ›ä¸å…¼èŒ"
           åˆ™è¯·æ”¹å†™ä¸ºä¸¤æ¡ç‹¬ç«‹çš„ä¸»é¢˜ï¼š
           - "ç»æµå‹åŠ›"
           - "å…¼èŒå·¥ä½œ"
        4. topic åº”è¯¥æ˜¯**åè¯æˆ–åè¯çŸ­è¯­**ï¼Œå°½é‡ç®€çŸ­æ¸…æ™°ï¼Œå¹¶æœ‰ä¸€å®šæ™®éæ€§ï¼Œæ–¹ä¾¿ä¸‹é¢å†æ‰©å±•å‡ºå¤šä¸ªå­ä¸»é¢˜ï¼›
           - âœ… æ¨èç¤ºä¾‹ï¼š "ç»æµå‹åŠ›"ã€"ç¡çœ é—®é¢˜"ã€"èº«ä½“å½¢è±¡ç„¦è™‘"
           - âŒ ä¸è¦ï¼š "å…³äºæˆ‘æœ€è¿‘ç»æµå‹åŠ›å¾ˆå¤§çš„é—®é¢˜"ï¼ˆå¤ªé•¿ã€åƒä¸€å¥è¯ï¼‰
        5. åŒä¸€ç±»è¯­ä¹‰éå¸¸ç›¸è¿‘çš„ä¸»é¢˜ï¼Œè¯·ä½¿ç”¨ä¸€ä¸ªæ›´é€šç”¨ã€æ¦‚æ‹¬æ€§çš„åå­—ï¼š
           - ä¾‹å¦‚ "èº«ä½“å½¢è±¡ä¸å¥åº·"ã€"å‡è‚¥ä¸èº«ä½“å¥åº·"ã€"èº«æç„¦è™‘"
           æœ€ç»ˆå¯ä»¥ç»Ÿä¸€ä¸ºä¸€ä¸ªæ›´æ¦‚æ‹¬çš„ä¸»é¢˜ï¼š "èº«ä½“å½¢è±¡ä¸å¥åº·çŠ¶å†µ" æˆ– "èº«ä½“å½¢è±¡ç„¦è™‘"
           ï¼ˆæ³¨æ„ä»ç„¶ä¸è¦ç”¨ "Xä¸Y" æ—¶ï¼Œä¼˜å…ˆå†™æˆ "èº«ä½“å½¢è±¡ä¸å¥åº·çŠ¶å†µ" è¿™ç§æ•´ä½“æ¦‚å¿µï¼Œ
            æˆ–è€…ç›´æ¥å†™ "èº«ä½“å½¢è±¡ä¸å¥åº·çŠ¶å†µé—®é¢˜"â€”â€”**ä¸è¦æ˜æ˜¾çœ‹æˆä¸¤ä¸ªå¹¶åˆ—å¯¹è±¡**ï¼‰

        ã€è¾“å‡ºè¦æ±‚ã€‘ï¼š
        1. ä¸¥æ ¼è¾“å‡ºä¸ºæ ‡å‡† JSON æ•°ç»„ï¼Œç¦æ­¢ä»£ç å—æ ‡è®°å’Œå¤šä½™æ–‡å­—ã€‚
        2. æ¯ä¸ªä¸»é¢˜åŒ…å«å­—æ®µï¼š
           - "topic": ä¸»é¢˜åç§°ï¼ˆç¬¦åˆä»¥ä¸Šçº¦æŸï¼‰
           - "support_count": ä»æ‘˜è¦ä¸­å¯ä½è¯è¯¥ä¸»é¢˜çš„è¦ç‚¹æ•°é‡ï¼ˆç²—ç•¥ä¼°è®¡ï¼Œæ•´æ•°ï¼‰
           - "support_examples": 1~3 æ¡æ‘˜è‡ªæ‘˜è¦çš„çŸ­è¯æ®ç‰‡æ®µï¼ˆå¿…é¡»æ˜¯åŸæ–‡å­ä¸²ï¼‰
        3. ä¸»é¢˜åº”äº’ç›¸åŒºåˆ†ã€æ¶µç›–ä¸»è¦è¯­ä¹‰æ–¹å‘ï¼›å¦‚æ— è¶³å¤Ÿè¯æ®ï¼Œä¸è¦è‡†é€ ã€‚

        ã€æ­£ç¡®è¾“å‡ºç¤ºä¾‹ï¼ˆç¤ºæ„ï¼‰ã€‘ï¼š
        [
          {{
            "topic": "ç»æµå‹åŠ›",
            "support_count": 3,
            "support_examples": ["â€¦åŸæ–‡ç‰‡æ®µAâ€¦", "â€¦åŸæ–‡ç‰‡æ®µBâ€¦"]
          }},
          {{
            "topic": "å…¼èŒå·¥ä½œ",
            "support_count": 2,
            "support_examples": ["â€¦åŸæ–‡ç‰‡æ®µCâ€¦"]
          }},
          {{
            "topic": "èº«ä½“å½¢è±¡ç„¦è™‘",
            "support_count": 2,
            "support_examples": ["â€¦åŸæ–‡ç‰‡æ®µDâ€¦"]
          }}
        ]
    """
    completion = openai.chat.completions.create(
        model="gpt-4o",
        temperature=0.2,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€åæ–‡æœ¬èšç±»åˆ†æå¸ˆï¼Œæ“…é•¿ä»å¯¹è¯ä¸­æå–å‡ºå¯¹è¯ä¸»é¢˜ã€‚"},
            {"role": "user", "content": prompt }
            ],)
    try:
        result = json.loads(completion.choices[0].message.content)
    except json.JSONDecodeError:
        result = []

    return result

def Topic_cleaning(history, topic_description, min_support=2):
    """
    ç‰ˆæœ¬è¯´æ˜ï¼š
    - æ­¥éª¤1ï¼šæŒ‰ topic å­—ç¬¦ä¸²èšåˆï¼ˆå®Œå…¨ç›¸åŒçš„ä¸»é¢˜ååˆå¹¶ï¼Œsupport_count ç´¯åŠ ï¼‰
    - æ­¥éª¤2ï¼šæŒ‰ support_count è¿‡æ»¤æ‰å‡ºç°æ¬¡æ•°å¤ªå°‘çš„ä¸»é¢˜
    - æ­¥éª¤3ï¼šè°ƒç”¨ LLM åšâ€œè¯­ä¹‰å»é‡â€ï¼Œä½†åªèƒ½åœ¨åŸå§‹ topic åé‡Œé€‰å­é›†
             ï¼ˆç¦æ­¢æ”¹åã€ç¦æ­¢ç”Ÿæˆæ–°ä¸»é¢˜ï¼‰
    - è¾“å‡ºï¼šæœ€ç»ˆä¿ç•™çš„ä¸»é¢˜å¯¹è±¡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸€å®šæ¥è‡ªåŸå§‹è¾“å…¥
    """
    # -------- 0. è¾“å…¥å…œåº• --------
    if not isinstance(topic_description, list):
        return []

    # -------- 1. å…ˆåšæœ¬åœ°èšåˆï¼šåŒå topic åˆå¹¶ --------
    # key: topic åå­—ï¼ˆå»æ‰é¦–å°¾ç©ºæ ¼ï¼‰
    agg = {}  # topic_name -> merged_obj

    for item in topic_description:
        if not isinstance(item, dict):
            continue
        raw_name = (item.get("topic") or "").strip()
        if not raw_name:
            continue

        # åˆå§‹åŒ–
        if raw_name not in agg:
            new_item = dict(item)
            # ä¿è¯æœ‰ support_count å­—æ®µ
            sc = new_item.get("support_count")
            if isinstance(sc, int):
                pass
            else:
                # æ²¡ç»™å°±å½“ä½œ 1 æ¬¡
                new_item["support_count"] = 1
            # ç¡®ä¿ support_examples ä¸º list
            se = new_item.get("support_examples")
            if se is None:
                new_item["support_examples"] = []
            elif isinstance(se, list):
                new_item["support_examples"] = se
            else:
                new_item["support_examples"] = [str(se)]
            agg[raw_name] = new_item
        else:
            # å·²ç»æœ‰ä¸€ä¸ªä»£è¡¨ï¼Œåšç´¯åŠ 
            exist = agg[raw_name]
            # support_count ç´¯åŠ 
            sc_old = exist.get("support_count", 0)
            sc_new = item.get("support_count", 0)
            try:
                sc_old = int(sc_old)
            except Exception:
                sc_old = 0
            try:
                sc_new = int(sc_new)
            except Exception:
                sc_new = 0
            exist["support_count"] = sc_old + sc_new

            # åˆå¹¶ support_examples
            se_old = exist.get("support_examples") or []
            if not isinstance(se_old, list):
                se_old = [str(se_old)]
            se_new = item.get("support_examples") or []
            if not isinstance(se_new, list):
                se_new = [str(se_new)]
            merged_examples = se_old + se_new
            # å»é‡ + æˆªæ–­åˆ°æœ€å¤š 3 æ¡
            dedup_examples = []
            for ex in merged_examples:
                ex = str(ex)
                if ex not in dedup_examples:
                    dedup_examples.append(ex)
                if len(dedup_examples) >= 3:
                    break
            exist["support_examples"] = dedup_examples

    # -------- 2. support_count è¿‡æ»¤ï¼šå‡ºç°æ¬¡æ•°å¤ªå°‘çš„å‰”é™¤ --------
    filtered = []
    for name, obj in agg.items():
        sc = obj.get("support_count", 0)
        try:
            sc = int(sc)
        except Exception:
            sc = 0
        if sc < min_support:
            # ä¸¢æ‰ä½é¢‘ä¸»é¢˜
            continue
        filtered.append(obj)

    # å¦‚æœè¿‡æ»¤å®Œä¹‹åç©ºäº†ï¼Œç›´æ¥è¿”å›èšåˆç»“æœï¼ˆæœ€å¤šåªåšè¿‡æœ¬åœ°è¿‡æ»¤ï¼‰
    if not filtered:
        return list(agg.values())

    # -------- 3. è°ƒç”¨ LLM åšè¯­ä¹‰å»é‡ï¼ˆä½†ç¦æ­¢æ–°ä¸»é¢˜/æ”¹åï¼‰ --------
    # å‡†å¤‡ç»™ LLM çš„ç®€åŒ–ç»“æ„ï¼Œåªä¼  topic å + support_count
    candidates = [
        {
            "topic": (t.get("topic") or "").strip(),
            "support_count": int(t.get("support_count", 0)),
        }
        for t in filtered
        if (t.get("topic") or "").strip()
    ]

    topics_for_llm = json.dumps(candidates, ensure_ascii=False)

    dedup_prompt = f"""ä½ å°†çœ‹åˆ°ä¸€ç»„å€™é€‰çš„ä¸€çº§ä¸»é¢˜ï¼Œå®ƒä»¬æœ‰å¯èƒ½è¯­ä¹‰ä¸Šæœ‰é‡å¤æˆ–éå¸¸ç›¸è¿‘ã€‚

        å€™é€‰ä¸»é¢˜åˆ—è¡¨ï¼ˆJSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« topic å’Œ support_countï¼‰ï¼š
        {topics_for_llm}

        ä½ çš„ä»»åŠ¡ï¼š
        1. è¯†åˆ«å…¶ä¸­è¯­ä¹‰é«˜åº¦é‡å¤ã€ä»…è¡¨è¿°ç•¥æœ‰ä¸åŒçš„ä¸»é¢˜ï¼›
        2. åœ¨è¿™äº›é‡å¤ä¸»é¢˜ä¸­ï¼Œé€‰æ‹©ä¸€ä¸ªä½œä¸ºâ€œä»£è¡¨ä¸»é¢˜â€ä¿ç•™ï¼Œå…¶ä½™è§†ä¸ºè¢«åˆå¹¶ï¼Œä¸å†å•ç‹¬ä¿ç•™ï¼›
        3. ä½ åªèƒ½åœ¨ã€åŸå§‹ topic å­—ç¬¦ä¸²ã€‘ä¸­é€‰æ‹©ä¿ç•™å¯¹è±¡ï¼š
        - ä¸å…è®¸å¯¹ topic æ–‡æœ¬è¿›è¡Œä»»ä½•æ”¹å†™ï¼›
        - ä¸å…è®¸ç”Ÿæˆæ–°çš„ä¸»é¢˜åç§°ï¼›
        - è¾“å‡ºä¸­çš„æ¯ä¸€ä¸ªå­—ç¬¦ä¸²å¿…é¡»ä¸¥æ ¼ç­‰äºè¾“å…¥é‡ŒæŸä¸ªå¯¹è±¡çš„ topic å­—æ®µã€‚

        é€‰æ‹©ç­–ç•¥å»ºè®®ï¼ˆä¸æ˜¯ç¡¬æ€§è¦æ±‚ï¼‰ï¼š
        - å¯ä»¥ä¼˜å…ˆä¿ç•™ support_count è¾ƒå¤§çš„é‚£ä¸ªï¼›
        - å¦‚æœ support_count æ¥è¿‘ï¼Œå¯ä»¥ä¿ç•™è¯­ä¹‰æ›´æ¸…æ™°ã€ä¿¡æ¯é‡æ›´å¤§çš„é‚£ä¸ªï¼›
        - å¦‚æœä¸¤ä¸ªä¸»é¢˜è¯­ä¹‰å·®å¼‚è¾ƒå¤§ï¼ˆä¾‹å¦‚â€œå°±ä¸šå‹åŠ›â€å’Œâ€œèŒä¸šå‘å±•è§„åˆ’â€ï¼‰ï¼Œè¯·ä¸è¦åˆå¹¶ã€‚

        è¾“å‡ºè¦æ±‚ï¼š
        - ä¸¥æ ¼è¾“å‡ºä¸€ä¸ª JSON æ•°ç»„ï¼›
        - æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¯¹åº”éœ€è¦ä¿ç•™çš„ topic åç§°ï¼›
        - è¿™äº›å­—ç¬¦ä¸²å¿…é¡»å…¨éƒ¨æ¥è‡ªè¾“å…¥çš„ topic å­—æ®µï¼Œä¸å…è®¸æ–°å¢ã€ä¸å…è®¸æ”¹å†™ï¼›
        - ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€æ³¨é‡Šæˆ–ä»£ç å—æ ‡è®°ï¼ˆä¾‹å¦‚ ```jsonï¼‰ã€‚
        ç¤ºä¾‹ï¼ˆä»…ç¤ºæ„æ ¼å¼ï¼‰ï¼š
        ["è‡ªæˆ‘ä»·å€¼æ€€ç–‘", "å¿ƒç†å¥åº·é—®é¢˜", "å°±ä¸šå‹åŠ›", "èŒä¸šå‘å±•è§„åˆ’"]
        """

    try:
        completion = openai.chat.completions.create(
            model="gpt-4o",
            temperature=0.1,
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€åä¸¥æ ¼çš„ä¸»é¢˜å»é‡åŠ©æ‰‹ï¼Œåªåœ¨ç»™å®šçš„ topic åç§°ä¸­é€‰æ‹©å­é›†ï¼Œä¸ä¼šæ”¹å†™æˆ–ç”Ÿæˆæ–°ä¸»é¢˜ã€‚",
                },
                {"role": "user", "content": dedup_prompt},
            ],
        )
        raw = completion.choices[0].message.content.strip()

        # å»æ‰å¯èƒ½çš„ ```json ...
        clean = raw
        if clean.startswith("```"):
            first_newline = clean.find("\n")
            if first_newline != -1:
                clean = clean[first_newline + 1 :]
            end_fence = clean.rfind("```")
            if end_fence != -1:
                clean = clean[:end_fence]
            clean = clean.strip()

        # æˆªå–ç¬¬ä¸€ä¸ª [ ... ] åŒºé—´
        if "[" in clean and "]" in clean:
            start = clean.find("[")
            end = clean.rfind("]")
            if start != -1 and end != -1 and end > start:
                clean = clean[start : end + 1].strip()

        kept_names = json.loads(clean)
        if not isinstance(kept_names, list):
            raise ValueError("LLM è¾“å‡ºä¸æ˜¯æ•°ç»„")

        # -------- 4. ä¸¥æ ¼å…œåº•ï¼šåªä¿ç•™â€œåŸå§‹ topic åé›†åˆâ€ä¸­çš„å­—ç¬¦ä¸² --------
        original_names = { (t.get("topic") or "").strip() for t in filtered }
        final_names = []
        for name in kept_names:
            if not isinstance(name, str):
                continue
            name = name.strip()
            if name in original_names and name not in final_names:
                final_names.append(name)

        # ä¸‡ä¸€ LLM å…¨åˆ äº†ï¼Œå°±é€€å›åˆ° filtered å…¨éƒ¨ä¿ç•™
        if not final_names:
            return filtered

        # æ ¹æ®æœ€ç»ˆä¿ç•™çš„åå­—ï¼Œä» filtered ä¸­å–å‡ºå¯¹åº”å¯¹è±¡
        name_to_obj = { (t.get("topic") or "").strip(): t for t in filtered }
        result = [name_to_obj[n] for n in final_names if n in name_to_obj]
        return result

    except Exception as e:
        print("âš ï¸ [Topic_cleaning] LLM å»é‡é˜¶æ®µå¤±è´¥ï¼Œå°†è¿”å›æœ¬åœ°è¿‡æ»¤ç»“æœã€‚é”™è¯¯ï¼š", e)
        return filtered


def Topic_Allocation(history, cleaned_topics, top_k_chunks=12):

    # 0. æ„å»ºå¯¹è¯å‘é‡åº“
    if not (isinstance(history, list) and history):
        print("âš ï¸ Topic_Allocation_v2: history ä¸ºç©ºæˆ–æ ¼å¼å¼‚å¸¸ï¼Œå°†è¿”å›ç©ºç»“æœã€‚")
        return []

    store = ConvVectorStore.from_history(history, window_size=40, stride=40)

    results = []

    for topic_obj in cleaned_topics:
        topic_name = topic_obj.get("topic", "").strip()
        if not topic_name:
            continue

        support_examples = topic_obj.get("support_examples", []) or []
        if not isinstance(support_examples, list):
            support_examples = [str(support_examples)]

        # 1) æ„é€  queryï¼štopic å + æ”¯æ’‘ä¾‹å­
        query_text = topic_name + "\n" + "\n".join(support_examples)

        # 2) æ£€ç´¢ä¸è¯¥ topic ç›¸å…³çš„å¯¹è¯ç‰‡æ®µ
        context = store.build_context(query_text, top_k=top_k_chunks)
        if not context.strip():
            # æ²¡æ£€ç´¢åˆ°å°±è·³è¿‡æˆ–ç»™ç©º slots
            results.append({"topic": topic_name, "slots": []})
            continue

        # 3) è®© LLM åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­æŠ½å–äºŒçº§å­ä¸»é¢˜ï¼ˆslotsï¼‰
        #    æ³¨æ„ï¼šä¸Šä¸‹æ–‡ä¸­æ¯è¡Œéƒ½æ˜¯ "[id][role]: content"ï¼Œè®©æ¨¡å‹å¤åˆ¶ id å³å¯
        prompt = f"""è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
            ä»»åŠ¡ï¼šä½ å°†çœ‹åˆ°ä¸€æ®µä¸æŸä¸ªä¸€çº§ä¸»é¢˜é«˜åº¦ç›¸å…³çš„å¯¹è¯åŸæ–‡ç‰‡æ®µã€‚
            ä¸€çº§ä¸»é¢˜ä¸ºï¼š"{topic_name}"

            å¯¹è¯ç‰‡æ®µï¼ˆæ¯è¡Œä»¥ [id][role]: å¼€å¤´ï¼‰ï¼š
            {context}

            å¯¹è¯ç‰‡æ®µä¸­ï¼Œæ¯ä¸€è¡Œçš„æ ¼å¼ç±»ä¼¼ï¼š
            [id][role]: content

            å…¶ä¸­ï¼š
            - idï¼šä¸€ä¸ªæ•´æ•°ï¼Œæ˜¯è¯¥è¡Œå¯¹è¯çš„å”¯ä¸€ç¼–å·ï¼›
            - roleï¼šè¯´è¯äººè§’è‰²æ ‡è®°ï¼›
            - contentï¼šè¿™ä¸€è¡Œå¯¹è¯çš„å…·ä½“æ–‡æœ¬ã€‚

            è¯·ä½ åœ¨ä¸Šè¿°å¯¹è¯ä¸­ï¼ŒæŠ½å–è‹¥å¹²ä¸è¯¥ä¸»é¢˜å¯†åˆ‡ç›¸å…³çš„â€œäºŒçº§å­ä¸»é¢˜â€(slot)ã€‚

            è¾“å‡ºä¸º JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªå¯¹è±¡ï¼ŒåŒ…å«å­—æ®µï¼š
            - "sentence":  
                - å–è‡ªå¯¹è¯ç‰‡æ®µä¸­æŸä¸€è¡Œçš„ content åŸæ–‡ï¼›
                - å¿…é¡»ä¸åŸæ–‡å®Œå…¨ä¸€è‡´ï¼ˆå…è®¸åªåŠ å‡æå°‘é‡å‰åæ ‡ç‚¹ï¼‰ï¼Œä¸è¦æ”¹å†™ã€æ€»ç»“æˆ–ç¿»è¯‘ï¼›
                - ä¸è¦æŠŠå¤šè¡Œåˆå¹¶æˆä¸€å¥ã€‚
            - "slot": 
                - å¯¹è¯¥ sentence çš„ä¸€ä¸ªâ€œäºŒçº§å­ä¸»é¢˜â€åç§°ï¼›
                - å¿…é¡»**éå¸¸ç®€æ´**ï¼Œä¸¥æ ¼æ§åˆ¶åœ¨**ä¸è¶…è¿‡ 6 ä¸ªæ±‰å­—**ï¼ˆä¸è®¡ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰ï¼›
                - ä½¿ç”¨ç®€çŸ­ã€å…·ä½“çš„åè¯çŸ­è¯­æˆ–åŠ¨å®¾çŸ­è¯­ï¼Œä¾‹å¦‚â€œå‚æ•°æ ¡å‡†â€â€œæŒ‡æ ‡åˆ†æâ€â€œé£é™©è¯„ä¼°â€ï¼Œä¸è¦å†™æˆå®Œæ•´å¥å­ï¼›
                - **ç¦æ­¢é‡å¤ä¸€çº§ä¸»é¢˜ "{topic_name}"**ï¼Œä¸èƒ½å‡ºç°â€œ{topic_name}çš„XXXâ€â€œå…³äº{topic_name}XXXâ€ç­‰å½¢å¼ï¼Œä¹Ÿä¸è¦æŠŠ topic åç›´æ¥å†™è¿› slotï¼›
                - **ç¦æ­¢å¹¶åˆ—ç»“æ„**ï¼Œä¾‹å¦‚ï¼š
                    - â€œXXXä¸XXXâ€â€œXXXå’ŒXXXâ€â€œXXXåŠXXXâ€â€œXXXã€XXXâ€ç­‰å½¢å¼éƒ½ä¸å…è®¸ï¼›
                    - å¦‚æœåŸå¥ä¸­åŒ…å«å¤šä¸ªè¦ç‚¹ï¼Œåªé€‰æ‹©ä½ è®¤ä¸ºæœ€æ ¸å¿ƒçš„ä¸€ä¸ªè¦ç‚¹ï¼Œç”¨å•ä¸€æ¦‚å¿µè¡¨è¾¾ï¼›
                - è‹¥æç‚¼å‡ºçš„çŸ­è¯­è¶…è¿‡ 6 ä¸ªæ±‰å­—ï¼Œè¯·è¿›ä¸€æ­¥å‹ç¼©ï¼Œå®å¯çœç•¥ä¿®é¥°è¯ï¼Œä¹Ÿä¸è¦è¶…è¿‡é•¿åº¦é™åˆ¶ã€‚
            - "id": 
                - è¯¥ sentence æ‰€åœ¨è¡Œå‰é¢çš„ idï¼ˆä¸€ä¸ªæ•´æ•°ï¼‰ï¼›
                - å¿…é¡»ç›´æ¥æ¥è‡ªå¯¹è¯ç‰‡æ®µä¸­å¯¹åº”è¡Œçš„ [id]ï¼Œç¦æ­¢è‡ªå·±ç¼–é€ æ–°çš„ idã€‚
            - "sentiment": 
                - è¯¥å¥å­çš„æƒ…ç»ªåˆ†æ•°ï¼ŒèŒƒå›´ä¸º -1 åˆ° 1ï¼š
                    - æ¥è¿‘ 1ï¼šæ˜æ˜¾ç§¯æã€èµç¾ã€ä¹è§‚ã€è¡¨è¾¾æ„Ÿè°¢/æ»¡æ„ï¼›
                    - æ¥è¿‘ -1ï¼šæ˜æ˜¾æ¶ˆæã€æŠ±æ€¨ã€æ²®ä¸§ã€ç„¦è™‘ã€æ‰¹è¯„ï¼›
                    - æ¥è¿‘ 0ï¼šå®¢è§‚é™ˆè¿°ã€æŠ€æœ¯æ€§æè¿°ã€æ™®é€šç–‘é—®ç­‰ä¸­æ€§è¯­æ°”ï¼›
                - å¦‚æœä½ ä¸ç¡®å®šï¼Œå¯ä»¥ä½¿ç”¨ 0 æˆ–æ¥è¿‘ 0 çš„å€¼ã€‚
            - "source":  
                - è¯´è¯äººè§’è‰²æ ‡ç­¾ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²ï¼›
                - å¿…é¡»ç›´æ¥æ¥è‡ªè¯¥è¡Œå¯¹è¯ä¸­æ–¹æ‹¬å·é‡Œçš„ [role]ï¼Œå»æ‰æ–¹æ‹¬å·ååŸæ ·å¡«å†™ï¼›
                    - ä¾‹å¦‚åŸè¡Œæ˜¯ `[12][user]: ...`ï¼Œåˆ™ source å¡« `"user"`ï¼›
                    - åŸè¡Œæ˜¯ `[35][Speaker_A]: ...`ï¼Œåˆ™ source å¡« `"Speaker_A"`ï¼›
                    - åŸè¡Œæ˜¯ `[7][ä¸»æŒäºº]: ...`ï¼Œåˆ™ source å¡« `"ä¸»æŒäºº"`ï¼›
                - ä¸è¦è‡ªè¡Œåˆ›é€ æ–°çš„è§’è‰²åç§°ï¼Œä¹Ÿä¸è¦ç¿»è¯‘æˆ–æ”¹å†™ã€‚
            - "is_question":  
                - å¸ƒå°”å€¼ true/falseï¼›
                - å½“è¯¥ sentence æ˜¯è¯´è¯äººæå‡ºçš„ä¸€ä¸ª**æ˜ç¡®çš„é—®é¢˜ã€è¯·æ±‚å¸®åŠ©æˆ–è§£å†³éœ€æ±‚**æ—¶ï¼Œè¯·å¡« trueï¼ˆæ— è®º source æ˜¯è°ï¼‰ï¼›
                    - ä¾‹å¦‚åŒ…å«æ˜æ˜¾çš„ç–‘é—®ã€å¾æ±‚æ„è§ã€è¯·æ±‚æ“ä½œç­‰ï¼›
                - å…¶ä»–æ‰€æœ‰æƒ…å†µï¼ˆæ™®é€šé™ˆè¿°ã€æƒ…ç»ªè¡¨è¾¾ã€æ€»ç»“ã€å›åº”ç­‰ï¼‰ä¸€å¾‹å¡« falseã€‚

            ã€æŠ½å–è§„åˆ™ä¸çº¦æŸã€‘
            1. åªè€ƒè™‘ä¸ä¸€çº§ä¸»é¢˜ "{topic_name}" æ˜ç¡®ç›¸å…³çš„å¥å­ï¼›
            2. å¯¹äºåŒä¸€ä¸ª idï¼Œåœ¨ç»“æœ JSON ä¸­**æœ€å¤šå‡ºç°ä¸€æ¬¡**ï¼š
                - å³ä½¿ä½ è§‰å¾—è¿™å¥æ¶‰åŠå¤šä¸ªå­ä¸»é¢˜ï¼Œä¹Ÿåªé€‰æ‹©ä½ è®¤ä¸ºâ€œæœ€æ ¸å¿ƒâ€çš„ä¸€ä¸ª slotï¼›
                - ä¸¥ç¦ä¸ºåŒä¸€ä¸ª id è¾“å‡ºå¤šæ¡è®°å½•ã€‚
            3. æ¯æ¡ "sentence" åªèƒ½å¯¹åº”ä¸€ä¸ª "slot"ï¼Œä¸è¦æŠŠåŒä¸€ sentence æ‹†æˆå¤šä¸ªå¯¹è±¡ã€‚
            4. å¦‚æœå¤šå¥è¡¨è¾¾çš„æ˜¯å‡ ä¹å®Œå…¨ç›¸åŒçš„å­ä¸»é¢˜ï¼Œä½ å¯ä»¥åªä¿ç•™ä¿¡æ¯æ›´å®Œæ•´ã€è¯­ä¹‰æ›´æ¸…æ¥šçš„ä¸€å¥ã€‚
            5. ä¸¥æ ¼è¾“å‡º JSON æ•°ç»„ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼Œä¹Ÿä¸è¦ä½¿ç”¨ä»£ç å—æ ‡è®°ã€‚

            ç¤ºä¾‹è¾“å‡ºï¼ˆä»…ç¤ºæ„ï¼Œæ³¨æ„å®é™…å†…å®¹åº”æ¥è‡ªå½“å‰å¯¹è¯ç‰‡æ®µï¼‰ï¼š
            [
              {{"sentence": "æˆ‘åº”è¯¥æ€ä¹ˆæ”¹è¿› SWMM æ¨¡å‹çš„å‚æ•°æ ¡å‡†ï¼Ÿ", "slot": "SWMM å‚æ•°æ ¡å‡†æ–¹æ³•", "id": 45, "sentiment": 0.2, "source": "user", "is_question": true}},
              {{"sentence": "æœ¬æ¬¡ä¸»è¦è®¨è®º DrainScope ä¸­çš„æ’æ°´é£é™©æŒ‡æ ‡å¯è§†åˆ†æã€‚", "slot": "æŒ‡æ ‡å¯è§†åˆ†æ", "id": 52, "sentiment": -0.1, "source": "bot", "is_question": false}}
            ]
        """

        completion = openai.chat.completions.create(
            model="gpt-4o",
            temperature=0.2,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€åå¯¹è¯åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä»å¯¹è¯ä¸­æŠ½å–ç»“æ„åŒ–ä¸»é¢˜ä¿¡æ¯ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )

        raw = completion.choices[0].message.content.strip()

        # 4) è§£æ JSONï¼ˆå’Œä½ æ¸…æ´—é‚£è¾¹ä¸€æ ·çš„é²æ£’å¥—è·¯ï¼‰
        clean = raw
        if clean.startswith("```"):
            first_newline = clean.find("\n")
            if first_newline != -1:
                clean = clean[first_newline + 1:]
            end_fence = clean.rfind("```")
            if end_fence != -1:
                clean = clean[:end_fence]
            clean = clean.strip()

        if "[" in clean and "]" in clean:
            start = clean.find("[")
            end = clean.rfind("]")
            if start != -1 and end != -1 and end > start:
                clean = clean[start:end + 1].strip()

        try:
            slots = json.loads(clean)
        except json.JSONDecodeError as e:
            print(f"âš ï¸ [Topic_Allocation_v2] è§£æ slots JSON å¤±è´¥ï¼Œtopic={topic_name}, é”™è¯¯: {e}")
            slots = []

        # 5) è§„èŒƒåŒ– & æ’åºï¼ˆæŒ‰ id æ—¶é—´é¡ºåºï¼‰
        norm_slots = []
        seen_ids = set()

        for s in slots:
            if not isinstance(s, dict):
                continue
            sent = s.get("sentence", "").strip()
            slot_name = s.get("slot", "").strip()
            try:
                sid = int(s.get("id"))
            except Exception:
                continue
            sentiment = s.get("sentiment", 0.0)  # é»˜è®¤ 0ï¼Œå¦‚æœç¼ºå¤±

            # è§£æ sourceï¼šç›´æ¥ä¿ç•™æ¨¡å‹ç»™å‡ºçš„è§’è‰²æ ‡ç­¾
            raw_source = s.get("source")
            if raw_source is None:
                source = ""
            else:
                source = str(raw_source).strip()

            # è§£æ is_questionï¼šç›¸ä¿¡æ¨¡å‹çš„å¸ƒå°”å€¼ï¼Œä¸å†å¼ºåˆ¶ä¾èµ– source
            raw_iq = s.get("is_question")
            if isinstance(raw_iq, bool):
                is_question = raw_iq
            elif isinstance(raw_iq, str):
                is_question = raw_iq.strip().lower() == "true"
            else:
                is_question = False

            if not sent or not slot_name:
                continue

            # âœ… å»é‡ï¼šåŒä¸€ä¸ª topic é‡Œï¼Œä¸€ä¸ª id åªä¿ç•™ä¸€æ¬¡
            if sid in seen_ids:
                # å¦‚æœä½ ä»¥åæƒ³æ¢ç­–ç•¥ï¼ˆæ¯”å¦‚é€‰æƒ…ç»ªæ›´å¼ºçš„é‚£æ¡ï¼‰ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ”¹é€»è¾‘
                continue
            seen_ids.add(sid)

            norm_slots.append({
                "sentence": sent,
                "slot": slot_name,
                "id": sid,
                "sentiment": sentiment,  # æ–°å¢å­—æ®µ
                "source": source,
                "is_question": is_question
            })

        norm_slots.sort(key=lambda x: x["id"])

        # 6) ä¸¥æ ¼æŒ‰ä½ è¦æ±‚çš„æ ¼å¼å†™å…¥ç»“æœ
        results.append({
            "topic": topic_name,
            "slots": norm_slots
        })

    return results

def build_local_window(history, center_id, window_size=8):
    """
    æŒ‰ id åœ¨ history é‡Œæˆªä¸€æ®µçª—å£ï¼š
    [center_id - window_size, center_id + window_size]
    è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ŒæŒ‰å¯¹è¯é¡ºåºæ‹¼å¥½ï¼Œä¾› LLM åˆ¤æ–­ã€‚
    """
    # 1. å…ˆæŒ‰ id æ’ä¸ªåºï¼Œç¡®ä¿é¡ºåºä¸€è‡´
    sorted_msgs = sorted(history, key=lambda m: m.get("id", 0))

    # 2. æ‰¾åˆ° center_id å¯¹åº”ä½ç½®
    center_idx = None
    for i, m in enumerate(sorted_msgs):
        if m.get("id") == center_id:
            center_idx = i
            break
    if center_idx is None:
        return ""

    start = max(0, center_idx - window_size)
    end = min(len(sorted_msgs), center_idx + window_size + 1)
    window_msgs = sorted_msgs[start:end]

    # 3. æ ¼å¼åŒ–æˆç±»ä¼¼ï¼š
    # [12][user]: xxx
    # [13][bot]: yyy
    lines = []
    for m in window_msgs:
        mid = m.get("id")
        role = m.get("role") or m.get("from") or "user"
        text = (m.get("content") or m.get("text") or "").strip()
        lines.append(f"[{mid}][{role}]: {text}")
    return "\n".join(lines)


def ask_if_resolved(history, slot_obj):
    """
    history: åŸå§‹å¯¹è¯ [{id, role, content}, ...]
    slot_obj: {"sentence", "slot", "id", "source", ...}
    è¿”å› True/False
    """
    sid = slot_obj["id"]
    sentence = slot_obj["sentence"]
    slot_name = slot_obj["slot"]

    local_ctx = build_local_window(history, sid, window_size=8)
    if not local_ctx.strip():
        return False

    prompt = f"""ä½ æ˜¯ä¸€åå¯¹è¯åˆ†æåŠ©æ‰‹ã€‚
        ç°åœ¨ç»™ä½ ä¸€æ®µå¯¹è¯ç‰‡æ®µï¼Œä»¥åŠå…¶ä¸­ä¸€æ¡â€œæŸä½è¯´è¯äººæå‡ºçš„é—®é¢˜/éœ€æ±‚â€æ‰€åœ¨çš„å¥å­ã€‚

        å¯¹è¯ç‰‡æ®µå¦‚ä¸‹ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰ï¼š
        {local_ctx}

        å…¶ä¸­ï¼Œåœ¨ id = {sid} çš„è¿™ä¸€å¥ä¸­ï¼Œè¯¥è¯´è¯äººæå‡ºäº†ä¸€ä¸ªå­ä¸»é¢˜/é—®é¢˜ï¼š
        "{sentence}"
        å­ä¸»é¢˜åç§°ä¸ºï¼š"{slot_name}"

        è¯·ä½ åªæ ¹æ®ä¸Šé¢çš„å¯¹è¯ç‰‡æ®µï¼Œåˆ¤æ–­è¿™ä¸ªé—®é¢˜/éœ€æ±‚åœ¨åç»­å¯¹è¯ä¸­æ˜¯å¦å·²ç»åœ¨å¯¹è¯ä¸­è¢«åŸºæœ¬å›åº”æˆ–è§£å†³ã€‚
        è¿™é‡Œçš„â€œè§£å†³â€æŒ‡ï¼š
        - æœ‰å‘è¨€ç»™å‡ºäº†æ˜ç¡®ã€å…·ä½“ã€ä¸è¯¥é—®é¢˜é«˜åº¦å¯¹åº”çš„å›ç­”ã€è§£é‡Šæˆ–å¯æ‰§è¡Œæ–¹æ¡ˆï¼›
        - ä¸è¦æ±‚æé—®è€…æ˜¾å¼è¯´â€œè°¢è°¢ï¼Œè§£å†³äº†â€ï¼Œä½†è§£å†³è€…çš„å›åº”åº”è¯¥è¦†ç›–äº†æ ¸å¿ƒç–‘é—®ã€‚

        å¦‚æœè§£å†³è€…åªæ˜¯ç®€å•å®‰æ…°ã€æ¨¡ç³Šå›åº”ã€éƒ¨åˆ†ç­”å¤ï¼Œæˆ–è€…æ²¡æœ‰æ˜æ˜¾é’ˆå¯¹è¯¥é—®é¢˜çš„å›ç­”ï¼Œéƒ½è§†ä¸ºâ€œæœªè§£å†³â€ã€‚

        è¯·ä¸¥æ ¼è¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«å¤šä½™æ–‡å­—ï¼Œä¸è¦ä½¿ç”¨ä»£ç å—ï¼š
        ä¾‹å¦‚ï¼š
        {{"resolved": true}}
        æˆ–
        {{"resolved": false}}
        """

    completion = openai.chat.completions.create(
        model="gpt-4o",
        temperature=0.2,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸¥è°¨çš„å¯¹è¯åˆ†æåŠ©æ‰‹ï¼Œåªè¾“å‡ºç»“æ„åŒ– JSONã€‚"},
            {"role": "user", "content": prompt}
        ],
    )

    raw = completion.choices[0].message.content.strip()

    # ç®€å•é²æ£’è§£æ
    try:
        # æœ‰äº›æ¨¡å‹ä¼šè¾“å‡ºå‰åç©ºè¡Œæˆ–å…¶å®ƒä¸œè¥¿ï¼Œå°±ç²—ç•¥æˆªä¸€ä¸‹ {...} éƒ¨åˆ†
        start = raw.find("{")
        end = raw.rfind("}")
        if start != -1 and end != -1 and end > start:
            raw = raw[start:end+1]
        data = json.loads(raw)
        val = data.get("resolved")
        if isinstance(val, bool):
            return val
        if isinstance(val, str):
            return val.strip().lower() == "true"
    except Exception as e:
        print(f"âš ï¸ ask_if_resolved è§£æå¤±è´¥ï¼Œé»˜è®¤ False: {e}, raw={raw}")
    return False

def refine_slot_resolution(history, topics_with_slots, 
                           max_slots=50):
    """
    history: [{id, role, content}, ...]
    topics_with_slots: Topic_Allocation çš„è¾“å‡ºï¼š
      [
        {"topic": "...", "slots": [ {...}, {...} ]},
        ...
      ]

    è¿”å›ï¼šç»“æ„ç›¸åŒï¼Œä½†æ¯ä¸ª slot çš„ resolved å­—æ®µç»è¿‡äºŒé˜¶æ®µ LLM å¤æ ¸ã€‚
    max_slots: æœ€å¤šå¤æ ¸å¤šå°‘ä¸ª slotï¼Œé˜²æ­¢çˆ†è°ƒç”¨ã€‚
    æ–°é€»è¾‘ï¼šä¸åŒºåˆ† sourceï¼Œåªè¦ is_question == True çš„ slot éƒ½ä¼šå°è¯•åˆ¤æ–­æ˜¯å¦å·²è§£å†³ã€‚
    """
    refined = []
    # ç»Ÿè®¡ä¸€ä¸‹å·²ç»å¤æ ¸äº†å¤šå°‘ä¸ªï¼Œé¿å…å¯¹è¯ç‰¹åˆ«é•¿æ—¶å¤ªè´µ
    checked = 0

    for topic_obj in topics_with_slots:
        topic_name = topic_obj.get("topic")
        slots = topic_obj.get("slots", []) or []
        new_slots = []

        for s in slots:
            s2 = dict(s)

            # 1) ä¸æ˜¯é—®å¥ï¼ˆis_question != Trueï¼‰ï¼Œä¸éœ€è¦åˆ¤æ–­è§£å†³ä¸å¦
            if not s2.get("is_question", False):
                # æ˜ç¡®æ ‡è®°ï¼šä¸æ˜¯é—®é¢˜ï¼Œè‡ªç„¶ä¸å­˜åœ¨â€œå·²è§£å†³â€
                s2["resolved"] = False
                new_slots.append(s2)
                continue

            # 2) æ˜¯é—®å¥ï¼Œä½†å·²ç»è¶…è¿‡è°ƒç”¨ä¸Šé™ï¼Œé¿å…èŠ±å¤ªå¤š token
            if checked >= max_slots:
                # è¶…ä¸Šé™ï¼Œä¸å†è°ƒç”¨ LLMï¼Œä¿ç•™ is_question=True ä½† resolved é»˜è®¤ False
                s2["resolved"] = False
                new_slots.append(s2)
                continue

            final_resolved = ask_if_resolved(history, s2)
            s2["resolved"] = final_resolved
            checked += 1

            new_slots.append(s2)


        refined.append({
            "topic": topic_name,
            "slots": new_slots,
        })

    return refined

def pipeline_on_messages(messages):

    # 1. å¦‚æœæ²¡æœ‰ idï¼Œå°±é¡ºæ‰‹è¡¥ä¸€éé€’å¢ idï¼Œä¿è¯åé¢èƒ½ç”¨ id åšæ—¶é—´è½´
    normalized_messages = []
    for idx, m in enumerate(messages, start=1):
        normalized_messages.append({
            "id": m.get("id", idx),
            "role": m.get("role") or m.get("from") or "user",
            "content": (m.get("content") or m.get("text") or "").strip()
        })

    # ğŸ§  ç¬¬ä¸€æ­¥ï¼šè¯­ä¹‰é¢„æ‰«æï¼ˆç²—æŠ½ï¼‰
    pre_scan_result = Semantic_pre_scanning(normalized_messages)
    # pre_scan_result ç»“æ„åº”è¯¥å°±æ˜¯ä½ ä¹‹å‰ all_results çš„é‚£ä¸€ç±» topic/slots åˆ—è¡¨

    # ğŸ§¹ ç¬¬äºŒæ­¥ï¼šä¸»é¢˜æ¸…æ´— / å»å™ª / åˆå¹¶
    cleaned_topics = Topic_cleaning(normalized_messages, pre_scan_result)

    # ğŸ¯ ç¬¬ä¸‰æ­¥ï¼šæŠŠ slot é‡æ–°å¯¹é½åˆ°å…·ä½“çš„æ¶ˆæ¯ / turn ä¸Š
    allocated_topics = Topic_Allocation(normalized_messages, cleaned_topics)


    # ğŸŒˆ ç¬¬å››æ­¥ï¼šæ˜¯å¦è§£å†³é—®é¢˜
    refined_result = refine_slot_resolution(messages, allocated_topics,
                                        max_slots=80,   # æŒ‰ä½ èƒ½æ¥å—çš„è°ƒç”¨é‡è°ƒ
                                        only_user=True)

    # ğŸ¨ ç¬¬äº”æ­¥ï¼šç»™æ¯ä¸ª topic åˆ†é…é¢œè‰²
    colored_results = assign_colors(refined_result)

    # â›°ï¸ ç¬¬å…­æ­¥ï¼šæŒ‰æ—¶é—´è½´åˆ‡æ®µï¼Œç»™å‰ç«¯ç”»å¸¦çŠ¶å›¾
    segmented_results = segment_by_timeline(colored_results)

    return segmented_results

# ç”Ÿæˆ embedding
def get_embedding(text):
    emb = openai.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return np.array(emb.data[0].embedding, dtype="float32")

# ===== åˆå§‹åŒ– Memoryï¼ˆå¯ä»¥æ¢æˆæ•°æ®åº“ï¼‰ =====
user_memory_db = {}  # ç¤ºä¾‹: {user_id: {key: value}}

# ===== Memoryæ“ä½œå‡½æ•° =====
# å¾—åˆ°ç”¨æˆ·Memory
def get_user_memory(user_id):
    """
    è¿”å›æ•´ç†å¥½çš„æ–‡æœ¬ï¼Œç”¨äºæ‹¼æ¥åˆ°prompt
    """
    if user_id not in user_memory_db:
        return ""
    memory = user_memory_db[user_id]
    return "\n".join([f"{k}: {v}" for k, v in memory.items()])

# æ›´æ–°ç”¨æˆ·Memory
def update_user_memory(user_id, key, value):
    global user_memory_db
    if user_id not in user_memory_db:
        user_memory_db[user_id] = {}
    
    # å¦‚æœ key å·²å­˜åœ¨ï¼Œåšâ€œè¿½åŠ â€è€Œä¸æ˜¯è¦†ç›–
    if key in user_memory_db[user_id]:
        old_value = user_memory_db[user_id][key]
        if value not in old_value:
            user_memory_db[user_id][key] = f"{old_value}; {value}"
    else:
        user_memory_db[user_id][key] = value
    
    user_memory_db[user_id]['last_updated'] = str(datetime.now())

    print(f"Memoryæ›´æ–°ï¼š{user_id} - {key} - {value}")
    print("Memoryåº“:" ,user_memory_db)

def extract_memory_from_text(user_id, new_sentence):
    """
    è°ƒç”¨ GPT è‡ªåŠ¨ä»æ–‡æœ¬ä¸­æŠ½å–å…³é”®ä¿¡æ¯ï¼ˆå¦‚åå­—ã€å…´è¶£ã€åå¥½ç­‰ï¼‰
    å¹¶æ›´æ–° Memory
    """
    prompt = f"""
    è¯·ä»ä¸‹é¢çš„æ–‡æœ¬ä¸­æå–ç”¨æˆ·å¯èƒ½æƒ³è®°ä½çš„å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬å§“åã€å…´è¶£ã€çˆ±å¥½ã€å­¦ä¹ ç›®æ ‡ç­‰ã€‚
    è¾“å‡ºå¿…é¡»æ˜¯æ ‡å‡† JSON å¯¹è±¡ï¼Œä¸¥ç¦åŒ…å«ä»£ç å—æ ‡è®°ï¼ˆå¦‚```jsonï¼‰æˆ–å¤šä½™æ–‡å­—ï¼Œé”®åéšæ„ä½†è¦èƒ½æè¿°ä¿¡æ¯ï¼Œå¦‚ï¼š
    {{
        "name": "Alice",
        "favorite_language": "Python"
    }}

    æ–‡æœ¬å†…å®¹ï¼š
    {new_sentence}
    """

    completion = openai.chat.completions.create(
        model="gpt-4o",
        temperature=0,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€åä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ]
    )
    result = (completion.choices[0].message.content)
    # æ›´æ–°è§£æç»“æœåˆ°Memory
    try:
        memory_data = json.loads(result)
        for key, value in memory_data.items():
            update_user_memory(user_id, key, value)
    except json.JSONDecodeError:
        # å‡ºç°è§£æé”™è¯¯æ—¶å¯ä»¥å¿½ç•¥æˆ–è®°å½•æ—¥å¿—
        print("Memory JSONè§£æå¤±è´¥:", result)

# ===== GPT + Memory + RAGå‡½æ•° =====
def talk_to_chatbot(user_id, content, source, history_msgs, top_k=3):
    global index

    # 1. å…ˆæ£€ç´¢Memory
    memory_context = get_user_memory(user_id)

    # 2. ä¸ºç”¨æˆ·è¾“å…¥ç”Ÿæˆ embedding
    query_emb = get_embedding(content).reshape(1, -1)

    # 3. æ£€ç´¢ç›¸å…³ Memory
    related_context = ""
    if index is not None and memory_context:
        D, I = index.search(query_emb, top_k)
        # ç®€å•ç¤ºä¾‹ï¼šç”¨ç´¢å¼•å¯¹åº”çš„ Memory è¡Œï¼ˆå‡è®¾ memory_text åˆ†è¡Œå­˜å‚¨ï¼‰
        memory_lines = memory_context.split("\n")
        related_context = "\n".join([memory_lines[i] for i in I[0] if i < len(memory_lines)])

    # 4.å…ˆæŠŠå†å²å¯¹è¯æ•´ç†æˆæ–‡æœ¬
    history_text = "\n".join(
    [f"{msg.get('from', 'user').capitalize()}: {msg.get('text', '')}" for msg in history_msgs])

    # 5. ç»„ç»‡ promptï¼ŒæŠŠ Memory å’Œ RAG æ£€ç´¢å†…å®¹éƒ½æ‹¼è¿›å»
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€åå¯¹è¯åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä¸ç”¨æˆ·è¿›è¡Œæ²Ÿé€š, è¯·æ ¹æ®ç”¨æˆ·çš„è¾“å…¥ï¼Œåˆç†å›ç­”ï¼Œå¹¶ä¿æŒæ²Ÿé€šè¿è´¯ã€‚"},
        {"role": "user", "content": f"""
        ä¸‹é¢æ˜¯ä¸æœ¬é—®é¢˜ç›¸å…³çš„å†å²å¯¹è¯ï¼š{history_text}
        ç”¨æˆ·ä¿¡æ¯ï¼š
        {memory_context}
        ç›¸å…³ Memory æ£€ç´¢å†…å®¹ï¼š
        {related_context}
        ç°åœ¨ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š
        {content}"""}
        ]
    
    # 6. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›å¤ 
    completion = openai.chat.completions.create(
        model="gpt-4o",
        temperature=0.5,
        messages=messages,
        )
    result = (completion.choices[0].message.content)

    # 7. æ›´æ–° Memoryï¼ˆé•¿æœŸè®°å¿†ï¼‰åˆ° FAISS
    if index is not None and memory_context:
        memory_text = "\n".join([f"{k}: {v}" for k, v in user_memory_db[user_id].items()])
        emb = get_embedding(memory_text).reshape(1, -1)
        index.add(emb)

    # 8. è‡ªåŠ¨æŠ½å–æœ€æ–° Memory ä¿¡æ¯
    extract_memory_from_text(user_id, content)

    return result

def create_theme_variables(result_dict):
    for theme, questions in result_dict.items():
        # åˆ›å»ºåˆæ³•å˜é‡åï¼ˆç§»é™¤éæ³•å­—ç¬¦ï¼‰
        var_name = theme.replace(" ", "_").replace("ï¼š", "").replace("-", "_")
        globals()[var_name] = questions
    
    print(f"{var_name} = {questions}")  # å¯é€‰ï¼šæ‰“å°å‡ºæ¥
