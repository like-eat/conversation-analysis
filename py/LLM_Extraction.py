import os
import re
import json
import openai
import faiss
import numpy as np
from datetime import datetime
from Methods import *
openai.api_key = "sk-3fk05T3Cme02GzUGBc56BaBfA7Ff4dCa9d7dE5AeA689913c"

openai.base_url = "https://api.gpt.ge/v1/"
openai.default_headers = {"x-foo": "true"}

# ===== 1. åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼ˆFAISSï¼‰ =====
dimension = 1536  # OpenAI text-embedding-3-small è¾“å‡ºå‘é‡ç»´åº¦
index = faiss.IndexFlatL2(dimension)  # L2 è·ç¦»ç´¢å¼•

# ====== å·¥å…·ï¼šæŠŠå¯¹è¯åˆ‡æˆçª—å£ ======
def build_conv_chunks(history, window_size=20, stride=20):
    """
    æŠŠæ•´è½®å¯¹è¯æŒ‰ id æ’åºåï¼Œåˆ‡æˆä¸€æ®µæ®µ chunkï¼Œ
    æ¯æ®µåŒ…å« window_size æ¡å¯¹è¯ï¼ˆå¯é‡å ï¼Œæ­¥é•¿ strideï¼‰ã€‚
    æ¯ä¸ª chunk ç»“æ„ï¼š{start_id, end_id, text}
    """
    # å…ˆæŒ‰ id æ’åº
    history_sorted = sorted(
        [m for m in history if isinstance(m, dict) and m.get("content")],
        key=lambda x: x.get("id", 0)
    )

    chunks = []
    n = len(history_sorted)
    if n == 0:
        return chunks

    idx = 0
    while idx < n:
        sub = history_sorted[idx: idx + window_size]
        if not sub:
            break
        text = "\n".join(
            f"[{m['id']}][{m['role']}]: {m['content'].strip()}"
            for m in sub if m.get("content")
        )
        chunks.append({
            "start_id": sub[0]["id"],
            "end_id": sub[-1]["id"],
            "text": text,
        })
        if idx + window_size >= n:
            break
        idx += stride

    return chunks

def embed_texts(
        text_list, 
        model="text-embedding-3-large",
        max_chars_per_item=2000,\
        max_batch_chars=6000,
        max_batch_items=8 ):
    """
    è¾“å…¥: [str, str, ...]
    è¾“å‡º: [np.array(d), ...]
    """
    processed = []
    for text in text_list:
        t = str(text)
        if len(t) > max_chars_per_item:
            t = t[:max_chars_per_item]
        processed.append(t)

    embs = []
    batch = []
    batch_chars = 0

    def _flush_batch(batch_texts):
        if not batch_texts:
            return []
        resp = openai.embeddings.create(
            model=model,
            input=batch_texts,
        )
        return [np.array(d.embedding, dtype="float32") for d in resp.data]

    for t in processed:
        # å¦‚æœå†åŠ è¿™ä¸€æ¡ä¼šè¶…å‡ºé™åˆ¶ï¼Œå°±å…ˆæŠŠå½“å‰ batch å‘å‡ºå»
        if (batch and (batch_chars + len(t) > max_batch_chars
                       or len(batch) >= max_batch_items)):
            embs.extend(_flush_batch(batch))
            batch = []
            batch_chars = 0

        batch.append(t)
        batch_chars += len(t)

    # åˆ«å¿˜äº† flush æœ€åä¸€ä¸ª batch
    if batch:
        embs.extend(_flush_batch(batch))

    return embs

# å‘é‡æ•°æ®åº“ç±»
class ConvVectorStore:
    def __init__(self, chunks, index, emb_dim):
        self.chunks = chunks          # list[dict], æ¯ä¸ªæœ‰ start_id/end_id/text
        self.index = index            # faiss index
        self.emb_dim = emb_dim

    @classmethod
    def from_history(cls, history, window_size=20, stride=20):
        """
        ä»ä¸€æ•´è½®å¯¹è¯æ„å»ºå‘é‡åº“ï¼š
        - åˆ‡ chunk
        - å¯¹æ¯ä¸ª text åš embedding
        - ç”¨ FAISS å»º IndexFlatIP
        """
        chunks = build_conv_chunks(history, window_size=window_size, stride=stride)
        if not chunks:
            # ç©º history
            index = None
            return cls([], index, 0)

        texts = [c["text"] for c in chunks]
        embs = embed_texts(texts)  # list[np.array]
        emb_dim = embs[0].shape[0]

        emb_matrix = np.stack(embs, axis=0)  # (N, d)
        index = faiss.IndexFlatIP(emb_dim)   # å†…ç§¯ï¼Œç›¸å½“äºä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆéœ€å…ˆå½’ä¸€åŒ–çš„è¯å¯ä»¥å†å°è£…ï¼‰
        index.add(emb_matrix)

        # æŠŠ embedding ä¸€èµ·å­˜ä½æ–¹ä¾¿ debugï¼ˆä¸ä¸€å®šå¿…é¡»ï¼‰
        for c, e in zip(chunks, embs):
            c["embedding"] = e

        return cls(chunks, index, emb_dim)

    def search_by_text(self, query_text, top_k=8):
        """
        ç»™ä¸€æ®µ query æ–‡æœ¬ï¼Œè¿”å›æœ€ç›¸å…³çš„ top_k ä¸ª chunkï¼ˆå·²ç»æŒ‰ç›¸ä¼¼åº¦æ’å¥½ï¼‰
        """
        if not self.chunks or self.index is None:
            return []

        q_emb = embed_texts([query_text])[0].reshape(1, -1).astype("float32")  # (1, d)
        D, I = self.index.search(q_emb, top_k)  # I: (1, top_k)

        idxs = I[0]
        selected = [self.chunks[i] for i in idxs if 0 <= i < len(self.chunks)]
        # æŒ‰æ—¶é—´é¡ºåºæ’ä¸€ä¸‹ï¼Œæ–¹ä¾¿ä½ åé¢ç”¨
        selected.sort(key=lambda c: c["start_id"])
        return selected

    def build_context(self, query_text, top_k=5):
        """
        æŠŠæ£€ç´¢åˆ°çš„ top_k chunks æ‹¼æˆä¸€ä¸ªä¸Šä¸‹æ–‡æ–‡æœ¬ï¼Œç”¨æ¥ä¸¢ç»™ LLM çœ‹ã€‚
        """
        selected = self.search_by_text(query_text, top_k=top_k)
        if not selected:
            return ""

        ctx = "\n\n".join(
            f"[å¯¹è¯ç‰‡æ®µ {c['start_id']}~{c['end_id']}]\n{c['text']}"
            for c in selected
        )
        return ctx


def Semantic_pre_scanning(history):
    if isinstance(history, dict):
        history = history.get("content", "")
    else:
        history = str(history)

    prompt = f"""è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
        ä»»åŠ¡ï¼šè¯·ä½ åŸºäºä»¥ä¸‹çš„è¯­ä¹‰æ‘˜è¦ï¼Œæ ¹æ®è¿™æ®µæ‘˜è¦ç”Ÿæˆå¯èƒ½å­˜åœ¨çš„ä¸€çº§å¯¹è¯ä¸»é¢˜ã€‚
        è¯­ä¹‰æ‘˜è¦ï¼š{history}

        è¾“å‡ºè¦æ±‚ï¼š
        1. ä¸¥æ ¼è¾“å‡ºä¸ºæ ‡å‡† JSON æ•°ç»„ï¼Œç¦æ­¢ä»£ç å—æ ‡è®°å’Œå¤šä½™æ–‡å­—ã€‚
        2. æ¯ä¸ªä¸»é¢˜åŒ…å«å­—æ®µï¼š
        - "topic": ä¸»é¢˜åç§°ï¼ˆåè¯æˆ–åè¯çŸ­è¯­ï¼Œä¸»é¢˜å¿…é¡»å…·æœ‰æ™®éæ€§ï¼Œå¹¶ä¸”ä¸æ˜“è¿‡äºå…·ä½“ï¼Œæ–¹ä¾¿æ‰©å……å‡ºæ›´å¤šçš„å­ä¸»é¢˜ï¼‰
        - "support_count": ä»æ‘˜è¦ä¸­å¯ä½è¯è¯¥ä¸»é¢˜çš„è¦ç‚¹æ•°é‡ï¼ˆç²—ç•¥ä¼°è®¡ï¼Œæ•´æ•°ï¼‰
        - "support_examples": 1~3 æ¡æ‘˜è‡ªæ‘˜è¦çš„çŸ­è¯æ®ç‰‡æ®µï¼ˆå¿…é¡»æ˜¯åŸæ–‡å­ä¸²ï¼‰
        3. ä¸»é¢˜åº”äº’ç›¸åŒºåˆ†ã€æ¶µç›–ä¸»è¦è¯­ä¹‰æ–¹å‘ï¼›å¦‚æ— è¶³å¤Ÿè¯æ®ï¼Œä¸è¦è‡†é€ ã€‚
        æ­£ç¡®è¾“å‡ºç¤ºä¾‹ï¼ˆç¤ºæ„ï¼‰ï¼š
        [
            {{
            "topic": "äººå·¥æ™ºèƒ½",
            "support_count": 3,
            "support_examples": ["â€¦åŸæ–‡ç‰‡æ®µAâ€¦", "â€¦åŸæ–‡ç‰‡æ®µBâ€¦"]
            }},
            {{
            "topic": "åŸå¸‚æ’æ°´ä»¿çœŸ",
            "support_count": 2,
            "support_examples": ["â€¦åŸæ–‡ç‰‡æ®µCâ€¦"]
            }}
        ]
    """
    completion = openai.chat.completions.create(
        model="gpt-4o",
        temperature=0.2,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€åæ–‡æœ¬èšç±»åˆ†æå¸ˆï¼Œæ“…é•¿ä»å¯¹è¯ä¸­æå–å‡ºå¯¹è¯ä¸»é¢˜ã€‚"},
            {"role": "user", "content": prompt }
            ],)
    try:
        result = json.loads(completion.choices[0].message.content)
    except json.JSONDecodeError:
        result = []

    return result

def Topic_cleaning(history,topic_description,min_support=2):

    # 1) æ„å»ºå‘é‡åº“ï¼ˆhistory åº”è¯¥æ˜¯ list[dict] çš„å¯¹è¯å†å²ï¼‰
    if isinstance(history, list) and history:
        store = ConvVectorStore.from_history(history, window_size=40, stride=40)

        # 2) æŠŠ topic_description ä½œä¸º queryï¼Œæ£€ç´¢ç›¸å…³ç‰‡æ®µ
        if isinstance(topic_description, (list, tuple)):
            topic_json_str = json.dumps(topic_description, ensure_ascii=False)
        else:
            topic_json_str = str(topic_description)

        # ä»å‘é‡åº“é‡Œå– top_k ä¸ªç‰‡æ®µï¼Œæ‹¼æˆä¸Šä¸‹æ–‡
        history_context = store.build_context(topic_json_str)

    else:
        # å¦‚æœ history ä¸æ˜¯æ ‡å‡† list[dict]ï¼ˆä¾‹å¦‚å·²ç»æ˜¯å­—ç¬¦ä¸²ï¼‰ï¼Œ
        # å°±ç›´æ¥å½“æˆä¸Šä¸‹æ–‡ä½¿ç”¨ï¼ˆé€€åŒ–æˆé RAGï¼Œä¿è¯å…¼å®¹ï¼‰
        if isinstance(topic_description, (list, tuple)):
            topic_json_str = json.dumps(topic_description, ensure_ascii=False)
        else:
            topic_json_str = str(topic_description)

        history_context = str(history)

    prompt = f"""è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
        ä»»åŠ¡ï¼šå¯¹ä¸‹é¢çš„â€œä¸»é¢˜åˆ—è¡¨â€è¿›è¡Œæ¸…æ´—ä¸åˆå¹¶ã€‚
        è¯­ä¹‰æ‘˜è¦ï¼ˆä¾›ç›¸å…³æ€§å‚è€ƒï¼‰ï¼š
        {history_context}
        ä¸»é¢˜åˆ—è¡¨ï¼ˆJSONæ•°ç»„ï¼Œå…ƒç´ å¯èƒ½åŒ…å« support_count/support_examplesï¼Œä¹Ÿå¯èƒ½ä¸åŒ…å«ï¼‰ï¼š{topic_json_str}

        æ¸…æ´—è§„åˆ™ï¼š
        1. ç›¸å…³æ€§ä¸è¯æ®ï¼š
        - å¦‚å­˜åœ¨ "support_count"ï¼Œè¦æ±‚ support_count â‰¥ {min_support}ï¼›
        - å¦‚ä¸å­˜åœ¨ "support_count"ï¼Œè¯·åŸºäºè¯­ä¹‰ä¸æ‘˜è¦æ˜¯å¦åŒ¹é…æ¥åˆ¤å®šæ˜¯å¦ä¿ç•™ï¼ˆä¿å®ˆç­–ç•¥ï¼Œå®ç¼ºæ¯‹æ»¥ï¼‰ã€‚
        2. å»é‡åˆå¹¶ï¼š
        - åˆå¹¶è¯­ä¹‰é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„ä¸»é¢˜ï¼Œåˆå¹¶ååç§°æ›´æ¸…æ™°ã€æè¿°æ›´å…·ä½“ï¼›
        - å¦‚å­˜åœ¨å¤šä¸ª support_countï¼Œè¯·ç´¯åŠ æˆ–å–æœ€å¤§å€¼ï¼›
        - "support_examples" åˆå¹¶åä¿ç•™ 1~3 æ¡ä»£è¡¨æ€§åŸæ–‡å­ä¸²ã€‚
        3. ç©ºæ³›ä¸»é¢˜å‰”é™¤ï¼šå¦‚ä»…å‡ºç°â€œç ”ç©¶/é—®é¢˜/ç°çŠ¶/å‘å±•/è®¨è®ºâ€ç­‰ã€‚
        4. å­—æ®µç»“æ„ï¼š
        - è‹¥è¾“å…¥å…ƒç´ å«æœ‰ "support_count"/"support_examples"ï¼Œè¯·ä¿ç•™ï¼›
        - è‹¥è¾“å…¥å…ƒç´ æ²¡æœ‰è¿™äº›å­—æ®µï¼Œä¸è¦æ–°å¢ï¼ˆä¿æŒä¸åŸç»“æ„ä¸€è‡´ï¼‰ã€‚
        è¾“å‡ºè¦æ±‚ï¼š
        - ä¸¥æ ¼è¾“å‡ºæ ‡å‡† JSON æ•°ç»„ï¼Œä¸å¾—å‡ºç°ä»£ç å—æ ‡è®°æˆ–å¤šä½™æ–‡å­—ã€‚
    """
    completion = openai.chat.completions.create(
        model="gpt-4o",
        temperature=0.2,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€åæ–‡æœ¬åˆ†æå¸ˆï¼Œæ“…é•¿ä¸»é¢˜å»é‡ã€ç›¸å…³æ€§åˆ¤å®šå’Œè¯æ®æ£€æŸ¥ã€‚"},
            {"role": "user", "content": prompt }
            ])
    
    raw = completion.choices[0].message.content.strip()

    # 3. å»æ‰å¯èƒ½çš„ ```json åŒ…è£¹ç­‰å™ªå£°
    clean = raw
    if clean.startswith("```"):
        first_newline = clean.find("\n")
        if first_newline != -1:
            clean = clean[first_newline+1:]
        end_fence = clean.rfind("```")
        if end_fence != -1:
            clean = clean[:end_fence]
        clean = clean.strip()

    # 4. å¦‚æœå‰åè¿˜æœ‰è§£é‡Šæ–‡å­—ï¼Œåªå–ç¬¬ä¸€ä¸ª '[' åˆ°æœ€åä¸€ä¸ª ']' ä¹‹é—´
    if "[" in clean and "]" in clean:
        start = clean.find("[")
        end = clean.rfind("]")
        if start != -1 and end != -1 and end > start:
            clean = clean[start:end+1].strip()

    try:
        result = json.loads(clean)
    except json.JSONDecodeError as e:
        print("âš ï¸ [Topic_cleaning] JSON è§£æå¤±è´¥ï¼Œå°†ç›´æ¥è¿”å›åŸå§‹ topic_descriptionã€‚é”™è¯¯ï¼š", e)
        print("âš ï¸ åŸå§‹å†…å®¹ç‰‡æ®µï¼š", clean[:500])
        # è§£æå¤±è´¥æ—¶ï¼Œå®å¯åŸæ ·è¿”å›ï¼Œä¸è¦æ¸…ç©º
        if isinstance(topic_description, list):
            result = topic_description
        else:
            result = []

    return result

def Topic_Allocation(history, cleaned_topics, top_k_chunks=6):

    # 0. æ„å»ºå¯¹è¯å‘é‡åº“
    if not (isinstance(history, list) and history):
        print("âš ï¸ Topic_Allocation_v2: history ä¸ºç©ºæˆ–æ ¼å¼å¼‚å¸¸ï¼Œå°†è¿”å›ç©ºç»“æœã€‚")
        return []

    store = ConvVectorStore.from_history(history, window_size=40, stride=40)

    results = []

    for topic_obj in cleaned_topics:
        topic_name = topic_obj.get("topic", "").strip()
        if not topic_name:
            continue

        support_examples = topic_obj.get("support_examples", []) or []
        if not isinstance(support_examples, list):
            support_examples = [str(support_examples)]

        # 1) æ„é€  queryï¼štopic å + æ”¯æ’‘ä¾‹å­
        query_text = topic_name + "\n" + "\n".join(support_examples)

        # 2) æ£€ç´¢ä¸è¯¥ topic ç›¸å…³çš„å¯¹è¯ç‰‡æ®µ
        context = store.build_context(query_text, top_k=top_k_chunks)
        if not context.strip():
            # æ²¡æ£€ç´¢åˆ°å°±è·³è¿‡æˆ–ç»™ç©º slots
            results.append({"topic": topic_name, "slots": []})
            continue

        # 3) è®© LLM åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­æŠ½å–äºŒçº§å­ä¸»é¢˜ï¼ˆslotsï¼‰
        #    æ³¨æ„ï¼šä¸Šä¸‹æ–‡ä¸­æ¯è¡Œéƒ½æ˜¯ "[id][role]: content"ï¼Œè®©æ¨¡å‹å¤åˆ¶ id å³å¯
        prompt = f"""è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
            ä»»åŠ¡ï¼šä½ å°†çœ‹åˆ°ä¸€æ®µä¸æŸä¸ªä¸€çº§ä¸»é¢˜é«˜åº¦ç›¸å…³çš„å¯¹è¯åŸæ–‡ç‰‡æ®µã€‚
            ä¸€çº§ä¸»é¢˜ä¸ºï¼š"{topic_name}"

            å¯¹è¯ç‰‡æ®µï¼ˆæ¯è¡Œä»¥ [id][role]: å¼€å¤´ï¼‰ï¼š
            {context}

            è¯·ä½ åœ¨ä¸Šè¿°å¯¹è¯ä¸­ï¼ŒæŠ½å–è‹¥å¹²ä¸è¯¥ä¸»é¢˜å¯†åˆ‡ç›¸å…³çš„â€œäºŒçº§å­ä¸»é¢˜â€(slot)ã€‚
            è¾“å‡ºä¸º JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªå¯¹è±¡ï¼ŒåŒ…å«å­—æ®µï¼š
            - "sentence": å¯¹è¯ä¸­çš„åŸæ–‡å¥å­ï¼ˆå¿…é¡»ä¸ä¸Šé¢æŸä¸€è¡Œçš„ content å®Œå…¨ä¸€è‡´ï¼Œå¯ä»¥åŒ…å«å‰åå°‘é‡æ ‡ç‚¹ï¼Œä½†ä¸è¦è‡ªè¡Œæ”¹å†™ï¼‰
            - "slot": è¯¥å¥å­å¯¹åº”çš„äºŒçº§å­ä¸»é¢˜åç§°ï¼ˆç®€çŸ­ã€å…·ä½“çš„åè¯çŸ­è¯­æˆ–åŠ¨å®¾çŸ­è¯­ï¼‰
            - "id": è¯¥å¥å­å¯¹åº”è¡Œå‰é¢çš„ idï¼ˆæ•´æ•°ï¼‰
            - "sentiment": è¯¥å¥å­çš„æƒ…ç»ªåˆ†æ•°ï¼ŒèŒƒå›´ -1 (æœ€è´Ÿé¢ï¼Œå¦‚æ²®ä¸§ã€æ‰¹è¯„) åˆ° 1 (æœ€æ­£é¢ï¼Œå¦‚èµç¾ã€ä¹è§‚)ï¼Œ0 è¡¨ç¤ºä¸­æ€§ã€‚
            - "source": è¯´è¯äººè§’è‰²ï¼Œåªèƒ½æ˜¯ "user" æˆ– "bot"ï¼š
                * å¦‚æœè¯¥è¡Œçš„ [role] è¡¨ç¤ºæ¥è®¿è€… / ç”¨æˆ·ï¼ˆä¾‹å¦‚åŒ…å« "user"ã€"User"ã€"ç”¨æˆ·" ç­‰ï¼‰ï¼Œè¯·å¡« "user"ï¼›
                * å¦‚æœè¯¥è¡Œçš„ [role] è¡¨ç¤ºåŠ©ç† / AIï¼ˆä¾‹å¦‚åŒ…å« "assistant"ã€"Assistant"ã€"bot"ã€"åŠ©æ‰‹" ç­‰ï¼‰ï¼Œè¯·å¡« "bot"ã€‚

            å…·ä½“è¦æ±‚ï¼š
            1. åªè€ƒè™‘ä¸ä¸€çº§ä¸»é¢˜ "{topic_name}" æ˜ç¡®ç›¸å…³çš„å¥å­ï¼›
            2. å¯¹äºåŒä¸€ä¸ª idï¼Œåªèƒ½åœ¨è¾“å‡ºæ•°ç»„ä¸­å‡ºç°ä¸€æ¬¡ï¼š
               - å¦‚æœä½ è®¤ä¸ºåŒä¸€ä¸ª id çš„å¥å­æ¶‰åŠå¤šä¸ªå­ä¸»é¢˜ï¼Œè¯·åªé€‰æ‹©ä½ è®¤ä¸ºâ€œæœ€æ ¸å¿ƒâ€çš„ä¸€ä¸ªä½œä¸º slotï¼›
               - ä¸¥ç¦ä¸ºåŒä¸€ä¸ª id è¾“å‡ºå¤šæ¡è®°å½•ã€‚
            3. æ¯æ¡ sentence æœ€å¤šå¯¹åº”ä¸€ä¸ª slot
            4. å¦‚æœå¤šå¥è¡¨è¾¾å®Œå…¨ç›¸åŒçš„äºŒçº§å­ä¸»é¢˜ï¼Œå¯åªä¿ç•™ä¿¡æ¯æ›´å®Œæ•´çš„å¥å­ï¼›
            5. ä¸¥æ ¼è¾“å‡º JSON æ•°ç»„ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼Œä¹Ÿä¸è¦ä½¿ç”¨ä»£ç å—æ ‡è®°ã€‚
            ç¤ºä¾‹è¾“å‡ºï¼ˆç¤ºæ„ï¼‰ï¼š
            [
              {{"sentence": "æˆ‘ä»¬éœ€è¦æ”¹è¿› SWMM æ¨¡å‹çš„å‚æ•°æ ¡å‡†è¿‡ç¨‹ã€‚", "slot": "SWMM å‚æ•°æ ¡å‡†", "id": 45, "sentiment": 0.2, "source": "user"}},
              {{"sentence": "æœ¬æ¬¡ä¸»è¦è®¨è®º DrainScope ä¸­çš„æ’æ°´é£é™©æŒ‡æ ‡å¯è§†åˆ†æã€‚", "slot": "æ’æ°´é£é™©æŒ‡æ ‡å¯è§†åˆ†æ", "id": 52, "sentiment": -0.1, "source": "bot"}}
            ]
        """

        completion = openai.chat.completions.create(
            model="gpt-4o",
            temperature=0.2,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€åå¯¹è¯åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä»å¯¹è¯ä¸­æŠ½å–ç»“æ„åŒ–ä¸»é¢˜ä¿¡æ¯ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )

        raw = completion.choices[0].message.content.strip()

        # 4) è§£æ JSONï¼ˆå’Œä½ æ¸…æ´—é‚£è¾¹ä¸€æ ·çš„é²æ£’å¥—è·¯ï¼‰
        clean = raw
        if clean.startswith("```"):
            first_newline = clean.find("\n")
            if first_newline != -1:
                clean = clean[first_newline + 1:]
            end_fence = clean.rfind("```")
            if end_fence != -1:
                clean = clean[:end_fence]
            clean = clean.strip()

        if "[" in clean and "]" in clean:
            start = clean.find("[")
            end = clean.rfind("]")
            if start != -1 and end != -1 and end > start:
                clean = clean[start:end + 1].strip()

        try:
            slots = json.loads(clean)
        except json.JSONDecodeError as e:
            print(f"âš ï¸ [Topic_Allocation_v2] è§£æ slots JSON å¤±è´¥ï¼Œtopic={topic_name}, é”™è¯¯: {e}")
            slots = []

        # 5) è§„èŒƒåŒ– & æ’åºï¼ˆæŒ‰ id æ—¶é—´é¡ºåºï¼‰
        norm_slots = []
        seen_ids = set()

        for s in slots:
            if not isinstance(s, dict):
                continue
            sent = s.get("sentence", "").strip()
            slot_name = s.get("slot", "").strip()
            try:
                sid = int(s.get("id"))
            except Exception:
                continue
            sentiment = s.get("sentiment", 0.0)  # é»˜è®¤ 0ï¼Œå¦‚æœç¼ºå¤±

            raw_source = (s.get("source") or "").strip().lower()
            if raw_source in ["user", "u", "client", "æ¥è®¿è€…", "ç”¨æˆ·"]:
                source = "user"
            elif raw_source in ["bot", "assistant", "ai", "åŠ©æ‰‹", "ç³»ç»Ÿ"]:
                source = "bot"
            else:
                source = "user"  # å®åœ¨ä¸ç¡®å®šå°±é»˜è®¤ userï¼Œæˆ–è€…ä½ å¯ä»¥æ”¹æˆ None

            if not sent or not slot_name:
                continue

            # âœ… å»é‡ï¼šåŒä¸€ä¸ª topic é‡Œï¼Œä¸€ä¸ª id åªä¿ç•™ä¸€æ¬¡
            if sid in seen_ids:
                # å¦‚æœä½ ä»¥åæƒ³æ¢ç­–ç•¥ï¼ˆæ¯”å¦‚é€‰æƒ…ç»ªæ›´å¼ºçš„é‚£æ¡ï¼‰ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ”¹é€»è¾‘
                continue
            seen_ids.add(sid)

            norm_slots.append({
                "sentence": sent,
                "slot": slot_name,
                "id": sid,
                "sentiment": sentiment,  # æ–°å¢å­—æ®µ
                "source": source
            })

        norm_slots.sort(key=lambda x: x["id"])

        # 6) ä¸¥æ ¼æŒ‰ä½ è¦æ±‚çš„æ ¼å¼å†™å…¥ç»“æœ
        results.append({
            "topic": topic_name,
            "slots": norm_slots
        })

    return results


# def llm_extract_information_incremental(history_sentences,new_sentence, existing_topics=None): 
    
#     """
#     å¯¹æ–°å¥å­è¿›è¡Œä¸»é¢˜æŠ½å–ï¼Œå¹¶ä¸å·²æœ‰æŠ½å–ç»“æœåˆå¹¶
#     """
#     existing_topics = existing_topics or []

#     # ğŸ‘‰ æ”¯æŒ dict æˆ– str
#     if isinstance(new_sentence, dict):
#         sentence_text = new_sentence.get("content", "")
#     else:
#         sentence_text = str(new_sentence)
#         history_sentences = str(history_sentences)

#     prompt = f"""è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

#         ä»»åŠ¡ï¼šé¦–å…ˆä½ éœ€è¦å°†æ–°çš„ä¸€å¥å¯¹è¯ä¸­æ— å…³ç´§è¦çš„ä¿¡æ¯è¿›è¡Œè¿‡æ»¤ï¼Œç„¶åå¯¹è¿™å¥å¯¹è¯è¿›è¡Œä¸»é¢˜æŠ½å–ã€‚
#         å†å²å¯¹è¯:{history_sentences}

#         æ–°çš„å¯¹è¯ï¼š{new_sentence}

#         æ–°çš„å¥å­: {sentence_text}

#         å·²æœ‰ä¸»é¢˜: {json.dumps(existing_topics, ensure_ascii=False)}

#         æŠ½å–ä¸»é¢˜è¿‡ç¨‹è¦æŒ‰ç…§ä¸‹é¢ä¸‰æ­¥æ¥è¿›è¡Œï¼š
#         Step 1ï¼šç†è§£æ•´è½®è¯­ä¹‰èƒŒæ™¯
#         è¯·å…ˆé˜…è¯»å†å²å¯¹è¯å†…å®¹ï¼Œç†è§£æ•´è½®å¯¹è¯çš„ä¸»è¦è¯­ä¹‰ç„¦ç‚¹æˆ–è®¨è®ºæ–¹å‘ã€‚

#         Step 2ï¼šèšç„¦å½“å‰è½®çš„å‰10å¥
#         ä»å½“å‰å¯¹è¯ä¸­é€‰å–**æ–°çš„å¥å­: {sentence_text}çš„å‰10å¥**ï¼ˆè‹¥ä¸è¶³10å¥åˆ™å…¨éƒ¨ä½¿ç”¨ï¼‰ï¼Œ
#         å’Œå®ƒä»¬çš„ä¸»é¢˜ã€‚

#         Step 3ï¼šä¸»é¢˜æŠ½å–ä¸è¾“å‡º
#         ç»“åˆ Step 1 çš„å…¨å±€è¯­ä¹‰ç†è§£ä¸ Step 2 çš„å±€éƒ¨ç„¦ç‚¹ï¼ŒæŠ½å–å‡ºæœ¬è½®å¯¹è¯çš„ä¸»é¢˜ï¼Œè¯·åªè¾“å‡ºæ–°å¥å­çš„ä¸»é¢˜ JSONï¼Œä¸ä¿®æ”¹å·²æœ‰ä¸»é¢˜ã€‚

#         è¾“å‡ºè¦æ±‚ï¼š
#         1. è¾“å‡ºå¿…é¡»æ˜¯æ ‡å‡† JSON å¯¹è±¡ï¼Œä¸¥ç¦åŒ…å«ä»£ç å—æ ‡è®°ï¼ˆå¦‚```jsonï¼‰æˆ–å¤šä½™æ–‡å­—ã€‚
#         2. æ¯ä¸ªä¸»é¢˜åŒ…å«å­—æ®µï¼š
#         - "topic": ä¸»é¢˜åç§°ï¼ˆæœ€é«˜å±‚é¢†åŸŸåï¼Œå¦‚â€œäººå·¥æ™ºèƒ½â€â€œå¯è§†åŒ–â€â€œæ™ºæ…§åŸå¸‚â€ï¼‰ï¼Œä¸ºåè¯çŸ­è¯­ã€‚
#         - "slots": ä¸€ä¸ªæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {{ "sentence": åŸå§‹å¥å­, "slot": å¯¹åº”çš„å­ä¸»é¢˜}}
#         3. slotå¿…é¡»æ˜¯**ç®€æ´ã€å…·ä½“ã€å¯è½åœ°çš„åè¯çŸ­è¯­æˆ–åŠ¨å®¾çŸ­è¯­**ï¼Œèƒ½æŒ‡å‘ä¸€ä¸ªæ¸…æ™°çš„å…³æ³¨ç‚¹ã€‚
#         4. ä¿æŒä¸»é¢˜ä¸å­ä¸»é¢˜è¡¨è¿°ç®€æ´ã€‚
#         5. è¾“å‡ºçš„æ ‡å‡† JSON æ ¼å¼ï¼š
#         [
#             {{
#                 "topic": "ä¸»é¢˜åç§°",
#                 "slots": [
#                     {{"sentence": "åŸå§‹å¥å­", "slot": "å­ä¸»é¢˜"}}
#                 ]
#             }}
#         ]
#         ä¾‹å­ï¼š
#         [
#             {{
#                 "topic": "äººå·¥æ™ºèƒ½",
#                 "slots": [
#                     {{"sentence": "äººå·¥æ™ºèƒ½ä¼¦ç†å…³æ³¨çš„ä¸ä»…æ˜¯ç®—æ³•çš„å…¬å¹³æ€§ä¸éšç§ä¿æŠ¤ï¼Œè¿˜åŒ…æ‹¬æ•°æ®ä½¿ç”¨çš„é€æ˜åº¦ã€æ¨¡å‹å†³ç­–çš„å¯è§£é‡Šæ€§ã€‚", "slot": "äººå·¥æ™ºèƒ½ä¼¦ç†"}}
#                 ]
#             }}
#         ]


#         è§„åˆ™è¡¥å……ï¼š
#         1. æ‰€æœ‰é—®é¢˜é¦–å…ˆè¦è¯†åˆ«æœ€é«˜å±‚çš„å¤§ä¸»é¢˜ï¼Œä½œä¸ºå”¯ä¸€çš„topicã€‚
#         2. è‹¥å¥å­æ¶‰åŠå¤šä¸ªå†…å®¹ï¼Œè¯·æç‚¼å‡ºæœ€æ ¸å¿ƒçš„ä¸»é¢˜ã€‚
#         3. slot **ç¦æ­¢**ç©ºæ³›/ç¬¼ç»Ÿ/æŠ½è±¡æŒ‡ä»£ï¼Œä¾‹å¦‚åªå†™â€œç ”ç©¶â€â€œé—®é¢˜â€â€œåº”ç”¨â€â€œæ–¹æ³•è®ºâ€â€œå½±å“â€â€œå‘å±•â€â€œç°çŠ¶â€â€œè®¨è®ºâ€ç­‰æ³›è¯ã€‚
#         4. topicåªè¡¨ç¤ºæ ¸å¿ƒé¢†åŸŸï¼Œslots è´Ÿè´£ç»†åˆ†é—®é¢˜ã€‚
#         5. å¤§éƒ¨åˆ†æ—¶é—´botçš„å›å¤æ˜¯æ ¹æ®userçš„é—®é¢˜æ¥çš„ï¼Œæ‰€ä»¥å¤§éƒ¨åˆ†æ—¶é—´botå›å¤çš„ä¸»é¢˜å’Œuserçš„é—®é¢˜çš„ä¸»é¢˜æ˜¯ä¸€è‡´çš„ã€‚
#         """
    
#     completion = openai.chat.completions.create(
#         model="gpt-4o",
#         temperature=0.5,
#         messages=[
#             {"role": "system", "content": "ä½ æ˜¯ä¸€åå¯¹è¯åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä»å¯¹è¯ä¸­æå–å‡ºç”¨æˆ·çš„å¯¹è¯ä¸»é¢˜å¹¶åˆå¹¶åˆ°å·²æœ‰ä¸»é¢˜ã€‚"},
#             {"role": "user", "content": prompt }
#             ],)
    
#     try:
#         result = json.loads(completion.choices[0].message.content)
#     except json.JSONDecodeError:
#         result = []

#     print("æŠ½å–ç»“æœï¼š")
#     print(result)

#     return result

# ç”Ÿæˆ embedding
def get_embedding(text):
    emb = openai.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return np.array(emb.data[0].embedding, dtype="float32")

# ===== åˆå§‹åŒ– Memoryï¼ˆå¯ä»¥æ¢æˆæ•°æ®åº“ï¼‰ =====
user_memory_db = {}  # ç¤ºä¾‹: {user_id: {key: value}}

# ===== Memoryæ“ä½œå‡½æ•° =====
# å¾—åˆ°ç”¨æˆ·Memory
def get_user_memory(user_id):
    """
    è¿”å›æ•´ç†å¥½çš„æ–‡æœ¬ï¼Œç”¨äºæ‹¼æ¥åˆ°prompt
    """
    if user_id not in user_memory_db:
        return ""
    memory = user_memory_db[user_id]
    return "\n".join([f"{k}: {v}" for k, v in memory.items()])

# æ›´æ–°ç”¨æˆ·Memory
def update_user_memory(user_id, key, value):
    global user_memory_db
    if user_id not in user_memory_db:
        user_memory_db[user_id] = {}
    
    # å¦‚æœ key å·²å­˜åœ¨ï¼Œåšâ€œè¿½åŠ â€è€Œä¸æ˜¯è¦†ç›–
    if key in user_memory_db[user_id]:
        old_value = user_memory_db[user_id][key]
        if value not in old_value:
            user_memory_db[user_id][key] = f"{old_value}; {value}"
    else:
        user_memory_db[user_id][key] = value
    
    user_memory_db[user_id]['last_updated'] = str(datetime.now())

    print(f"Memoryæ›´æ–°ï¼š{user_id} - {key} - {value}")
    print("Memoryåº“:" ,user_memory_db)

def extract_memory_from_text(user_id, new_sentence):
    """
    è°ƒç”¨ GPT è‡ªåŠ¨ä»æ–‡æœ¬ä¸­æŠ½å–å…³é”®ä¿¡æ¯ï¼ˆå¦‚åå­—ã€å…´è¶£ã€åå¥½ç­‰ï¼‰
    å¹¶æ›´æ–° Memory
    """
    prompt = f"""
    è¯·ä»ä¸‹é¢çš„æ–‡æœ¬ä¸­æå–ç”¨æˆ·å¯èƒ½æƒ³è®°ä½çš„å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬å§“åã€å…´è¶£ã€çˆ±å¥½ã€å­¦ä¹ ç›®æ ‡ç­‰ã€‚
    è¾“å‡ºå¿…é¡»æ˜¯æ ‡å‡† JSON å¯¹è±¡ï¼Œä¸¥ç¦åŒ…å«ä»£ç å—æ ‡è®°ï¼ˆå¦‚```jsonï¼‰æˆ–å¤šä½™æ–‡å­—ï¼Œé”®åéšæ„ä½†è¦èƒ½æè¿°ä¿¡æ¯ï¼Œå¦‚ï¼š
    {{
        "name": "Alice",
        "favorite_language": "Python"
    }}

    æ–‡æœ¬å†…å®¹ï¼š
    {new_sentence}
    """

    completion = openai.chat.completions.create(
        model="gpt-4o",
        temperature=0,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€åä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ]
    )
    result = (completion.choices[0].message.content)
    # æ›´æ–°è§£æç»“æœåˆ°Memory
    try:
        memory_data = json.loads(result)
        for key, value in memory_data.items():
            update_user_memory(user_id, key, value)
    except json.JSONDecodeError:
        # å‡ºç°è§£æé”™è¯¯æ—¶å¯ä»¥å¿½ç•¥æˆ–è®°å½•æ—¥å¿—
        print("Memory JSONè§£æå¤±è´¥:", result)

# ===== GPT + Memory + RAGå‡½æ•° =====
def talk_to_chatbot(user_id, content, source, history_msgs, top_k=3):
    global index

    # 1. å…ˆæ£€ç´¢Memory
    memory_context = get_user_memory(user_id)

    # 2. ä¸ºç”¨æˆ·è¾“å…¥ç”Ÿæˆ embedding
    query_emb = get_embedding(content).reshape(1, -1)

    # 3. æ£€ç´¢ç›¸å…³ Memory
    related_context = ""
    if index is not None and memory_context:
        D, I = index.search(query_emb, top_k)
        # ç®€å•ç¤ºä¾‹ï¼šç”¨ç´¢å¼•å¯¹åº”çš„ Memory è¡Œï¼ˆå‡è®¾ memory_text åˆ†è¡Œå­˜å‚¨ï¼‰
        memory_lines = memory_context.split("\n")
        related_context = "\n".join([memory_lines[i] for i in I[0] if i < len(memory_lines)])

    # 4.å…ˆæŠŠå†å²å¯¹è¯æ•´ç†æˆæ–‡æœ¬
    history_text = "\n".join(
    [f"{msg.get('from', 'user').capitalize()}: {msg.get('text', '')}" for msg in history_msgs])

    # 5. ç»„ç»‡ promptï¼ŒæŠŠ Memory å’Œ RAG æ£€ç´¢å†…å®¹éƒ½æ‹¼è¿›å»
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€åå¯¹è¯åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä¸ç”¨æˆ·è¿›è¡Œæ²Ÿé€š, è¯·æ ¹æ®ç”¨æˆ·çš„è¾“å…¥ï¼Œåˆç†å›ç­”ï¼Œå¹¶ä¿æŒæ²Ÿé€šè¿è´¯ã€‚"},
        {"role": "user", "content": f"""
        ä¸‹é¢æ˜¯ä¸æœ¬é—®é¢˜ç›¸å…³çš„å†å²å¯¹è¯ï¼š{history_text}
        ç”¨æˆ·ä¿¡æ¯ï¼š
        {memory_context}
        ç›¸å…³ Memory æ£€ç´¢å†…å®¹ï¼š
        {related_context}
        ç°åœ¨ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š
        {content}"""}
        ]
    
    # 6. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›å¤ 
    completion = openai.chat.completions.create(
        model="gpt-4o",
        temperature=0.5,
        messages=messages,
        )
    result = (completion.choices[0].message.content)

    # 7. æ›´æ–° Memoryï¼ˆé•¿æœŸè®°å¿†ï¼‰åˆ° FAISS
    if index is not None and memory_context:
        memory_text = "\n".join([f"{k}: {v}" for k, v in user_memory_db[user_id].items()])
        emb = get_embedding(memory_text).reshape(1, -1)
        index.add(emb)

    # 8. è‡ªåŠ¨æŠ½å–æœ€æ–° Memory ä¿¡æ¯
    extract_memory_from_text(user_id, content)

    return result

def create_theme_variables(result_dict):
    for theme, questions in result_dict.items():
        # åˆ›å»ºåˆæ³•å˜é‡åï¼ˆç§»é™¤éæ³•å­—ç¬¦ï¼‰
        var_name = theme.replace(" ", "_").replace("ï¼š", "").replace("-", "_")
        globals()[var_name] = questions
    
    print(f"{var_name} = {questions}")  # å¯é€‰ï¼šæ‰“å°å‡ºæ¥
