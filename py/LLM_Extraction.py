import os
import re
import json
import openai
import faiss
import numpy as np
from datetime import datetime
from typing import List, Dict, Any, Optional
from Methods import *
openai.api_key = "sk-3fk05T3Cme02GzUGBc56BaBfA7Ff4dCa9d7dE5AeA689913c"

openai.base_url = "https://api.gpt.ge/v1/"
openai.default_headers = {"x-foo": "true"}

# ===== 1. åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼ˆFAISSï¼‰ =====
dimension = 1536  # OpenAI text-embedding-3-small è¾“å‡ºå‘é‡ç»´åº¦
index = faiss.IndexFlatL2(dimension)  # L2 è·ç¦»ç´¢å¼•

def embed_texts(
        text_list, 
        model="text-embedding-3-large",
        max_chars_per_item=2000,\
        max_batch_chars=6000,
        max_batch_items=8 ):
    """
    è¾“å…¥: [str, str, ...]
    è¾“å‡º: [np.array(d), ...]
    """
    processed = []
    for text in text_list:
        t = str(text)
        if len(t) > max_chars_per_item:
            t = t[:max_chars_per_item]
        processed.append(t)

    embs = []
    batch = []
    batch_chars = 0

    def _flush_batch(batch_texts):
        if not batch_texts:
            return []
        resp = openai.embeddings.create(
            model=model,
            input=batch_texts,
        )
        return [np.array(d.embedding, dtype="float32") for d in resp.data]

    for t in processed:
        # å¦‚æœå†åŠ è¿™ä¸€æ¡ä¼šè¶…å‡ºé™åˆ¶ï¼Œå°±å…ˆæŠŠå½“å‰ batch å‘å‡ºå»
        if (batch and (batch_chars + len(t) > max_batch_chars
                       or len(batch) >= max_batch_items)):
            embs.extend(_flush_batch(batch))
            batch = []
            batch_chars = 0

        batch.append(t)
        batch_chars += len(t)

    # åˆ«å¿˜äº† flush æœ€åä¸€ä¸ª batch
    if batch:
        embs.extend(_flush_batch(batch))

    return embs

# å‘é‡æ•°æ®åº“ç±»
class ConvVectorStore:
    def __init__(self, chunks, index, emb_dim):
        self.chunks = chunks          # list[dict], æ¯ä¸ªæœ‰ start_id/end_id/text
        self.index = index            # faiss index
        self.emb_dim = emb_dim

    @classmethod
    def from_history(cls, history, window_size=20, stride=20):
        """
        ä»ä¸€æ•´è½®å¯¹è¯æ„å»ºå‘é‡åº“ï¼š
        - åˆ‡ chunk
        - å¯¹æ¯ä¸ª text åš embedding
        - ç”¨ FAISS å»º IndexFlatIP
        """
        chunks = build_conv_chunks(history, window_size=window_size, stride=stride)
        if not chunks:
            # ç©º history
            index = None
            return cls([], index, 0)

        texts = [c["text"] for c in chunks]
        embs = embed_texts(texts)  # list[np.array]
        emb_dim = embs[0].shape[0]

        emb_matrix = np.stack(embs, axis=0)  # (N, d)
        index = faiss.IndexFlatIP(emb_dim)   # å†…ç§¯ï¼Œç›¸å½“äºä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆéœ€å…ˆå½’ä¸€åŒ–çš„è¯å¯ä»¥å†å°è£…ï¼‰
        index.add(emb_matrix)

        # æŠŠ embedding ä¸€èµ·å­˜ä½æ–¹ä¾¿ debugï¼ˆä¸ä¸€å®šå¿…é¡»ï¼‰
        for c, e in zip(chunks, embs):
            c["embedding"] = e

        return cls(chunks, index, emb_dim)

    def search_by_text(self, query_text, top_k=8):
        """
        ç»™ä¸€æ®µ query æ–‡æœ¬ï¼Œè¿”å›æœ€ç›¸å…³çš„ top_k ä¸ª chunkï¼ˆå·²ç»æŒ‰ç›¸ä¼¼åº¦æ’å¥½ï¼‰
        """
        if not self.chunks or self.index is None:
            return []

        q_emb = embed_texts([query_text])[0].reshape(1, -1).astype("float32")  # (1, d)
        D, I = self.index.search(q_emb, top_k)  # I: (1, top_k)

        idxs = I[0]
        selected = [self.chunks[i] for i in idxs if 0 <= i < len(self.chunks)]
        # æŒ‰æ—¶é—´é¡ºåºæ’ä¸€ä¸‹ï¼Œæ–¹ä¾¿ä½ åé¢ç”¨
        selected.sort(key=lambda c: c["start_id"])
        return selected

    def build_context(self, query_text, top_k=5):
        """
        æŠŠæ£€ç´¢åˆ°çš„ top_k chunks æ‹¼æˆä¸€ä¸ªä¸Šä¸‹æ–‡æ–‡æœ¬ï¼Œç”¨æ¥ä¸¢ç»™ LLM çœ‹ã€‚
        """
        selected = self.search_by_text(query_text, top_k=top_k)
        if not selected:
            return ""

        ctx = "\n\n".join(
            f"[å¯¹è¯ç‰‡æ®µ {c['start_id']}~{c['end_id']}]\n{c['text']}"
            for c in selected
        )
        return ctx

def Score_turn_importance(history):
    """
    history: list[dict]ï¼Œå½¢å¦‚ï¼š
      [{"id": 1, "role": "user", "content": "..."}, ...]
    è¿”å›ï¼šåŒæ ·é•¿åº¦çš„ listï¼Œæ¯ä¸ªå…ƒç´ å¤šä¸€ä¸ª "info_score" å­—æ®µï¼ˆ0.2 ~ 1.0ï¼‰
    """

    if not isinstance(history, list) or not history:
        print("âš ï¸ Score_turn_importance: history ä¸ºç©ºæˆ–æ ¼å¼å¼‚å¸¸ï¼Œå°†è¿”å›åŸæ ·ã€‚")
        return history

    # 1) æŠŠå¯¹è¯æ•´ç†æˆ [id][role]: content å½¢å¼ï¼Œç»™ LLM çœ‹
    lines = []
    for m in history:
        mid = m.get("id")
        role = m.get("role") or m.get("from") or "user"
        text = (m.get("content") or m.get("text") or "").strip()
        if mid is None or text == "":
            continue
        lines.append(f"[{mid}][{role}]: {text}")

    if not lines:
        return history

    conv_text = "\n".join(lines)

 # 2. æ„é€  promptï¼šåªè®©æ¨¡å‹è¾“å‡º id + info_score
    prompt = f"""ä½ æ˜¯ä¸€åä¸¥è°¨çš„å¯¹è¯åˆ†æåŠ©æ‰‹ã€‚

        ç°åœ¨ç»™ä½ ä¸€æ®µå¤šè½®å¯¹è¯ï¼Œæ¯ä¸€è¡Œçš„æ ¼å¼ä¸ºï¼š
        [id][role]: content

        å…¶ä¸­ï¼š
        - id æ˜¯å¯¹è¯è½®æ¬¡çš„æ•´æ•°ç¼–å·ï¼›
        - role æ˜¯è¯´è¯äººè§’è‰²ï¼›
        - content æ˜¯è¯¥è½®çš„å‘è¨€å†…å®¹ã€‚

        è¯·ä½ æ ¹æ®æ•´æ®µå¯¹è¯çš„è¯­ä¹‰ï¼Œä¸ºå…¶ä¸­æ¯ä¸€è½®â€œå®é™…æœ‰å†…å®¹çš„å¯¹è¯â€æ‰“ä¸€ä¸ªâ€œä¿¡æ¯é‡/é‡è¦ç¨‹åº¦â€åˆ†æ•° info_scoreï¼Œç”¨æ¥è¡¡é‡è¿™æ¡å‘è¨€åœ¨æ•´æ®µå¯¹è¯ä¸­çš„é‡è¦æ€§ã€‚

        è¦æ±‚ï¼š
        1. å¯¹æ¯ä¸€æ¡å‡ºç°çš„ idï¼ˆå³æ¯ä¸€è¡Œå‘è¨€ï¼‰éƒ½ç»™å‡ºä¸€ä¸ª info_scoreï¼›
        2. info_score ä¸ºæµ®ç‚¹æ•°ï¼ŒèŒƒå›´åœ¨ 0.2 ~ 1.0 ä¹‹é—´ï¼š
        - è¶Šæ¥è¿‘ 1.0ï¼Œè¯´æ˜è¿™è½®å‘è¨€è¶Šå…³é”®ã€ä¿¡æ¯é‡è¶Šå¤§ï¼›
        - è¶Šæ¥è¿‘ 0.2ï¼Œè¯´æ˜è¿™è½®å‘è¨€è¶Šè¾¹ç¼˜ã€é‡å¤æˆ–é—²èŠæ€§è´¨ï¼›
        3. ä¸éœ€è¦è¾“å‡º role æˆ– contentï¼Œåªéœ€è¦è¾“å‡º id å’Œ info_scoreï¼›
        4. ä¸¥æ ¼è¾“å‡ºä¸€ä¸ª JSON æ•°ç»„ï¼Œç¦æ­¢ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€æ³¨é‡Šæˆ–ä»£ç å—æ ‡è®°ã€‚

        å¯¹è¯å†…å®¹å¦‚ä¸‹ï¼š
        {conv_text}

        è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆç¤ºä¾‹ï¼‰ï¼š
        [
        {{"id": 1, "info_score": 0.85}},
        {{"id": 2, "info_score": 0.35}}
        ]
        """

    completion = openai.chat.completions.create(
        model="gpt-5.2",
        temperature=0.2,
        messages=[
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€åä¸¥è°¨çš„å¯¹è¯åˆ†æåŠ©æ‰‹ï¼Œåªè¾“å‡ºä¸¥æ ¼ JSONã€‚",
            },
            {"role": "user", "content": prompt},
        ],
    )

    raw = completion.choices[0].message.content.strip()

    # 3) ç®€å•é²æ£’è§£æï¼šå»æ‰ ```json åŒ…è£¹
    clean = raw
    if clean.startswith("```"):
        first_newline = clean.find("\n")
        if first_newline != -1:
            clean = clean[first_newline + 1 :]
        end_fence = clean.rfind("```")
        if end_fence != -1:
            clean = clean[:end_fence]
        clean = clean.strip()

    if "[" in clean and "]" in clean:
        start = clean.find("[")
        end = clean.rfind("]")
        if start != -1 and end != -1 and end > start:
            clean = clean[start : end + 1].strip()

    # æˆªå–ç¬¬ä¸€ä¸ª [ åˆ° æœ€åä¸€ä¸ª ] ä¹‹é—´
    if "[" in clean and "]" in clean:
        start = clean.find("[")
        end = clean.rfind("]")
        if start != -1 and end != -1 and end > start:
            clean = clean[start : end + 1].strip()

    id2score: Dict[int, float] = {}

    try:
        arr = json.loads(clean)
        if isinstance(arr, list):
            for item in arr:
                if not isinstance(item, dict):
                    continue
                try:
                    mid = int(item.get("id"))
                except Exception:
                    continue
                score = item.get("info_score")
                try:
                    score = float(score)
                except Exception:
                    score = 0.5
                # çº¦æŸåˆ° [0.2, 1.0]
                score = max(0.2, min(1.0, score))
                id2score[mid] = score
    except Exception as e:
        print(f"âš ï¸ Score_turn_importance: JSON è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°ã€‚err={e}, raw={raw}")

    # 4. æŠŠ score è´´å›åŸ history_chunkï¼Œä¿è¯æ¯æ¡éƒ½æœ‰ info_score
    new_history = []
    for m in history:
        mid = m.get("id")
        m2 = dict(m)
        # å¦‚æœè¯¥ id æ²¡åœ¨ LLM è¾“å‡ºé‡Œï¼Œå°±ç»™ä¸€ä¸ªé»˜è®¤å€¼ 0.5
        m2["info_score"] = float(id2score.get(mid, 0.5))
        new_history.append(m2)

    return new_history

def Topic_Edge_detection(history):
    def to_lines(h):
        if isinstance(h, str):
            return h.strip()

        if isinstance(h, dict):
            if isinstance(h.get("messages"), list):
                h = h["messages"]
            elif isinstance(h.get("history"), list):
                h = h["history"]
            else:
                h = [h]

        if isinstance(h, list):
            lines = []
            for m in h:
                if not isinstance(m, dict) or "id" not in m:
                    continue
                try:
                    mid = int(m.get("id"))
                except Exception:
                    continue
                role = (m.get("role") or m.get("from") or m.get("source") or "unknown").strip()
                content = (m.get("content") or m.get("text") or "").replace("\n", " ").strip()
                if content:
                    lines.append(f"[{mid}][{role}]: {content}")
            return "\n".join(lines).strip()

        return str(h).strip()

    history_text = to_lines(history)

    prompt = f"""ä½ æ˜¯â€œå¯¹è¯è¯é¢˜è¾¹ç•Œæ£€æµ‹å™¨â€ã€‚åªè¾“å‡º JSON æ•°ç»„ï¼Œä¸è¦è§£é‡Šï¼Œä¸è¦ Markdownã€‚

    å¯¹è¯ç‰‡æ®µï¼ˆæ¯è¡Œä»¥ [id][role]: å¼€å¤´ï¼‰ï¼š
    -role: è¡¨æ˜è¿™å¥è¯çš„å‘è¨€äººæ˜¯è°
    {history_text}

    ä»»åŠ¡ï¼šæŠŠå…¨ä½“æ¶ˆæ¯åˆ‡åˆ†æˆè‹¥å¹²â€œè¿ç»­ã€ä¸é‡å ã€è¦†ç›–å…¨éƒ¨â€çš„è¯é¢˜æ®µã€‚

    è¦æ±‚ï¼š
    1) æ¯æ®µåŒ…å«è¿ç»­çš„ id èŒƒå›´ï¼šstart_id <= id <= end_id
    2) æ®µä¸æ®µä¹‹é—´å¿…é¡»é¦–å°¾ç›¸æ¥ï¼šä¸‹ä¸€æ®µ start_id = ä¸Šä¸€æ®µ end_id + 1
    3) è¦†ç›–å…¨éƒ¨æ¶ˆæ¯ï¼šç¬¬ä¸€æ®µ start_id=æœ€å°idï¼Œæœ€åä¸€æ®µ end_id=æœ€å¤§id
    4) slot ç”¨ä¸­æ–‡åè¯çŸ­è¯­ï¼Œ2~6å­—ï¼Œé¿å…å£è¯­/è™šè¯ï¼Œä¸”å¿…é¡»åªè¡¨è¾¾ä¸€ä¸ªæ ¸å¿ƒä¸»é¢˜ï¼ˆç¦æ­¢â€œXä¸Y/å’Œ/åŠ/å¤šä¸ªã€â€ï¼‰
    5) is_questionï¼šè¯¥æ®µæ˜¯å¦æ˜¯æé—®ï¼ˆtrue/falseï¼‰
    6) sourceï¼šè¯¥æ®µæ¥æº, è¡¨æ˜è¿™æ®µå¯¹è¯çš„ä¸»è¦å‘è¨€äºº
    6) æ®µå¤ªçŸ­(<3æ¡)å°½é‡åˆå¹¶ï¼Œæ®µå¤ªé•¿(>40æ¡)å°½é‡æ‹†åˆ†
    7) è¾“å‡ºå­—æ®µå¿…é¡»åŒ…å«ï¼šstart_id, end_id, slot, is_question, confidence(0~1)

    ä¸¥æ ¼è¾“å‡º JSONï¼Œä¾‹å¦‚ï¼š
    [
    {{"start_id":1,"end_id":12,"slot":"å©šå§»è§‚å¿µ","is_question":false,"source":"XXX"}},
    {{"start_id":13,"end_id":27,"slot":"ç¤¾ä¼šå‹åŠ›","is_question":true,"source":"XXX"}}
    ]
    """

    completion = openai.chat.completions.create(
        model="gpt-5.2",
        temperature=0.2,
        messages=[
            {"role": "system", "content": "ä½ åªè¾“å‡º JSON æ•°ç»„ï¼Œä¸è¦è¾“å‡ºè§£é‡Šï¼Œä¸è¦ Markdownã€‚"},
            {"role": "user", "content": prompt},
        ],
    )

    raw = (completion.choices[0].message.content or "").strip()
    arr = parse_json_array_loose(raw)

    # è½»åº¦è§„æ•´ï¼šä¿è¯å­—æ®µå­˜åœ¨ã€ç±»å‹æ­£ç¡®
    out = []
    for it in arr:
        if not isinstance(it, dict):
            continue
        try:
            s = int(it.get("start_id"))
            e = int(it.get("end_id"))
            source = str(it.get("source"))
        except Exception:
            continue
        slot = (it.get("slot") or "").strip()
        if not slot:
            continue
        out.append({
            "start_id": s,
            "end_id": e,
            "slot": slot,
            "is_question": parse_bool(it.get("is_question")),
            "source": source,
        })
    return out

def Topic_merge(topic_description: List[Dict[str, Any]]):
    slot_items = []
    for it in topic_description:
        if not isinstance(it, dict):
            continue
        try:
            start_id = int(it.get("start_id"))
            end_id = int(it.get("end_id"))
            source = str(it.get("source"))
        except Exception:
            continue
        slot_name = (it.get("slot") or it.get("topic_label") or "").strip()
        if not slot_name:
            continue


        slot_items.append({
            "slot": slot_name,
            "id": start_id,
            "source": source,
            "is_question": parse_bool(it.get("is_question")),
            "start_id": start_id,
            "end_id": end_id,
        })

    slot_list_for_prompt = [
        {k: s[k] for k in ("slot","id","source","is_question","start_id","end_id")}
        for s in slot_items
    ]

    prompt = f"""ä½ æ˜¯â€œå¯¹è¯è¯é¢˜èšç±»å™¨â€ã€‚åªè¾“å‡º JSON æ•°ç»„ï¼Œä¸è¦è§£é‡Šï¼Œä¸è¦ Markdownã€‚

    è¾“å…¥ï¼šè‹¥å¹² slotï¼ˆæ¯ä¸ª slot ä»£è¡¨ä¸€æ®µè¿ç»­å¯¹è¯ï¼‰ï¼Œéœ€è¦æŠŠå®ƒä»¬èšç±»æˆæ›´é«˜å±‚ topicã€‚
    æ¯ä¸ª slot å­—æ®µï¼š
    - slot, id, source, is_question, start_id, end_id

    èšç±»è¦æ±‚ï¼š
    1) è¾“å‡ºè‹¥å¹² topicï¼Œæ¯ä¸ª topic åŒ…å«è‹¥å¹² slotã€‚
    2) æ¯ä¸ª slot å¿…é¡»ä¸”åªèƒ½å‡ºç°ä¸€æ¬¡ï¼ˆä¸èƒ½é—æ¼ã€ä¸èƒ½é‡å¤ï¼‰ã€‚
    3) topic åç§°ï¼šä¸­æ–‡åè¯çŸ­è¯­ï¼Œ2~8å­—ï¼Œé¿å…å£è¯­/è™šè¯ã€‚
    ã€topic çš„çº¦æŸã€‘
        1. æ¯ä¸€ä¸ª "topic" å¿…é¡»åªè¡¨è¾¾**ä¸€ä¸ª**æ ¸å¿ƒä¸»é¢˜ï¼Œè€Œä¸æ˜¯ä¸¤ä¸ªæˆ–å¤šä¸ªå¹¶åˆ—çš„ä¸»é¢˜ã€‚
        2. ç¦æ­¢ä½¿ç”¨å¦‚ä¸‹å¹¶åˆ—å†™æ³•ï¼š
           - "XXXä¸YYY"
           - "XXXå’ŒYYY"
           - "XXXåŠYYY"
           - "XXX / YYY"
        3. å¦‚æœä½ å‘ç°æŸä¸ªæ–¹å‘å…¶å®åŒ…å«ä¸¤ä¸ªå­ä¸»é¢˜ï¼Œä¾‹å¦‚ï¼š
           - åŸæœ¬ä½ æƒ³å†™æˆ "ç»æµå‹åŠ›ä¸å…¼èŒ"
           åˆ™è¯·æ”¹å†™ä¸ºä¸¤æ¡ç‹¬ç«‹çš„ä¸»é¢˜ï¼š
           - "ç»æµå‹åŠ›"
           - "å…¼èŒå·¥ä½œ"
    4) åˆå¹¶è¯­ä¹‰æ¥è¿‘çš„ slotï¼ˆå¦‚â€œæ’­å®¢ä»‹ç»/æ’­å®¢å†…å®¹è®¨è®ºâ€åº”å½’ä¸ºåŒä¸€ topicï¼‰ã€‚
    5) topic æ•°é‡ä¸€èˆ¬ 2~8 ä¸ªï¼ˆslot å¾ˆå¤šæ—¶å¯æ›´å¤šï¼‰ã€‚
    6) è¾“å‡ºç»“æ„å¿…é¡»æ˜¯ï¼š
    [
    {{
        "topic": "...",
        "slots": [
        {{"slot":"...","id":1,"source":"...","is_question":false,"start_id":1,"end_id":12}},
        ...
        ]
    }},
    ...
    ]

ä¸‹é¢æ˜¯ slot åˆ—è¡¨ï¼ˆå¿…é¡»è¦†ç›–ä¸”åªè¦†ç›–è¿™äº› slotï¼‰ï¼š
{json.dumps(slot_list_for_prompt, ensure_ascii=False, indent=2)}
"""

    completion = openai.chat.completions.create(
        model="gpt-5.2",
        temperature=0.2,
        messages=[
            {"role": "system", "content": "ä½ åªè¾“å‡ºJSONæ•°ç»„ï¼Œä¸è¦è§£é‡Šï¼Œä¸è¦Markdownã€‚"},
            {"role": "user", "content": prompt},
        ],
    )

    raw = (completion.choices[0].message.content or "").strip()
    result = parse_json_array_loose(raw)

    # âœ… åå¤„ç†æ ¡éªŒï¼šç¡®ä¿æ¯ä¸ª slot åªå‡ºç°ä¸€æ¬¡ï¼ˆä¸ä¿¡ LLMï¼Œè‡ªå·±å…œåº•ï¼‰
    expected = {(s["slot"], s["id"]) for s in slot_items}
    seen = set()
    fixed = []
    for t in result if isinstance(result, list) else []:
        if not isinstance(t, dict) or "slots" not in t:
            continue
        topic = (t.get("topic") or "").strip()
        if not topic:
            continue
        new_slots = []
        for s in (t.get("slots") or []):
            if not isinstance(s, dict):
                continue
            key = (str(s.get("slot","")).strip(), int(s.get("id", -1)))
            if key in expected and key not in seen:
                seen.add(key)
                new_slots.append(s)
        if new_slots:
            fixed.append({"topic": topic, "slots": new_slots})

    # æŠŠé—æ¼çš„ slot å…œåº•å¡åˆ° â€œå…¶ä»–â€
    missing = [s for s in slot_items if (s["slot"], s["id"]) not in seen]
    if missing:
        fixed.append({
            "topic": "å…¶ä»–",
            "slots": [
                {k: m[k] for k in ("slot","id","source","is_question","start_id","end_id")}
                for m in missing
            ]
        })

    return fixed

def build_local_window(history, center_id, window_size=8):
    """
    æŒ‰ id åœ¨ history é‡Œæˆªä¸€æ®µçª—å£ï¼š
    [center_id - window_size, center_id + window_size]
    è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ŒæŒ‰å¯¹è¯é¡ºåºæ‹¼å¥½ï¼Œä¾› LLM åˆ¤æ–­ã€‚
    """
    # 1. å…ˆæŒ‰ id æ’ä¸ªåºï¼Œç¡®ä¿é¡ºåºä¸€è‡´
    sorted_msgs = sorted(history, key=lambda m: m.get("id", 0))

    # 2. æ‰¾åˆ° center_id å¯¹åº”ä½ç½®
    center_idx = None
    for i, m in enumerate(sorted_msgs):
        if m.get("id") == center_id:
            center_idx = i
            break
    if center_idx is None:
        return ""

    start = max(0, center_idx - window_size)
    end = min(len(sorted_msgs), center_idx + window_size + 1)
    window_msgs = sorted_msgs[start:end]

    # 3. æ ¼å¼åŒ–æˆç±»ä¼¼ï¼š
    # [12][user]: xxx
    # [13][bot]: yyy
    lines = []
    for m in window_msgs:
        mid = m.get("id")
        role = m.get("role") or m.get("from") or "user"
        text = (m.get("content") or m.get("text") or "").strip()
        lines.append(f"[{mid}][{role}]: {text}")
    return "\n".join(lines)

def ask_if_resolved(history: List[Dict[str, Any]], slot_obj: Dict[str, Any],
                       followup_horizon: int = 40,
                       max_chars: int = 3000) -> Dict[str, Any]:
    """
    history: åŸå§‹å¯¹è¯ [{id, role, content}, ...]
    slot_obj: {"slot","id","source","is_question","start_id","end_id", ...}
    è¿”å›: {"resolved": bool, "confidence": float}
    """
    # éé—®é¢˜ slotï¼šä½ å¯ä»¥ç›´æ¥æ ‡ False æˆ–è€…è·³è¿‡ä¸åˆ¤
    if not slot_obj.get("is_question", False):
        return {"resolved": False, "confidence": 0.0}

    hist = sort_history(history)

    try:
        start_id = int(slot_obj.get("start_id", slot_obj.get("id")))
        end_id = int(slot_obj.get("end_id", slot_obj.get("id")))
    except:
        return {"resolved": False, "confidence": 0.0}

    slot_name = (slot_obj.get("slot") or "").strip()
    source = (slot_obj.get("source") or "").strip()

    # 1) é—®é¢˜æ®µï¼ˆslot æ®µæœ¬èº«ï¼‰
    seg_msgs = slice_by_id(hist, start_id, end_id)
    seg_text = pack_msgs(seg_msgs, max_chars=max_chars)

    # 2) åç»­æ®µï¼ˆçœ‹æœ‰æ²¡æœ‰å›åº”/è§£å†³ï¼‰
    follow_msgs = followup_after_id(hist, end_id, horizon=followup_horizon)
    follow_text = pack_msgs(follow_msgs, max_chars=max_chars)

    if not seg_text.strip():
        return {"resolved": False, "confidence": 0.0}

    prompt = f"""ä½ æ˜¯ä¸€åä¸¥è°¨çš„å¯¹è¯åˆ†æåŠ©æ‰‹ã€‚åªè¾“å‡º JSONï¼Œä¸è¦è§£é‡Šï¼Œä¸è¦ Markdownã€‚

        ç°åœ¨ç»™ä½ ä¸€æ®µâ€œé—®é¢˜/éœ€æ±‚ slotâ€å¯¹åº”çš„å¯¹è¯æ®µï¼ˆstart_id~end_idï¼‰ï¼Œä»¥åŠå…¶åçš„åç»­å¯¹è¯ã€‚
        è¯·åˆ¤æ–­è¯¥ slot çš„é—®é¢˜/éœ€æ±‚æ˜¯å¦åœ¨åç»­å¯¹è¯ä¸­å·²è¢«åŸºæœ¬å›åº”æˆ–è§£å†³ã€‚

        slot ä¿¡æ¯ï¼š
        - slot: {slot_name}
        - source: {source}
        - start_id: {start_id}
        - end_id: {end_id}

        ã€slot æ®µå†…å®¹ã€‘ï¼ˆè¿™æ®µé‡Œæå‡ºäº†é—®é¢˜/éœ€æ±‚ï¼‰ï¼š
        {seg_text}

        ã€åç»­å¯¹è¯ã€‘ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦è§£å†³ï¼‰ï¼š
        {follow_text}

        åˆ¤å®šæ ‡å‡†ï¼š
        - resolved=trueï¼šåç»­å‡ºç°æ˜ç¡®ã€å…·ä½“ã€ä¸è¯¥ slot é«˜åº¦å¯¹åº”çš„å›ç­”/è§£é‡Š/å¯æ‰§è¡Œæ–¹æ¡ˆ/ç»“è®ºï¼Œæˆ–æ˜ç¡®è¾¾æˆå…±è¯†ï¼ˆå¦‚â€œæ˜ç™½äº†/é‚£å°±è¿™æ ·/OKâ€ï¼‰ã€‚
        - resolved=falseï¼šåç»­æ²¡æœ‰é’ˆå¯¹æ€§å›ç­”ï¼›åªæœ‰å®‰æ…°ã€æ¨¡ç³Šå›åº”ã€è·‘é¢˜ã€é‡å¤æé—®ã€æˆ–ä¿¡æ¯ä¸è¶³å¯¼è‡´æœªå½¢æˆè§£å†³ã€‚

        ä¸¥æ ¼è¾“å‡º JSONï¼š
        {{"resolved": true/false, "confidence": 0.0~1.0}}
        """

    completion = openai.chat.completions.create(
        model="gpt-5.2",
        temperature=0.2,
        messages=[
            {"role": "system", "content": "åªè¾“å‡ºJSONï¼Œä¸è¦è§£é‡Šï¼Œä¸è¦Markdownã€‚"},
            {"role": "user", "content": prompt},
        ],
    )

    raw = (completion.choices[0].message.content or "").strip()

    # é²æ£’æˆªå– { ... }
    l, r = raw.find("{"), raw.rfind("}")
    if l != -1 and r != -1 and r > l:
        raw = raw[l:r+1]

    try:
        obj = json.loads(raw)
        resolved = obj.get("resolved", False)
        conf = obj.get("confidence", 0.0)
        if isinstance(resolved, str):
            resolved = resolved.strip().lower() == "true"
        try:
            conf = float(conf)
        except:
            conf = 0.0
        return {"resolved": bool(resolved), "confidence": conf}
    except Exception:
        return {"resolved": False, "confidence": 0.0}

def refine_slot_resolution(history, topics_with_slots, 
                           max_slots=50):
    """
    history: [{id, role, content}, ...]
    topics_with_slots: Topic_Allocation çš„è¾“å‡ºï¼š
      [
        {"topic": "...", "slots": [ {...}, {...} ]},
        ...
      ]

    è¿”å›ï¼šç»“æ„ç›¸åŒï¼Œä½†æ¯ä¸ª slot çš„ resolved å­—æ®µç»è¿‡äºŒé˜¶æ®µ LLM å¤æ ¸ã€‚
    max_slots: æœ€å¤šå¤æ ¸å¤šå°‘ä¸ª slotï¼Œé˜²æ­¢çˆ†è°ƒç”¨ã€‚
    æ–°é€»è¾‘ï¼šä¸åŒºåˆ† sourceï¼Œåªè¦ is_question == True çš„ slot éƒ½ä¼šå°è¯•åˆ¤æ–­æ˜¯å¦å·²è§£å†³ã€‚
    """
    refined = []
    # ç»Ÿè®¡ä¸€ä¸‹å·²ç»å¤æ ¸äº†å¤šå°‘ä¸ªï¼Œé¿å…å¯¹è¯ç‰¹åˆ«é•¿æ—¶å¤ªè´µ
    checked = 0

    for topic_obj in topics_with_slots:
        topic_name = topic_obj.get("topic")
        slots = topic_obj.get("slots", []) or []
        new_slots = []

        for s in slots:
            s2 = dict(s)

            # 1) ä¸æ˜¯é—®å¥ï¼ˆis_question != Trueï¼‰ï¼Œä¸éœ€è¦åˆ¤æ–­è§£å†³ä¸å¦
            if not s2.get("is_question", False):
                # æ˜ç¡®æ ‡è®°ï¼šä¸æ˜¯é—®é¢˜ï¼Œè‡ªç„¶ä¸å­˜åœ¨â€œå·²è§£å†³â€
                s2["resolved"] = False
                new_slots.append(s2)
                continue

            # 2) æ˜¯é—®å¥ï¼Œä½†å·²ç»è¶…è¿‡è°ƒç”¨ä¸Šé™ï¼Œé¿å…èŠ±å¤ªå¤š token
            if checked >= max_slots:
                # è¶…ä¸Šé™ï¼Œä¸å†è°ƒç”¨ LLMï¼Œä¿ç•™ is_question=True ä½† resolved é»˜è®¤ False
                s2["resolved"] = False
                new_slots.append(s2)
                continue

            final_resolved = ask_if_resolved(history, s2)
            s2["resolved"] = bool(final_resolved.get("resolved", False))
            checked += 1

            new_slots.append(s2)


        refined.append({
            "topic": topic_name,
            "slots": new_slots,
        })

    return refined

def extract_wordcloud(
    history: List[Dict[str, Any]],
    topics_with_slots: List[Dict[str, Any]],
    max_words: int = 30,
    limit_slots: Optional[int] = None,
    max_chars: int = 2000,
):

    # 1) history æ’åº + id -> index
    hist = sorted(history, key=lambda m: int(m.get("id", 0)))

    # 2) ä¸»å¾ªç¯ï¼šslot -> è°ƒç”¨ LLM æŠ½è¯äº‘
    done = 0
    for topic_obj in topics_with_slots:
        topic_name = (topic_obj.get("topic") or "").strip()
        slots = topic_obj.get("slots") or []
        if not isinstance(slots, list):
            continue

        for s in slots:
            if not isinstance(s, dict):
                continue

            # æµ‹è¯•é™é¢
            if limit_slots is not None and done >= limit_slots:
                return topics_with_slots

            # è·³è¿‡å·²å­˜åœ¨
            if isinstance(s.get("wordcloud"), list) and len(s["wordcloud"]) > 0:
                continue

            slot_name = (s.get("slot") or "").strip()

            try:
                start_id = int(s.get("start_id", s.get("id")))
                end_id = int(s.get("end_id", s.get("id")))
            except Exception:
                continue

            seg_msgs = slice_by_id(hist, start_id, end_id)
            local_ctx = pack_msgs(seg_msgs, max_chars=max_chars)

            # å…œåº•ï¼šå¦‚æœè¿™ä¸ªæ®µæ²¡å†…å®¹ï¼Œå°±åˆ«æŠ½äº†ï¼ˆæˆ–é€€å› sentenceï¼‰
            if not local_ctx.strip():
                s["wordcloud"] = []
                continue

            k = max(10, min(int(max_words), 50))

            prompt = f"""ä½ æ˜¯ä¸€åä¸¥æ ¼çš„å…³é”®è¯æŠ½å–åŠ©æ‰‹ï¼Œåªè¾“å‡º JSON æ•°ç»„ã€‚
            ä¸ºä¸‹é¢çš„å±€éƒ¨å¯¹è¯ç‰‡æ®µç”Ÿæˆè¯äº‘å…³é”®è¯ã€‚

            ã€ä¸€çº§ä¸»é¢˜ã€‘{topic_name}
            ã€å½“å‰slotã€‘{slot_name}
            ã€slotèŒƒå›´ã€‘start_id={start_id}, end_id={end_id}

            ã€slotæ®µå¯¹è¯ã€‘
            {local_ctx}

            è¦æ±‚ï¼š
            - æŠ½å–ä¸è¶…è¿‡ {k} ä¸ªå…³é”®è¯/çŸ­è¯­ï¼ˆä¸­æ–‡ä¸ºä¸»ï¼Œ2~6 ä¸ªå­—ä¸ºä¸»ï¼Œåˆ«æ•´å¥ï¼‰
            - è¿‡æ»¤è™šè¯ï¼ˆå¦‚ æˆ‘ä»¬/ä½ ä»¬/ç„¶å/å°±æ˜¯/å…¶å®/å¯èƒ½/å¤§å®¶ ç­‰ï¼‰
            - æ¯ä¸ªè¯ç»™æƒé‡ weightï¼ˆ0~1ï¼‰ï¼Œï¼Œè¶Šé‡è¦è¶Šå¤§

            ä¸¥æ ¼è¾“å‡º JSON æ•°ç»„ï¼Œä¾‹å¦‚ï¼š
            [{{"word":"å©šå§»è§‚å¿µ","weight":0.92}},{{"word":"ç¦»å©šæˆæœ¬","weight":0.76}}]
            """

            # --- è°ƒ OpenAI ---
            completion = openai.chat.completions.create(
                model="gpt-5.2",
                temperature=0.2,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸¥æ ¼çš„å…³é”®è¯æŠ½å–åŠ©æ‰‹ï¼Œåªè¾“å‡º JSONã€‚"},
                    {"role": "user", "content": prompt},
                ],
            )

            raw = (completion.choices[0].message.content or "").strip()
            arr = parse_json_array_loose(raw)

            # 3) è½»åº¦æ ¡éªŒ + å»é‡ + æˆªæ–­
            out = []
            seen = set()
            for it in arr:
                if not isinstance(it, dict):
                    continue
                w = str(it.get("word", "")).strip()
                if not w or w in seen:
                    continue
                try:
                    weight = float(it.get("weight", 0.0))
                except Exception:
                    weight = 0.0
                weight = max(0.0, min(1.0, weight))
                out.append({"word": w, "weight": weight})
                seen.add(w)
                if len(out) >= k:
                    break

            s["wordcloud"] = out
            done += 1

    return topics_with_slots

def pipeline_on_messages(messages):

    # 1. å¦‚æœæ²¡æœ‰ idï¼Œå°±é¡ºæ‰‹è¡¥ä¸€éé€’å¢ idï¼Œä¿è¯åé¢èƒ½ç”¨ id åšæ—¶é—´è½´
    normalized_messages = []
    for idx, m in enumerate(messages, start=1):
        normalized_messages.append({
            "id": m.get("id", idx),
            "role": m.get("role") or m.get("from") or "user",
            "content": (m.get("content") or m.get("text") or "").strip()
        })

    # ğŸ§  ç¬¬ä¸€æ­¥ï¼šè¯­ä¹‰é¢„æ‰«æï¼ˆç²—æŠ½ï¼‰
    pre_scan_result = Semantic_pre_scanning(normalized_messages)
    # pre_scan_result ç»“æ„åº”è¯¥å°±æ˜¯ä½ ä¹‹å‰ all_results çš„é‚£ä¸€ç±» topic/slots åˆ—è¡¨

    # ğŸ§¹ ç¬¬äºŒæ­¥ï¼šä¸»é¢˜æ¸…æ´— / å»å™ª / åˆå¹¶
    cleaned_topics = Topic_cleaning(normalized_messages, pre_scan_result)

    # ğŸ¯ ç¬¬ä¸‰æ­¥ï¼šæŠŠ slot é‡æ–°å¯¹é½åˆ°å…·ä½“çš„æ¶ˆæ¯ / turn ä¸Š
    allocated_topics = Topic_Allocation(normalized_messages, cleaned_topics)


    # ğŸŒˆ ç¬¬å››æ­¥ï¼šæ˜¯å¦è§£å†³é—®é¢˜
    refined_result = refine_slot_resolution(messages, allocated_topics,
                                        max_slots=50)

    # ğŸ¨ ç¬¬äº”æ­¥ï¼šç»™æ¯ä¸ª topic åˆ†é…é¢œè‰²
    colored_results = assign_colors(refined_result)

    # â›°ï¸ ç¬¬å…­æ­¥ï¼šæŒ‰æ—¶é—´è½´åˆ‡æ®µï¼Œç»™å‰ç«¯ç”»å¸¦çŠ¶å›¾
    segmented_results = segment_by_timeline(colored_results)

    return segmented_results

# ç”Ÿæˆ embedding
def get_embedding(text):
    emb = openai.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return np.array(emb.data[0].embedding, dtype="float32")

# ===== åˆå§‹åŒ– Memoryï¼ˆå¯ä»¥æ¢æˆæ•°æ®åº“ï¼‰ =====
user_memory_db = {}  # ç¤ºä¾‹: {user_id: {key: value}}

# ===== Memoryæ“ä½œå‡½æ•° =====
# å¾—åˆ°ç”¨æˆ·Memory
def get_user_memory(user_id):
    """
    è¿”å›æ•´ç†å¥½çš„æ–‡æœ¬ï¼Œç”¨äºæ‹¼æ¥åˆ°prompt
    """
    if user_id not in user_memory_db:
        return ""
    memory = user_memory_db[user_id]
    return "\n".join([f"{k}: {v}" for k, v in memory.items()])

# æ›´æ–°ç”¨æˆ·Memory
def update_user_memory(user_id, key, value):
    global user_memory_db
    if user_id not in user_memory_db:
        user_memory_db[user_id] = {}
    
    # å¦‚æœ key å·²å­˜åœ¨ï¼Œåšâ€œè¿½åŠ â€è€Œä¸æ˜¯è¦†ç›–
    if key in user_memory_db[user_id]:
        old_value = user_memory_db[user_id][key]
        if value not in old_value:
            user_memory_db[user_id][key] = f"{old_value}; {value}"
    else:
        user_memory_db[user_id][key] = value
    
    user_memory_db[user_id]['last_updated'] = str(datetime.now())

    print(f"Memoryæ›´æ–°ï¼š{user_id} - {key} - {value}")
    print("Memoryåº“:" ,user_memory_db)

def extract_memory_from_text(user_id, new_sentence):
    """
    è°ƒç”¨ GPT è‡ªåŠ¨ä»æ–‡æœ¬ä¸­æŠ½å–å…³é”®ä¿¡æ¯ï¼ˆå¦‚åå­—ã€å…´è¶£ã€åå¥½ç­‰ï¼‰
    å¹¶æ›´æ–° Memory
    """
    prompt = f"""
    è¯·ä»ä¸‹é¢çš„æ–‡æœ¬ä¸­æå–ç”¨æˆ·å¯èƒ½æƒ³è®°ä½çš„å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬å§“åã€å…´è¶£ã€çˆ±å¥½ã€å­¦ä¹ ç›®æ ‡ç­‰ã€‚
    è¾“å‡ºå¿…é¡»æ˜¯æ ‡å‡† JSON å¯¹è±¡ï¼Œä¸¥ç¦åŒ…å«ä»£ç å—æ ‡è®°ï¼ˆå¦‚```jsonï¼‰æˆ–å¤šä½™æ–‡å­—ï¼Œé”®åéšæ„ä½†è¦èƒ½æè¿°ä¿¡æ¯ï¼Œå¦‚ï¼š
    {{
        "name": "Alice",
        "favorite_language": "Python"
    }}

    æ–‡æœ¬å†…å®¹ï¼š
    {new_sentence}
    """

    completion = openai.chat.completions.create(
        model="gpt-5.2",
        temperature=0,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€åä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ]
    )
    result = (completion.choices[0].message.content)
    # æ›´æ–°è§£æç»“æœåˆ°Memory
    try:
        memory_data = json.loads(result)
        for key, value in memory_data.items():
            update_user_memory(user_id, key, value)
    except json.JSONDecodeError:
        # å‡ºç°è§£æé”™è¯¯æ—¶å¯ä»¥å¿½ç•¥æˆ–è®°å½•æ—¥å¿—
        print("Memory JSONè§£æå¤±è´¥:", result)

# ===== GPT + Memory + RAGå‡½æ•° =====
def talk_to_chatbot(user_id, content, source, history_msgs, top_k=3):
    global index

    # 1. å…ˆæ£€ç´¢Memory
    memory_context = get_user_memory(user_id)

    # 2. ä¸ºç”¨æˆ·è¾“å…¥ç”Ÿæˆ embedding
    query_emb = get_embedding(content).reshape(1, -1)

    # 3. æ£€ç´¢ç›¸å…³ Memory
    related_context = ""
    if index is not None and memory_context:
        D, I = index.search(query_emb, top_k)
        # ç®€å•ç¤ºä¾‹ï¼šç”¨ç´¢å¼•å¯¹åº”çš„ Memory è¡Œï¼ˆå‡è®¾ memory_text åˆ†è¡Œå­˜å‚¨ï¼‰
        memory_lines = memory_context.split("\n")
        related_context = "\n".join([memory_lines[i] for i in I[0] if i < len(memory_lines)])

    # 4.å…ˆæŠŠå†å²å¯¹è¯æ•´ç†æˆæ–‡æœ¬
    history_text = "\n".join(
    [f"{msg.get('from', 'user').capitalize()}: {msg.get('text', '')}" for msg in history_msgs])

    # 5. ç»„ç»‡ promptï¼ŒæŠŠ Memory å’Œ RAG æ£€ç´¢å†…å®¹éƒ½æ‹¼è¿›å»
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€åå¯¹è¯åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä¸ç”¨æˆ·è¿›è¡Œæ²Ÿé€š, è¯·æ ¹æ®ç”¨æˆ·çš„è¾“å…¥ï¼Œåˆç†å›ç­”ï¼Œå¹¶ä¿æŒæ²Ÿé€šè¿è´¯ã€‚"},
        {"role": "user", "content": f"""
        ä¸‹é¢æ˜¯ä¸æœ¬é—®é¢˜ç›¸å…³çš„å†å²å¯¹è¯ï¼š{history_text}
        ç”¨æˆ·ä¿¡æ¯ï¼š
        {memory_context}
        ç›¸å…³ Memory æ£€ç´¢å†…å®¹ï¼š
        {related_context}
        ç°åœ¨ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š
        {content}"""}
        ]
    
    # 6. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›å¤ 
    completion = openai.chat.completions.create(
        model="gpt-5.2",
        temperature=0.5,
        messages=messages,
        )
    result = (completion.choices[0].message.content)

    # 7. æ›´æ–° Memoryï¼ˆé•¿æœŸè®°å¿†ï¼‰åˆ° FAISS
    if index is not None and memory_context:
        memory_text = "\n".join([f"{k}: {v}" for k, v in user_memory_db[user_id].items()])
        emb = get_embedding(memory_text).reshape(1, -1)
        index.add(emb)

    # 8. è‡ªåŠ¨æŠ½å–æœ€æ–° Memory ä¿¡æ¯
    extract_memory_from_text(user_id, content)

    return result

def create_theme_variables(result_dict):
    for theme, questions in result_dict.items():
        # åˆ›å»ºåˆæ³•å˜é‡åï¼ˆç§»é™¤éæ³•å­—ç¬¦ï¼‰
        var_name = theme.replace(" ", "_").replace("ï¼š", "").replace("-", "_")
        globals()[var_name] = questions
    
    print(f"{var_name} = {questions}")  # å¯é€‰ï¼šæ‰“å°å‡ºæ¥
