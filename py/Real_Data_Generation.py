import json
import os
from LLM_Extraction import llm_extract_information_incremental, Semantic_pre_scanning, Topic_cleaning, Topic_Allocation
from Methods import assign_colors, merge_topics_timeline

CHECKPOINT_PATH = "py/conversation_example/ChatGPT-DST-checkpoint.json"
FINAL_PATH = "py/conversation_example/ChatGPT-DST-processed.json"


def safe_process_llm_result(result, role, id_counter):
    """ç¡®ä¿ LLM è¿”å›ç»“æœæ˜¯åˆ—è¡¨å­—å…¸ï¼Œå¹¶ç»™æ¯ä¸ª slot æ·»åŠ  source å’Œ id"""
    if isinstance(result, str):
        try:
            result = json.loads(result)
        except json.JSONDecodeError:
            print("âš ï¸ è­¦å‘Šï¼šLLM è¿”å›çš„å­—ç¬¦ä¸²æ— æ³•è§£æä¸º JSONï¼Œå°†ä½œä¸ºå•æ¡æ–‡æœ¬å¤„ç†")
            result = [{"topic": "unknown", "slots": [{"slot": result, "source": role, "id": id_counter}]}]

    if isinstance(result, dict):
        result = [result]

    for topic in result:
        slots = topic.get("slots", [])
        if not isinstance(slots, list):
            slots = []
            topic["slots"] = slots
        for slot in slots:
            slot["source"] = role
            slot["id"] = id_counter

    return result


def load_checkpoint():
    """åŠ è½½ä¸­æ–­ç‚¹æ–‡ä»¶"""
    if os.path.exists(CHECKPOINT_PATH):
        try:
            with open(CHECKPOINT_PATH, "r", encoding="utf-8") as f:
                checkpoint = json.load(f)
            print(f"âœ… å·²åŠ è½½ä¸­æ–­ç‚¹ï¼Œæ¢å¤åˆ°ç¬¬ {checkpoint.get('last_id', 0)} æ¡è®°å½•ã€‚")
            return checkpoint
        except Exception as e:
            print("âš ï¸ åŠ è½½ä¸­æ–­ç‚¹å¤±è´¥ï¼Œé‡æ–°å¼€å§‹:", e)
    return {"merged_results_global": [], "last_id": 0}


def save_checkpoint(merged_results_global, last_id):
    """ä¿å­˜ä¸­æ–­ç‚¹ï¼ˆåŒ…å«å·²åˆå¹¶å¹¶åˆ†é…é¢œè‰²çš„å®Œæ•´ç»“æœï¼‰"""
    checkpoint_data = {
        "merged_results_global": merged_results_global,
        "last_id": last_id
    }
    with open(CHECKPOINT_PATH, "w", encoding="utf-8") as f:
        json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ å·²ä¿å­˜ä¸­æ–­ç‚¹ï¼ˆå«é¢œè‰²ï¼‰ï¼šå¤„ç†åˆ°ç¬¬ {last_id} æ¡æ¶ˆæ¯ã€‚")


def parse_conversation(file_path):
    """è¯»å–å¯¹è¯æ–‡æœ¬ï¼Œç”Ÿæˆæ¶ˆæ¯åˆ—è¡¨"""
    messages = []
    id_counter = 1
    role = None
    content = ""

    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    for line in lines:
        line = line.strip()
        if line.startswith("## Prompt:") or line.startswith("## Promptï¼š"):
            if content and role:
                messages.append({"id": id_counter, "role": role, "content": content.strip()})
                id_counter += 1
                content = ""
            role = "user"
            continue

        elif line.startswith("## Response:") or line.startswith("## Responseï¼š"):
            if content and role:
                messages.append({"id": id_counter, "role": role, "content": content.strip()})
                id_counter += 1
                content = ""
            role = "bot"
            continue

        if role:
            content += line + "\n"

    if content and role:
        messages.append({"id": id_counter, "role": role, "content": content.strip()})
    return messages

def chunk_text(text, max_chars=40000):
    """æŠŠé•¿æ–‡æœ¬åˆ‡æˆå®‰å…¨çš„å¤šæ®µ"""
    chunks = []
    current_chunk = []
    current_length = 0

    for line in text.split("\n"):
        line_length = len(line)
        if current_length + line_length > max_chars:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = [line]
            current_length = line_length
        else:
            current_chunk.append(line)
            current_length += line_length

    if current_chunk:
        chunks.append(current_chunk)

    return chunks

def segment_by_timeline(topics):

    # 1. å…ˆæŠŠæ‰€æœ‰ slot æ‰“å¹³ï¼Œå˜æˆä¸€ä¸ªæŒ‰å¥å­ç²’åº¦çš„åˆ—è¡¨
    flat_items = []
    for t in topics:
        topic_name = t.get("topic")
        topic_color = t.get("color")
        for s in t.get("slots", []):
            flat_items.append({
                "topic": topic_name,
                "topic_color": topic_color,
                "id": int(s["id"]),
                "sentence": s.get("sentence"),
                "slot": s.get("slot"),
                "color": s.get("color"),
            })

    # 2. æŒ‰ id ä»å°åˆ°å¤§æ’åº â€”â€” ä¸¥æ ¼æ—¶é—´é¡ºåº
    flat_items.sort(key=lambda x: x["id"])

    segments = []
    current_topic = None
    current_topic_color = None
    current_slots = []

    def flush_segment():
        nonlocal current_topic, current_topic_color, current_slots
        if not current_topic or not current_slots:
            return
        
        # æ®µå†…æŒ‰ id å†ä¿é™©æ’ä¸€ä¸‹ï¼Œå¹¶å¯é€‰åšå»é‡ï¼ˆæŒ‰ sentenceï¼‰
        best_by_slot = {}
        for s in current_slots:
            slot_name = s.get("slot")
            if not slot_name:
                continue
            if slot_name not in best_by_slot:
                best_by_slot[slot_name] = s
            else:
                # å¦‚æœå½“å‰è¿™æ¡çš„ id æ›´å°ï¼Œå°±æ›¿æ¢
                if s["id"] < best_by_slot[slot_name]["id"]:
                    best_by_slot[slot_name] = s
                    
         # æ®µå†…æŒ‰ id å†æ’ä¸€æ¬¡
        uniq_slots = sorted(best_by_slot.values(), key=lambda x: x["id"])

        if uniq_slots:
            segments.append({
                "topic": current_topic,
                "slots": uniq_slots,
                "color": current_topic_color,
            })
        current_topic = None
        current_topic_color = None
        current_slots = []

    # 3. æ²¿æ—¶é—´è½´æ‰«æï¼Œtopic ä¸€å˜å°±åˆ‡ä¸€æ®µ
    for item in flat_items:
        t = item["topic"]
        tc = item["topic_color"]
        slot = {
            "sentence": item["sentence"],
            "slot": item["slot"],
            "id": item["id"],
            "color": item["color"],
        }

        if current_topic is None:
            # ç¬¬ä¸€æ¡
            current_topic = t
            current_topic_color = tc
            current_slots = [slot]
        else:
            if t == current_topic:
                # åŒä¸€ä¸ª topicï¼Œå½’åˆ°å½“å‰æ®µ
                current_slots.append(slot)
            else:
                # topic å‘ç”Ÿåˆ‡æ¢ï¼Œå…ˆæ”¶å°¾å‰ä¸€æ®µï¼Œå†å¼€æ–°æ®µ
                flush_segment()
                current_topic = t
                current_topic_color = tc
                current_slots = [slot]

    # æ”¶æœ€åä¸€æ®µ
    flush_segment()
    return segments


def process_conversation(file_path):

    messages = parse_conversation(file_path)       # list[dict]: {id, role, content}
    lines = [f"[{m['id']}] ({m['role']}) {m['content'].strip()}" for m in messages if m.get('content')]
    full_text = "\n".join(lines)
    chunks = chunk_text(full_text, max_chars=40000)   # æ¯æ®µçº¦ 1/3 æ¨¡å‹ä¸Šé™
    all_results = []
    
    for i, chunk in enumerate(chunks, 1):
        print(f"ğŸ§  ç¬¬ {i}/{len(chunks)} æ®µæŠ½å–ä¸­...")
        # ç”Ÿæˆ chunk æ ¼å¼ä¿æŒç»“æ„çš„å¯¹è¯åˆ—è¡¨
        chunk_messages = []
        id_counter = 1
        for line in chunk:
            parts = line.split('] (')
            if len(parts) == 2:
                id_part, content = parts
                role = content.split(") ")[0]
                text = content.split(") ")[1] if len(content.split(") ")) > 1 else ""
                chunk_messages.append({"id": id_counter, "role": role, "content": text.strip()})
                id_counter += 1
        # print("chunk_messages:", chunk_messages)
        result = Semantic_pre_scanning(chunk_messages)  
        print("result:", result)        
        all_results.extend(result)
    clear_results = Topic_cleaning(messages, all_results)
    print("clear_results:", clear_results)
    last_result = Topic_Allocation(messages, clear_results)
    print("last_result:", last_result)
    colored_results = assign_colors(last_result)   
    print("colored_results:", colored_results)
    segmented_results = segment_by_timeline(colored_results)
    with open(FINAL_PATH, "w", encoding="utf-8") as f:
        json.dump(segmented_results, f, ensure_ascii=False, indent=2)
    print(f"âœ… å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜ï¼š{FINAL_PATH}")
    return segmented_results


    # print(f"ğŸ§© å…± {total} æ¡æ¶ˆæ¯ï¼Œå‡†å¤‡ä»ç¬¬ {last_id + 1} æ¡ç»§ç»­ã€‚")

    # # --- åˆå§‹åŒ–å†å²è®°å½• ---
    # history_so_far = []

    # for msg in messages:
    #     id_counter = msg.get("id", 1)
    #     role = msg.get("role", "user")
    #     text = msg.get("content", "").strip()

    #     if id_counter <= last_id or not text:
    #                 history_so_far.append(msg)
    #                 continue

    #     try:
    #         print(f"ğŸ§  æ­£åœ¨å¤„ç†ç¬¬ {id_counter}/{total} æ¡æ¶ˆæ¯ï¼ˆ{role}ï¼‰...")
    #         result = llm_extract_information_incremental(history_so_far,msg, existing_topics=merged_results_global)
    #         safe_result = safe_process_llm_result(result, role, id_counter)

    #         # åˆå¹¶ç»“æœ
    #         merged_results_global = merge_topics_timeline(merged_results_global + safe_result)

    #         # åˆ†é…é¢œè‰²
    #         colored_results = assign_colors(merged_results_global)

    #         # æ¯å¤„ç†ä¸€æ¡è‡ªåŠ¨ä¿å­˜
    #         save_checkpoint(merged_results_global, id_counter)

    #     except Exception as e:
    #         print(f"âŒ ç¬¬ {id_counter} æ¡å¤„ç†å¤±è´¥ï¼š{e}")
    #         save_checkpoint(merged_results_global, id_counter)
    #         continue  # ä¿æŒå¥å£®æ€§



    # # ä¿å­˜æœ€ç»ˆæ–‡ä»¶
    # with open(FINAL_PATH, "w", encoding="utf-8") as f:
    #     json.dump(colored_results, f, ensure_ascii=False, indent=2)
    # print(f"âœ… å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜ï¼š{FINAL_PATH}")

    # åˆ é™¤ä¸­æ–­ç‚¹ï¼ˆå¯é€‰ï¼‰
    # os.remove(CHECKPOINT_PATH)

    


if __name__ == "__main__":
    file_path = "py/conversation_example/ChatGPT-DST copy.txt"
    final_data = process_conversation(file_path)
    # ç”Ÿæˆä¹‹åçš„æ•°æ®è¦è½¬ç§»åˆ°publicç›®å½•ä¸‹
    # åˆ æ‰æœ€å¤–å±‚å­—å…¸
