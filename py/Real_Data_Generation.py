import os

import json
from typing import Any, Dict, List
from LLM_Extraction import Score_turn_importance ,Semantic_pre_scanning, Topic_cleaning, Topic_Allocation,refine_slot_resolution
from Methods import assign_colors, parse_conversation, parse_meeting_conversation, split_history_by_turns

CHECKPOINT_PATH = "py/conversation_example/ChatGPT-xinli_result.json"
FINAL_PATH = "py/conversation_example/ChatGPT-xinli_processed.json"
FINAL_PATH_SCORE = "py/conversation_example/meeting_score.json"


def safe_process_llm_result(result, role, id_counter):
    """ç¡®ä¿ LLM è¿”å›ç»“æœæ˜¯åˆ—è¡¨å­—å…¸ï¼Œå¹¶ç»™æ¯ä¸ª slot æ·»åŠ  source å’Œ id"""
    if isinstance(result, str):
        try:
            result = json.loads(result)
        except json.JSONDecodeError:
            print("âš ï¸ è­¦å‘Šï¼šLLM è¿”å›çš„å­—ç¬¦ä¸²æ— æ³•è§£æä¸º JSONï¼Œå°†ä½œä¸ºå•æ¡æ–‡æœ¬å¤„ç†")
            result = [{"topic": "unknown", "slots": [{"slot": result, "source": role, "id": id_counter}]}]

    if isinstance(result, dict):
        result = [result]

    for topic in result:
        slots = topic.get("slots", [])
        if not isinstance(slots, list):
            slots = []
            topic["slots"] = slots
        for slot in slots:
            slot["source"] = role
            slot["id"] = id_counter

    return result


def load_checkpoint():
    """åŠ è½½ä¸­æ–­ç‚¹æ–‡ä»¶"""
    if os.path.exists(CHECKPOINT_PATH):
        try:
            with open(CHECKPOINT_PATH, "r", encoding="utf-8") as f:
                checkpoint = json.load(f)
            print(f"âœ… å·²åŠ è½½ä¸­æ–­ç‚¹ï¼Œæ¢å¤åˆ°ç¬¬ {checkpoint.get('last_id', 0)} æ¡è®°å½•ã€‚")
            return checkpoint
        except Exception as e:
            print("âš ï¸ åŠ è½½ä¸­æ–­ç‚¹å¤±è´¥ï¼Œé‡æ–°å¼€å§‹:", e)
    return {"merged_results_global": [], "last_id": 0}


def save_checkpoint(merged_results_global, last_id):
    """ä¿å­˜ä¸­æ–­ç‚¹ï¼ˆåŒ…å«å·²åˆå¹¶å¹¶åˆ†é…é¢œè‰²çš„å®Œæ•´ç»“æœï¼‰"""
    checkpoint_data = {
        "merged_results_global": merged_results_global,
        "last_id": last_id
    }
    with open(CHECKPOINT_PATH, "w", encoding="utf-8") as f:
        json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ å·²ä¿å­˜ä¸­æ–­ç‚¹ï¼ˆå«é¢œè‰²ï¼‰ï¼šå¤„ç†åˆ°ç¬¬ {last_id} æ¡æ¶ˆæ¯ã€‚")

def chunk_text(text, max_chars=40000):
    """æŠŠé•¿æ–‡æœ¬åˆ‡æˆå®‰å…¨çš„å¤šæ®µ"""
    chunks = []
    current_chunk = []
    current_length = 0

    for line in text.split("\n"):
        line_length = len(line)
        if current_length + line_length > max_chars:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = [line]
            current_length = line_length
        else:
            current_chunk.append(line)
            current_length += line_length

    if current_chunk:
        chunks.append(current_chunk)

    return chunks

def segment_by_timeline(topics):
    # 1. å…ˆæŠŠæ‰€æœ‰ slot æ‰“å¹³ï¼Œå˜æˆä¸€ä¸ªæŒ‰å¥å­ç²’åº¦çš„åˆ—è¡¨
    flat_items = []
    for t in topics:
        topic_name = t.get("topic")
        topic_color = t.get("color")
        for s in t.get("slots", []):
            flat_items.append({
                "topic": topic_name,
                "topic_color": topic_color,
                "id": int(s["id"]),
                "sentence": s.get("sentence"),
                "slot": s.get("slot"),
                "color": s.get("color"),
                "sentiment": s.get("sentiment"),
                "source": s.get("source"),
                "is_question": s.get("is_question", False),
                "resolved": s.get("resolved", False),
            })

    # 2. æŒ‰ id ä»å°åˆ°å¤§æ’åº â€”â€” ä¸¥æ ¼æ—¶é—´é¡ºåº
    flat_items.sort(key=lambda x: x["id"])

    segments = []
    current_topic = None
    current_topic_color = None
    current_slots = []

    def flush_segment():
        nonlocal current_topic, current_topic_color, current_slots
        if not current_topic or not current_slots:
            return
        
        # æ®µå†…æŒ‰ id å†ä¿é™©æ’ä¸€ä¸‹ï¼Œå¹¶å¯é€‰åšå»é‡ï¼ˆæŒ‰ sentenceï¼‰
        best_by_slot = {}
        for s in current_slots:
            slot_name = s.get("slot")
            if not slot_name:
                continue
            if slot_name not in best_by_slot:
                best_by_slot[slot_name] = s
            else:
                # å¦‚æœå½“å‰è¿™æ¡çš„ id æ›´å°ï¼Œå°±æ›¿æ¢
                if s["id"] < best_by_slot[slot_name]["id"]:
                    best_by_slot[slot_name] = s
                    
        # æ®µå†…æŒ‰ id å†æ’ä¸€æ¬¡
        uniq_slots = sorted(best_by_slot.values(), key=lambda x: x["id"])

        if uniq_slots:
            segments.append({
                "topic": current_topic,
                "slots": uniq_slots,
                "color": current_topic_color,
            })
        current_topic = None
        current_topic_color = None
        current_slots = []

    # 3. æ²¿æ—¶é—´è½´æ‰«æï¼Œtopic ä¸€å˜å°±åˆ‡ä¸€æ®µ
    for item in flat_items:
        t = item["topic"]
        tc = item["topic_color"]
        slot = {
            "sentence": item["sentence"],
            "slot": item["slot"],
            "id": item["id"],
            "color": item["color"],
            "sentiment": item["sentiment"],
            "source": item["source"],
            "is_question": item.get("is_question", False),
            "resolved": item.get("resolved", False),
        }

        if current_topic is None:
            # ç¬¬ä¸€æ¡
            current_topic = t
            current_topic_color = tc
            current_slots = [slot]
        else:
            if t == current_topic:
                # åŒä¸€ä¸ª topicï¼Œå½’åˆ°å½“å‰æ®µ
                current_slots.append(slot)
            else:
                # topic å‘ç”Ÿåˆ‡æ¢ï¼Œå…ˆæ”¶å°¾å‰ä¸€æ®µï¼Œå†å¼€æ–°æ®µ
                flush_segment()
                current_topic = t
                current_topic_color = tc
                current_slots = [slot]

    # æ”¶æœ€åä¸€æ®µ
    flush_segment()
    return segments

def process_score(file_path):
    # 1) è§£æåŸå§‹å¯¹è¯ï¼š[{id, role, content}, ...]
    messages = parse_meeting_conversation(file_path)

    # 2) æŒ‰æ¡æ•°åˆ‡æˆå¤šä¸ª chunkï¼Œä¾‹å¦‚æ¯æ®µ 80 è½®
    history_chunks = split_history_by_turns(messages, max_turns=80)

    scored_all = []
    for i, chunk in enumerate(history_chunks, 1):
        print(f"ğŸ§  ç¬¬ {i}/{len(history_chunks)} æ®µæ‰“åˆ†ä¸­ï¼ŒåŒ…å« {len(chunk)} æ¡å¯¹è¯...")

        # 3) å¯¹æ¯ä¸ª chunk ç‹¬ç«‹æ‰“åˆ†
        scored_chunk = Score_turn_importance(chunk)

        # 4) æ‹¼å›å¤§åˆ—è¡¨ï¼ˆæ³¨æ„ï¼šchunk é‡Œçš„ id æ˜¯åŸå§‹ idï¼Œæ²¡æœ‰è¢«æ”¹åŠ¨ï¼‰
        scored_all.extend(scored_chunk)

    # 5) å†™å…¥æ–‡ä»¶ï¼ˆscored_all é•¿åº¦åº”è¯¥ == messages é•¿åº¦ï¼‰
    with open(FINAL_PATH_SCORE, "w", encoding="utf-8") as f:
        json.dump(scored_all, f, ensure_ascii=False, indent=2)

    print(f"âœ… å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜ï¼š{FINAL_PATH_SCORE}")
    return scored_all



def process_conversation(file_path):
    # userå’Œllmçš„å¯¹è¯æ¨¡å¼
    messages = parse_conversation(file_path)       # list[dict]: {id, role, content}
    lines = []
    for m in messages:
        content = (m.get("content") or "").replace("\n", " ").strip()
        if not content:
            continue
        lines.append(f"[{m['id']}] ({m['role']}) {content}")

    full_text = "\n".join(lines)
    chunks = chunk_text(full_text, max_chars=40000)   # æ¯æ®µçº¦ 1/3 æ¨¡å‹ä¸Šé™
    all_results = []
    
    for i, chunk in enumerate(chunks, 1):
        print(f"ğŸ§  ç¬¬ {i}/{len(chunks)} æ®µæŠ½å–ä¸­...")
        # ç”Ÿæˆ chunk æ ¼å¼ä¿æŒç»“æ„çš„å¯¹è¯åˆ—è¡¨
        chunk_messages = []
        for line in chunk:  # chunk æ˜¯ä¸€å † "[id] (role) content" çš„è¡Œ
            try:
                # "[12] (user) hello world"
                id_part, rest = line.split("] (", 1)   # id_part = "[12"
                mid = int(id_part[1:])                 # å»æ‰å·¦ä¸­æ‹¬å·ï¼Œè½¬ä¸º int
                role, text = rest.split(") ", 1)       # role = "user", text = "hello world"
            except ValueError:
                # è¡Œæ ¼å¼ä¸å¯¹å°±è·³è¿‡ï¼Œé¿å…ç‚¸
                continue
            chunk_messages.append({
                "id": mid,
                "role": role,
                "content": text.strip()
            })
        # print("chunk_messages:", chunk_messages)
        if not chunk_messages:
            print(f"âš ï¸ ç¬¬ {i} æ®µæ²¡æœ‰è§£æå‡ºæœ‰æ•ˆå¯¹è¯ï¼Œè·³è¿‡ Semantic_pre_scanning")
            continue

        result = Semantic_pre_scanning(chunk_messages)  
        print("result:", result)        
        all_results.extend(result)

    clear_results = Topic_cleaning(messages, all_results)
    print("clear_results:", clear_results)
    last_result = Topic_Allocation(messages, clear_results)
    print("last_result:", last_result)
    refined_result = refine_slot_resolution(messages, last_result,max_slots=80)
    print("refined_result:", refined_result)
    colored_results = assign_colors(refined_result)   
    print("colored_results:", colored_results)
    segmented_results = segment_by_timeline(colored_results)

    with open(FINAL_PATH, "w", encoding="utf-8") as f:
        json.dump(segmented_results, f, ensure_ascii=False, indent=2)
    print(f"âœ… å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜ï¼š{FINAL_PATH}")
    
    return segmented_results

STEP1_PATH = "py/conversation_example/test/step1_topics_raw.json"
STEP2_PATH = "py/conversation_example/test/step2_topics_clean.json"
STEP3_PATH = "py/conversation_example/test/step3_topics_with_slots.json"
FINAL_PATH = "py/conversation_example/test/final_result.json"

def run_step1_semantic_scan(file_path: str, out_path: str = STEP1_PATH):
    messages = parse_conversation(file_path)   # [{id, role, content}]
    lines = []
    for m in messages:
        content = (m.get("content") or "").replace("\n", " ").strip()
        if not content:
            continue
        lines.append(f"[{m['id']}] ({m['role']}) {content}")

    full_text = "\n".join(lines)
    chunks = chunk_text(full_text, max_chars=40000)

    all_results = []
    for i, chunk in enumerate(chunks, 1):
        print(f"ğŸ§  [Step1] ç¬¬ {i}/{len(chunks)} æ®µæŠ½å–ä¸­...")
        chunk_messages = []
        for line in chunk:
            try:
                id_part, rest = line.split("] (", 1)
                mid = int(id_part[1:])
                role, text = rest.split(") ", 1)
            except ValueError:
                continue
            chunk_messages.append({
                "id": mid,
                "role": role,
                "content": text.strip()
            })

        if not chunk_messages:
            print(f"âš ï¸ [Step1] ç¬¬ {i} æ®µæ²¡æœ‰è§£æå‡ºæœ‰æ•ˆå¯¹è¯ï¼Œè·³è¿‡ Semantic_pre_scanning")
            continue

        result = Semantic_pre_scanning(chunk_messages)
        print("  partial result:", result)
        all_results.extend(result)

    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    print(f"âœ… [Step1] è¯­ä¹‰é¢„æ‰«æå®Œæˆï¼Œç»“æœå·²ä¿å­˜ï¼š{out_path}")

def run_step2_topic_clean(file_path: str,
                          step1_path: str = STEP1_PATH,
                          out_path: str = STEP2_PATH):
    messages = parse_conversation(file_path)   # history: [{id, role, content}]

    with open(step1_path, "r", encoding="utf-8") as f:
        raw_topics = json.load(f)

    print(f"ğŸ§  [Step2] Topic_cleaning ä¸­ï¼ŒåŸå§‹ä¸»é¢˜æ•°ï¼š{len(raw_topics)}")

    clear_results = Topic_cleaning(messages, raw_topics)
    print(f"ğŸ§  [Step2] æ¸…æ´—åä¸»é¢˜æ•°ï¼š{len(clear_results)}")

    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(clear_results, f, ensure_ascii=False, indent=2)
    print(f"âœ… [Step2] ä¸»é¢˜æ¸…æ´—å®Œæˆï¼Œç»“æœå·²ä¿å­˜ï¼š{out_path}")

def postprocess_topics_unique_and_prune(topics: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    å¯¹ Topic_Allocation + refine_slot_resolution çš„ç»“æœåšä¸¤ä»¶äº‹ï¼š
      1ï¼‰å…¨å±€å»é‡ï¼šåŒä¸€ä¸ª id åªä¿ç•™åœ¨ç¬¬ä¸€ä¸ªå‡ºç°çš„ topic ä¸­ï¼›
      2ï¼‰è‡ªåŠ¨ä¸¢æ‰ slots ä¸ºç©ºçš„ topicã€‚

    è¾“å…¥ç»“æ„ç¤ºä¾‹ï¼š
    [
      {"topic": "...", "slots": [ {...}, {...} ]},
      {"topic": "...", "slots": [ {...} ]},
      ...
    ]
    """
    used_ids = set()
    new_topics: List[Dict[str, Any]] = []

    for t in topics:
        slots = t.get("slots") or []
        if not isinstance(slots, list):
            slots = []

        uniq_slots = []
        for s in slots:
            if not isinstance(s, dict):
                continue
            sid = s.get("id")
            if not isinstance(sid, int):
                # id å¼‚å¸¸çš„ç›´æ¥ä¸¢æ‰
                continue
            if sid in used_ids:
                # è¿™ä¸ª id å·²ç»è¢«å‰é¢çš„ topic å äº†ï¼Œè·³è¿‡
                continue
            used_ids.add(sid)
            uniq_slots.append(s)

        # å¦‚æœè¿™ä¸ª topic ç»è¿‡å»é‡åè¿˜æœ‰ slotï¼Œå°±ä¿ç•™ï¼›å¦åˆ™ä¸¢æ‰
        if uniq_slots:
            t_new = dict(t)      # æ‹·ä¸€ä»½ï¼Œé¿å…åŸåœ°ä¿®æ”¹
            # æŒ‰ id æ’ä¸ªåºï¼Œæ—¶é—´é¡ºåºæ›´ç¨³å®š
            t_new["slots"] = sorted(uniq_slots, key=lambda x: x["id"])
            new_topics.append(t_new)

    return new_topics


def run_step3_slots_and_resolution(file_path: str,
                                   step2_path: str = STEP2_PATH,
                                   out_path: str = STEP3_PATH):
    messages = parse_conversation(file_path)

    with open(step2_path, "r", encoding="utf-8") as f:
        cleaned_topics = json.load(f)

    print(f"ğŸ§  [Step3] Topic_Allocation ä¸­ï¼Œtopic æ•°ï¼š{len(cleaned_topics)}")
    topic_with_slots = Topic_Allocation(messages, cleaned_topics)
    print("ğŸ§  [Step3] Topic_Allocation å®Œæˆ")

    # äºŒé˜¶æ®µåˆ¤æ–­æ˜¯å¦è§£å†³
    refined = refine_slot_resolution(messages, topic_with_slots, 
                                     max_slots=80)
    print("ğŸ§  [Step3] refine_slot_resolution å®Œæˆ")

    result = postprocess_topics_unique_and_prune(refined)

    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"âœ… [Step3] slot + resolved ç»“æœå·²ä¿å­˜ï¼š{out_path}")

def run_step4_segment_and_color(step3_path: str = STEP3_PATH,
                                out_path: str = FINAL_PATH):
    with open(step3_path, "r", encoding="utf-8") as f:
        topics_with_slots = json.load(f)

    print(f"ğŸ§  [Step4] assign_colors ä¸­...")
    colored_results = assign_colors(topics_with_slots)

    print(f"ğŸ§  [Step4] segment_by_timeline ä¸­...")
    segmented_results = segment_by_timeline(colored_results)

    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(segmented_results, f, ensure_ascii=False, indent=2)
    print(f"âœ… [Step4] æœ€ç»ˆç»“æœå·²ä¿å­˜ï¼š{out_path}")

if __name__ == "__main__":
    print("ğŸ¤– å¯åŠ¨å¯¹è¯å¤„ç†ç¨‹åº...")
    file_path = "py/conversation_example/xinli-test.txt"
    # run_step1_semantic_scan(file_path)
    # run_step2_topic_clean(file_path=file_path, step1_path=STEP1_PATH,out_path=STEP2_PATH)
    # run_step3_slots_and_resolution(file_path=file_path,step2_path=STEP2_PATH,out_path=STEP3_PATH)
    # run_step4_segment_and_color(step3_path=STEP3_PATH, out_path=FINAL_PATH)
    
    # final_data = process_conversation(file_path)
    # final_data = process_score(file_path)

