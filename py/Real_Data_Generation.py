import json
import os
from LLM_Extraction import llm_extract_information_incremental
from Methods import assign_colors, merge_topics_timeline

CHECKPOINT_PATH = "py/conversation_example/ChatGPT-DST-checkpoint.json"
FINAL_PATH = "py/conversation_example/ChatGPT-DST-processed.json"


def safe_process_llm_result(result, role, id_counter):
    """ç¡®ä¿ LLM è¿”å›ç»“æœæ˜¯åˆ—è¡¨å­—å…¸ï¼Œå¹¶ç»™æ¯ä¸ª slot æ·»åŠ  source å’Œ id"""
    if isinstance(result, str):
        try:
            result = json.loads(result)
        except json.JSONDecodeError:
            print("âš ï¸ è­¦å‘Šï¼šLLM è¿”å›çš„å­—ç¬¦ä¸²æ— æ³•è§£æä¸º JSONï¼Œå°†ä½œä¸ºå•æ¡æ–‡æœ¬å¤„ç†")
            result = [{"topic": "unknown", "slots": [{"slot": result, "source": role, "id": id_counter}]}]

    if isinstance(result, dict):
        result = [result]

    for topic in result:
        slots = topic.get("slots", [])
        if not isinstance(slots, list):
            slots = []
            topic["slots"] = slots
        for slot in slots:
            slot["source"] = role
            slot["id"] = id_counter

    return result


def load_checkpoint():
    """åŠ è½½ä¸­æ–­ç‚¹æ–‡ä»¶"""
    if os.path.exists(CHECKPOINT_PATH):
        try:
            with open(CHECKPOINT_PATH, "r", encoding="utf-8") as f:
                checkpoint = json.load(f)
            print(f"âœ… å·²åŠ è½½ä¸­æ–­ç‚¹ï¼Œæ¢å¤åˆ°ç¬¬ {checkpoint.get('last_id', 0)} æ¡è®°å½•ã€‚")
            return checkpoint
        except Exception as e:
            print("âš ï¸ åŠ è½½ä¸­æ–­ç‚¹å¤±è´¥ï¼Œé‡æ–°å¼€å§‹:", e)
    return {"merged_results_global": [], "last_id": 0}


def save_checkpoint(merged_results_global, last_id):
    """ä¿å­˜ä¸­æ–­ç‚¹ï¼ˆåŒ…å«å·²åˆå¹¶å¹¶åˆ†é…é¢œè‰²çš„å®Œæ•´ç»“æœï¼‰"""
    checkpoint_data = {
        "merged_results_global": merged_results_global,
        "last_id": last_id
    }
    with open(CHECKPOINT_PATH, "w", encoding="utf-8") as f:
        json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ å·²ä¿å­˜ä¸­æ–­ç‚¹ï¼ˆå«é¢œè‰²ï¼‰ï¼šå¤„ç†åˆ°ç¬¬ {last_id} æ¡æ¶ˆæ¯ã€‚")


def parse_conversation(file_path):
    """è¯»å–å¯¹è¯æ–‡æœ¬ï¼Œç”Ÿæˆæ¶ˆæ¯åˆ—è¡¨"""
    messages = []
    id_counter = 1
    role = None
    content = ""

    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    for line in lines:
        line = line.strip()
        if line.startswith("## Prompt:") or line.startswith("## Promptï¼š"):
            if content and role:
                messages.append({"id": id_counter, "role": role, "content": content.strip()})
                id_counter += 1
                content = ""
            role = "user"
            continue

        elif line.startswith("## Response:") or line.startswith("## Responseï¼š"):
            if content and role:
                messages.append({"id": id_counter, "role": role, "content": content.strip()})
                id_counter += 1
                content = ""
            role = "bot"
            continue

        if role:
            content += line + "\n"

    if content and role:
        messages.append({"id": id_counter, "role": role, "content": content.strip()})
    return messages


def process_conversation(file_path):
    """ä¸»å¤„ç†é€»è¾‘ï¼šå¸¦ä¸­æ–­æ¢å¤"""
    checkpoint = load_checkpoint()
    merged_results_global = checkpoint["merged_results_global"]
    last_id = checkpoint["last_id"]

     # âœ… å…³é”®ï¼šå…ˆåˆå§‹åŒ–ï¼Œç¡®ä¿åé¢ä¸€å®šæœ‰å€¼
    colored_results = assign_colors(merged_results_global)

    messages = parse_conversation(file_path)
    total = len(messages)

    print(f"ğŸ§© å…± {total} æ¡æ¶ˆæ¯ï¼Œå‡†å¤‡ä»ç¬¬ {last_id + 1} æ¡ç»§ç»­ã€‚")

    for msg in messages:
        id_counter = msg.get("id", 1)
        role = msg.get("role", "user")
        text = msg.get("content", "").strip()
        if id_counter <= last_id:
            continue  # è·³è¿‡å·²å¤„ç†
        if not text:
            continue

        try:
            print(f"ğŸ§  æ­£åœ¨å¤„ç†ç¬¬ {id_counter}/{total} æ¡æ¶ˆæ¯ï¼ˆ{role}ï¼‰...")
            result = llm_extract_information_incremental(text, existing_topics=merged_results_global)
            safe_result = safe_process_llm_result(result, role, id_counter)

            # åˆå¹¶ç»“æœ
            merged_results_global = merge_topics_timeline(merged_results_global + safe_result)

            # åˆ†é…é¢œè‰²
            colored_results = assign_colors(merged_results_global)

            # æ¯å¤„ç†ä¸€æ¡è‡ªåŠ¨ä¿å­˜
            save_checkpoint(merged_results_global, id_counter)

        except Exception as e:
            print(f"âŒ ç¬¬ {id_counter} æ¡å¤„ç†å¤±è´¥ï¼š{e}")
            save_checkpoint(merged_results_global, id_counter)
            continue  # ä¿æŒå¥å£®æ€§



    # ä¿å­˜æœ€ç»ˆæ–‡ä»¶
    with open(FINAL_PATH, "w", encoding="utf-8") as f:
        json.dump(colored_results, f, ensure_ascii=False, indent=2)
    print(f"âœ… å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜ï¼š{FINAL_PATH}")

    # åˆ é™¤ä¸­æ–­ç‚¹ï¼ˆå¯é€‰ï¼‰
    # os.remove(CHECKPOINT_PATH)

    return colored_results


if __name__ == "__main__":
    file_path = "py/conversation_example/ChatGPT-DST.txt"
    final_data = process_conversation(file_path)
    # ç”Ÿæˆä¹‹åçš„æ•°æ®è¦è½¬ç§»åˆ°publicç›®å½•ä¸‹
    # åˆ æ‰æœ€å¤–å±‚å­—å…¸
