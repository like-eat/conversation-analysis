import json
import os
import re
from typing import Dict, List, Set, Callable
import jieba
from jieba import analyse
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

from Methods import parse_meeting_conversation, parse_conversation

# STEP3_PATH = "py/conversation_example/TF-IDF/step3_topics_with_slots.json"
# STEP4_PATH = "py/conversation_example/TF-IDF/step4_slot_with_wordcloud.json"

STEP3_PATH = "py/conversation_example/TF-IDF/step3_topics_with_slots copy.json"
STEP4_PATH = "py/conversation_example/TF-IDF/step4_slot_with_wordcloud copy.json"

DEFAULT_STOPWORDS = set([
  "æˆ‘ä»¬","ä½ ä»¬","ä»–ä»¬","ç„¶å","å°±æ˜¯","å…¶å®","å¯èƒ½","å¤§å®¶","è¿™ä¸ª","é‚£ä¸ª","ä¸€ä¸ª","ä¸€ä¸‹","ä»¥åŠ","å› ä¸º","æ‰€ä»¥",
  "çš„è¯","æ„Ÿè§‰","è§‰å¾—","æ¯”è¾ƒ","æœ‰ç‚¹","éå¸¸","ç‰¹åˆ«","çœŸçš„","ç¡®å®","åæ­£","å…¶å®","åŸºæœ¬","ä¸€ç›´","ç°åœ¨","ç›®å‰",
  "ä¸æ˜¯","æ²¡æœ‰","ä½†æ˜¯","è€Œä¸”","ç„¶åå‘¢","å¯¹ä¸å¯¹","è¡Œ","å§","å•Š","å—¯","å“¦","å“ˆ","å“ˆå“ˆ","å‘ƒ","å˜›",
  "æˆ‘è¦","æˆ‘æ€•","æˆ‘è§‰å¾—","ä½ è§‰å¾—","ä½ çœ‹","è¿™æ¡","è¿™æ˜¯","è¿™æ ·","é‚£æ ·","æ¯æ¬¡","ä¸è¦","å®šæˆ","å¯ä»¥"
])

FILLER_SUBSTR = [
  "è¿™æ¡","è¿™æ˜¯","è¿™æ ·","é‚£æ ·","çš„è¯","æ„Ÿè§‰","è§‰å¾—","ç„¶å","å°±æ˜¯",
  "æˆ‘è§‰å¾—","ä½ è§‰å¾—","æˆ‘æ€•","æˆ‘è¦","æ¯æ¬¡","ä¸è¦","å®šæˆ","å¯¹ä¸å¯¹"
]

def good_token(w: str) -> bool:
    w = w.strip()
    if not w or w in DEFAULT_STOPWORDS:
        return False
    # å•å­—é€šå¸¸æ˜¯è™šè¯/å™ªå£°ï¼ˆå¯è§†æƒ…å†µæ”¾å¼€ï¼‰
    if len(w) <= 1:
        return False
    # çº¯æ•°å­—/ç¬¦å·
    if re.fullmatch(r"[\d\W_]+", w):
        return False
    # å¤ªå£è¯­çš„é«˜é¢‘å­—å¤´ï¼ˆæŒ‰éœ€åŠ ï¼‰
    if w in {"è¿™","é‚£","å°±","è¿˜","åˆ","éƒ½","å¾ˆ","æŒº","å•Š","å—¯","å“¦","å“ˆ","å‘ƒ"}:
        return False
    return True

# ----------------------------
# åŸºç¡€æ¸…æ´—
# ----------------------------
def clean_text(s: str) -> str:
    s = (s or "").replace("\n", " ").strip()
    s = re.sub(r"\s+", " ", s)
    return s

# ----------------------------
# 1) è‡ªåŠ¨æ„å»ºâ€œäººå/æ˜µç§°â€é»‘åå•
# ----------------------------
def build_name_banlist(messages: List[dict], topics_with_slots: List[dict]) -> Set[str]:
    names = set()

    # history é‡Œå¯èƒ½æ˜¯ role / from / source
    for m in messages:
        for k in ("role", "from", "source"):
            v = (m.get(k) or "").strip()
            if v:
                names.add(v)

    # slots é‡Œä¹Ÿæœ‰ source
    for t in topics_with_slots:
        for s in (t.get("slots") or []):
            v = (s.get("source") or "").strip()
            if v:
                names.add(v)

    # æ‰©å±•ï¼šæŠŠåå­—åˆ†è¯åä¹ŸåŠ å…¥ï¼ˆé¿å… â€œæµ…äº•ä½‘å³â€ -> â€œæµ…äº•/ä½‘å³â€æ¼æ‰ï¼‰
    ban = set()
    for nm in names:
        ban.add(nm)
        for tok in jieba.lcut(nm):
            tok = tok.strip()
            if tok:
                ban.add(tok)

    # é¢å¤–ï¼šå¸¸è§å£å¤´ç¦…/ç§°å‘¼ï¼ˆå¯æŒ‰ä½ æ•°æ®å†åŠ ï¼‰
    ban.update({"è€å¸ˆ", "åŒå­¦", "ä¸»æŒäºº"})
    return ban

def make_is_good_phrase(stopwords: Set[str], name_ban: Set[str]) -> Callable[[str], bool]:
    def is_banned_phrase(p: str) -> bool:
        p = p.strip()
        if not p:
            return True
        if p in name_ban:
            return True
        # ä¸¥æ ¼ä¸€ç‚¹ï¼šåŒ…å«ä»»ä½•åå­—ç‰‡æ®µä¹Ÿç¦ï¼ˆä¼šæ›´å¹²å‡€ï¼Œä½†å¯èƒ½è¯¯æ€ï¼‰
        for nm in name_ban:
            if nm and len(nm) >= 2 and nm in p:
                return True
        return False

    def is_good_phrase(p: str) -> bool:
        def has_filler(p: str) -> bool:
            for t in FILLER_SUBSTR:
                if t in p:
                    return True
            return False

        p = p.strip()
        if len(p) < 2 or len(p) > 12:
            return False
        if p in stopwords:
            return False
        if re.fullmatch(r"[\d\W_]+", p):
            return False
        if re.search(r"[A-Za-z0-9_]", p):  # è¿‡æ»¤ yoyo / yo ä¹‹ç±»
            return False
        if is_banned_phrase(p):
            return False
        if has_filler(p):
            return False
        return True

    return is_good_phrase

# ----------------------------
# 2) TF-IDF å…¨å±€æ¨¡å‹
# ----------------------------
def fit_global_tfidf(corpus: List[str], stopwords: Set[str]) -> TfidfVectorizer:
    def jieba_tokenize(x: str):
        x = clean_text(x)
        return [w.strip() for w in jieba.lcut(x) if w.strip() and w not in stopwords]

    vec = TfidfVectorizer(tokenizer=jieba_tokenize, lowercase=False, min_df=1, max_df=0.98)
    vec.fit(corpus if corpus else [""])
    return vec

# ----------------------------
# 3) å€™é€‰ï¼šåˆ†è¯ + ngram
# ----------------------------
def extract_candidates_token_ngrams(text: str, is_good_phrase: Callable[[str], bool], stopwords: Set[str], max_ngram: int = 2) -> List[str]:
    text = clean_text(text)
    text = re.sub(r"[A-Za-z0-9_]+", " ", text)  # å»è‹±æ–‡æ•°å­—å™ªå£°

    toks = [w for w in jieba.lcut(text) if good_token(w)]

    # token ä¹Ÿè¦è¿‡æ»¤äººå/å¤ªçŸ­/ç¬¦å·
    toks = [w for w in toks if is_good_phrase(w)]

    cands = []
    L = len(toks)
    for n in range(1, max_ngram + 1):
        for i in range(0, L - n + 1):
            p = "".join(toks[i:i+n])
            if is_good_phrase(p):
                cands.append(p)

    # å»é‡ä¿åº
    seen, out = set(), []
    for c in cands:
        if c not in seen:
            seen.add(c)
            out.append(c)
    return out

# ----------------------------
# 4) TF-IDF / TextRank æ‰“åˆ† + èåˆ
# ----------------------------
def score_tfidf_candidates(vec: TfidfVectorizer, local_text: str, candidates: List[str], stopwords: Set[str]) -> Dict[str, float]:
    local_text = clean_text(local_text)
    X = vec.transform([local_text])
    vocab = vec.vocabulary_
    row = X.toarray()[0]

    def token_score(tok: str) -> float:
        idx = vocab.get(tok)
        return float(row[idx]) if idx is not None else 0.0

    scores = {}
    for c in candidates:
        toks = [t for t in jieba.lcut(c) if t.strip() and t not in stopwords]
        if not toks:
            continue
        s = sum(token_score(t) for t in toks)  # æˆ– np.mean(...)
        if s > 0:
            scores[c] = s
    return scores

def score_textrank(local_text: str, is_good_phrase: Callable[[str], bool], topk: int = 80) -> Dict[str, float]:
    local_text = clean_text(local_text)
    pairs = analyse.textrank(local_text, topK=topk, withWeight=True, allowPOS=("n","vn","v","ns","nt","nz"))
    scores = {}
    for w, s in pairs:
        w = (w or "").strip()
        if is_good_phrase(w):
            scores[w] = float(s)
    return scores

def norm(d: Dict[str, float]) -> Dict[str, float]:
    if not d:
        return {}
    vals = list(d.values())
    vmin, vmax = min(vals), max(vals)
    if abs(vmax - vmin) < 1e-12:
        return {k: 1.0 for k in d}
    return {k: (v - vmin) / (vmax - vmin) for k, v in d.items()}

def fuse_scores(a: Dict[str, float], b: Dict[str, float], alpha: float = 0.65) -> Dict[str, float]:
    keys = set(a) | set(b)
    if not keys:
        return {}
    A = norm(a)
    B = norm(b)
    return {k: alpha * A.get(k, 0.0) + (1 - alpha) * B.get(k, 0.0) for k in keys}

def extract_keywords_for_slot_stats(
    local_ctx: str,
    vec: TfidfVectorizer,
    is_good_phrase: Callable[[str], bool],
    stopwords: Set[str],
    k: int = 20,
    min_weight: float = 0.05,     # âœ… è¿‡æ»¤å¤ªä½çš„
) -> List[Dict[str, float]]:
    cand_phrases = extract_candidates_token_ngrams(local_ctx, is_good_phrase, stopwords, max_ngram=2)
    tr = score_textrank(local_ctx, is_good_phrase, topk=120)

    candidates = cand_phrases + list(tr.keys())
    tfidf = score_tfidf_candidates(vec, local_ctx, candidates, stopwords)
    fused = fuse_scores(tfidf, tr, alpha=0.65)

    items = sorted(fused.items(), key=lambda x: x[1], reverse=True)
    if not items:
        return []

    # è¾“å‡ºå‰å…ˆæŠŠåˆ†æ•°å‹åˆ° 0~1ï¼ˆä»…ç”¨äºå¯è§†åŒ–ï¼‰
    vmax = items[0][1]
    vmin = items[-1][1]
    denom = (vmax - vmin) if (vmax - vmin) > 1e-12 else 1.0

    out = []
    for w, s in items:
        ww = float((s - vmin) / denom)
        if ww < min_weight:
            continue
        out.append({"word": w, "weight": ww})
        if len(out) >= k:
            break
    return out

# ----------------------------
# 5) Step4 ä¸»æµç¨‹ï¼ˆå¸¦ window_size ä¸Šä¸‹æ–‡ï¼‰
# ----------------------------
def run_step4_slot_with_wordcloud(
    file_path: str,
    step3_path: str = STEP3_PATH,
    out_path: str = STEP4_PATH,
    window_size: int = 20,   # å‰åå„ window_size å¥
    k_words: int = 20,       # æ¯ä¸ª slot è¾“å‡ºè¯æ•°
):
    # messages = parse_meeting_conversation(file_path)
    messages = parse_conversation(file_path)

    with open(step3_path, "r", encoding="utf-8") as f:
        topics_with_slots = json.load(f)

    print("ğŸ§  [Step4] extract_wordcloud(stats) ä¸­...")

    # âœ… å…ˆæ„å»º NAME_BANï¼ˆå¿…é¡»åœ¨è¯»å®Œæ•°æ®ä¹‹åï¼‰
    name_ban = build_name_banlist(messages, topics_with_slots)
    is_good_phrase = make_is_good_phrase(DEFAULT_STOPWORDS, name_ban)

    # å…¨å±€è¯­æ–™å»º TF-IDF
    corpus = [clean_text(m.get("content") or m.get("text") or "") for m in messages]
    corpus = [c for c in corpus if c]
    vec = fit_global_tfidf(corpus, DEFAULT_STOPWORDS)

    # id2idxï¼Œç”¨äºä¸Šä¸‹æ–‡çª—å£
    hist = sorted(messages, key=lambda m: int(m.get("id", 0)))
    id2idx = {int(m["id"]): i for i, m in enumerate(hist) if "id" in m}

    def build_local_ctx(center_id: int) -> str:
        if center_id not in id2idx:
            return ""
        c = id2idx[center_id]
        s = max(0, c - window_size)
        e = min(len(hist), c + window_size + 1)

        # âœ… åªæ‹¼æ¥æ­£æ–‡ï¼Œåˆ«åŠ  [id][role] è¿™ç§å™ªå£°
        lines = []
        for m in hist[s:e]:
            text = clean_text(m.get("content") or m.get("text") or "")
            if text:
                lines.append(text)
        return "\n".join(lines)

    miss_cnt = 0

    for topic_obj in topics_with_slots:
        slots = topic_obj.get("slots", [])
        if not isinstance(slots, list):
            continue

        for s in slots:
            if isinstance(s.get("wordcloud"), list) and len(s["wordcloud"]) > 0:
                continue

            try:
                sid = int(s.get("id"))
            except Exception:
                sid = None

            sentence = clean_text(s.get("sentence", ""))

            local_ctx = build_local_ctx(sid) if sid is not None else ""
            if not local_ctx:
                miss_cnt += 1
                local_ctx = sentence

            out = extract_keywords_for_slot_stats(
                local_ctx=local_ctx,
                vec=vec,
                is_good_phrase=is_good_phrase,
                stopwords=DEFAULT_STOPWORDS,
                k=k_words,
                min_weight=0.05,
            )
            s["wordcloud"] = out

    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(topics_with_slots, f, ensure_ascii=False, indent=2)

    print(f"âœ… [Step4] å®Œæˆï¼Œç»“æœå·²ä¿å­˜ï¼š{out_path}")
    if miss_cnt > 0:
        print(f"âš ï¸ æœ‰ {miss_cnt} ä¸ª slot çš„ id æ²¡å‘½ä¸­ historyï¼Œå·²é€€å›åªç”¨ sentence æŠ½è¯ã€‚")

if __name__ == "__main__":
    file_path = "py/conversation_example/ChatGPT-xinli.txt"
    run_step4_slot_with_wordcloud(
        file_path=file_path,
        step3_path=STEP3_PATH,
        out_path=STEP4_PATH,
        window_size=15,
        k_words=30,
    )
